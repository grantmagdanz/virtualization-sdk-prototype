{
    "docs": [
        {
            "location": "/",
            "text": "Welcome!\n\u00b6\n\n\nWith this Delphix Virtualization SDK documentation we hope you will find all you need to know in order to develop your own plugins!\n\n\nOverview\n\u00b6\n\n\nIf you already know about plugins, and are looking for something specific, use the links to the left to find what you are looking for or search.\n\n\nIf this is your first time here, and you are wondering what developing a Delphix plugin will do for you\u2014read on!\n\n\nWhat Does a Delphix Plugin do?\n\u00b6\n\n\nThe Delphix Engine is an appliance that lets you quickly and cheaply make \nvirtual copies\n of large datasets. The engine has built-in support for interfacing with certain types of datasets, such as Oracle and SQL Server.\n\n\nWhen you develop a plugin, you enable end users to use your dataset type as if they were using a built-in dataset type, whether it\u2019s MongoDB, Cassandra, or something else. Your plugin will extend the Delphix Engine\u2019s capabilities by teaching it how to run essential virtual data operations on your datasets:\n\n\n\n\nHow to stop and start them\n\n\nWhere to store their data\n\n\nHow to make virtual copies\n\n\n\n\nThese plugin operations are the building blocks of the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality to the datasets you connect to such as:\n\n\n\n\nProvisioning\n\n\nRefreshing\n\n\nRewinding\n\n\nReplication\n\n\nSyncing\n\n\n\n\nWhere to Start\n\u00b6\n\n\nRead through the first few sections of this documentation, and we will walk you through how to get setup for development, then how to develop, build, and deploy your first plugin.\n\n\nGetting Started\n will show you how to setup the SDK. When you finish this section, you will have a full plugin development environment, and you will be ready to start building.\n\n\nBuilding Your First Plugin\n will walk you step-by-step through the process of developing a very simple plugin. With it, you will learn the concepts and techniques that you will need to develop fully-fledged plugins. That does not mean this first plugin is useless\u2014you will be able to virtualize simple datasets with it.\n\n\nOnce you complete these sections, use the rest of the documentation whenever you would like. In addition to a full \nreference section\n, we include an example of a full-featured plugin that does complicated tasks (\nComing Soon!\n).",
            "title": "Welcome!"
        },
        {
            "location": "/#welcome",
            "text": "With this Delphix Virtualization SDK documentation we hope you will find all you need to know in order to develop your own plugins!",
            "title": "Welcome!"
        },
        {
            "location": "/#overview",
            "text": "If you already know about plugins, and are looking for something specific, use the links to the left to find what you are looking for or search.  If this is your first time here, and you are wondering what developing a Delphix plugin will do for you\u2014read on!",
            "title": "Overview"
        },
        {
            "location": "/#what-does-a-delphix-plugin-do",
            "text": "The Delphix Engine is an appliance that lets you quickly and cheaply make  virtual copies  of large datasets. The engine has built-in support for interfacing with certain types of datasets, such as Oracle and SQL Server.  When you develop a plugin, you enable end users to use your dataset type as if they were using a built-in dataset type, whether it\u2019s MongoDB, Cassandra, or something else. Your plugin will extend the Delphix Engine\u2019s capabilities by teaching it how to run essential virtual data operations on your datasets:   How to stop and start them  Where to store their data  How to make virtual copies   These plugin operations are the building blocks of the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality to the datasets you connect to such as:   Provisioning  Refreshing  Rewinding  Replication  Syncing",
            "title": "What Does a Delphix Plugin do?"
        },
        {
            "location": "/#where-to-start",
            "text": "Read through the first few sections of this documentation, and we will walk you through how to get setup for development, then how to develop, build, and deploy your first plugin.  Getting Started  will show you how to setup the SDK. When you finish this section, you will have a full plugin development environment, and you will be ready to start building.  Building Your First Plugin  will walk you step-by-step through the process of developing a very simple plugin. With it, you will learn the concepts and techniques that you will need to develop fully-fledged plugins. That does not mean this first plugin is useless\u2014you will be able to virtualize simple datasets with it.  Once you complete these sections, use the rest of the documentation whenever you would like. In addition to a full  reference section , we include an example of a full-featured plugin that does complicated tasks ( Coming Soon! ).",
            "title": "Where to Start"
        },
        {
            "location": "/Getting_Started/",
            "text": "Getting Started\n\u00b6\n\n\nThe Virtualization SDK is a Python package on \nPyPI\n. Install it in your local development environment so that you can build and upload a plugin.\n\n\nThe SDK consists of three parts:\n\n\n\n\nThe \ndlpx.virtulization.platform\n module\n\n\nThe \ndlpx.virtualization.libs\n module\n\n\nA CLI\n\n\n\n\nThe platform and libs modules expose objects and methods needed to develop a plugin. The CLI is used to build and upload a plugin.\n\n\nRequirements\n\u00b6\n\n\n\n\nmacOS 10.14+, Ubuntu 16.04+, or Windows 10\n\n\nPython 2.7 (Python 3 is not supported)\n\n\nJava 7+\n\n\n\n\nInstallation\n\u00b6\n\n\nTo install the latest version of the SDK run:\n\n\n$ pip install dvp\n\n\n\n\n\n\n\nUse a Virtual Environment\n\n\nWe highly recommended that you develop plugins inside of a virtual environment. To learn more about virtual environments, refer to \nVirtualenv's documentation\n.\n\n\nThe virtual environment needs to use Python 2.7. This is configured when creating the virtualenv:\n\n\n$ virtualenv -p /path/to/python2.7/binary ENV\n\n\n\n\nTo install a specific version of the SDK run:\n\n\n$ pip install dvp==<version>\n\n\n\n\n\nTo upgrade an existing installation of the SDK run:\n\n\n$ pip install dvp --upgrade\n\n\n\n\n\n\n\nAPI Build Version\n\n\nThe version of the SDK defines the version of the Virtualization Platform API your plugin will be built against.\n\n\n\n\nBasic Usage\n\u00b6\n\n\nOur \nCLI reference\n describes commands, provides examples, and a help section.\n\n\nTo build your plugin:\n\n\n$ dvp build -c <plugin_config> -a <artifact_file>\n\n\n\n\n\nThis will generate an upload artifact at \n<artifact_file>\n. That file can then be uploaded with:\n\n\n$ dvp upload -e <delphix_engine_address> -u <delphix_admin_user> -a <artifact_file>\n\n\n\n\n\nYou will be prompt for the Delphix Engine user's password.\n\n\nTroubleshooting\n\u00b6\n\n\nInstallation fails with incorrect version spec\n\u00b6\n\n\n\n\nError\n\n\n'install_requires' must be a string or list of strings containing valid project version requirement specifiers; Expected version spec in enum34;python_version < '3.4' at ;python_version < '3.4'\n\n\n\n\nThis is likely caused by an out of date \nsetuptools\n version (minimum version \n38.0.0\n) which is often due to not installing the SDK into a virtual environment. To fix this, first setup a virtual environment and attempt to install the SDK there. If you are already using a virtual environment you can update \nsetuptools\n with:\n\n\n$ pip install setuptools --upgrade",
            "title": "Getting Started"
        },
        {
            "location": "/Getting_Started/#getting-started",
            "text": "The Virtualization SDK is a Python package on  PyPI . Install it in your local development environment so that you can build and upload a plugin.  The SDK consists of three parts:   The  dlpx.virtulization.platform  module  The  dlpx.virtualization.libs  module  A CLI   The platform and libs modules expose objects and methods needed to develop a plugin. The CLI is used to build and upload a plugin.",
            "title": "Getting Started"
        },
        {
            "location": "/Getting_Started/#requirements",
            "text": "macOS 10.14+, Ubuntu 16.04+, or Windows 10  Python 2.7 (Python 3 is not supported)  Java 7+",
            "title": "Requirements"
        },
        {
            "location": "/Getting_Started/#installation",
            "text": "To install the latest version of the SDK run:  $ pip install dvp   Use a Virtual Environment  We highly recommended that you develop plugins inside of a virtual environment. To learn more about virtual environments, refer to  Virtualenv's documentation .  The virtual environment needs to use Python 2.7. This is configured when creating the virtualenv:  $ virtualenv -p /path/to/python2.7/binary ENV   To install a specific version of the SDK run:  $ pip install dvp==<version>  To upgrade an existing installation of the SDK run:  $ pip install dvp --upgrade   API Build Version  The version of the SDK defines the version of the Virtualization Platform API your plugin will be built against.",
            "title": "Installation"
        },
        {
            "location": "/Getting_Started/#basic-usage",
            "text": "Our  CLI reference  describes commands, provides examples, and a help section.  To build your plugin:  $ dvp build -c <plugin_config> -a <artifact_file>  This will generate an upload artifact at  <artifact_file> . That file can then be uploaded with:  $ dvp upload -e <delphix_engine_address> -u <delphix_admin_user> -a <artifact_file>  You will be prompt for the Delphix Engine user's password.",
            "title": "Basic Usage"
        },
        {
            "location": "/Getting_Started/#troubleshooting",
            "text": "",
            "title": "Troubleshooting"
        },
        {
            "location": "/Getting_Started/#installation-fails-with-incorrect-version-spec",
            "text": "Error  'install_requires' must be a string or list of strings containing valid project version requirement specifiers; Expected version spec in enum34;python_version < '3.4' at ;python_version < '3.4'   This is likely caused by an out of date  setuptools  version (minimum version  38.0.0 ) which is often due to not installing the SDK into a virtual environment. To fix this, first setup a virtual environment and attempt to install the SDK there. If you are already using a virtual environment you can update  setuptools  with:  $ pip install setuptools --upgrade",
            "title": "Installation fails with incorrect version spec"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/",
            "text": "Overview\n\u00b6\n\n\nIn the following few pages, we will walk through an example of making a simple, working plugin.\n\n\nOur plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin will not care about the contents and will treat it as a directory tree full of files.\n\n\nData Flow in the Delphix Engine\n\u00b6\n\n\nHere we will briefly overview how data moves through the Delphix Engine.\n\n\nIngestion\n\u00b6\n\n\nIt all begins with Delphix ingesting data\u2014copying some data from what we call a \nsource environment\n  onto the Delphix Engine.\n\n\nPlugins can use either of two basic strategies to do this copying:\n\n\n\n\ndirect linking\n, where the Delphix Engine pulls data directly from the source environment.\n\n\nstaged linking\n, where the plugin is responsible for pulling data from the source environment.\n\n\n\n\nOur plugin will use the staged linking strategy.\n\n\nWith staged linking, Delphix exposes and mounts storage to a \nstaging environment\n.  This would be an NFS share for Unix environments and iSCSI disks for Windows environments. You can use either the source environment or a different environment for staging. We will write our plugin to handle both approaches.\n\n\nOnce Delphix mounts the storage share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the storage share, which is backed by Delphix Engine storage.\n\n\nWhen this initial copy is complete, Delphix will take a snapshot of the backing storage.\n\n\n(\nDiagram coming soon!\n)\n\n\nThis same basic operation will be repeated when Delphix mounts an NFS share: The plugin copies data onto it, then Delphix snapshots the result.\n\n\nProvisioning\n\u00b6\n\n\nProvisioning\n is when you take a Delphix Engine snapshot and create a virtual dataset from it.\n\n\nFirst the snapshot is cloned onto the Delphix Engine, then this newly-cloned data is mounted as a virtual dataset onto a \ntarget environment\n. While this new virtual dataset gets updated by its end users, the original snapshot is persistent. You can use it in a few ways:\n\n\n\n\nProvision other virtual datasets from it\n\n\nRewind the virtual dataset back to the state it represents\n\n\nCreate a physical database from it in what we call V2P: Virtual to Physical\n\n\n\n\n(\nDiagram coming soon!\n)\n\n\nParts of a Plugin\n\u00b6\n\n\nA plugin consists of three main parts. We will cover them briefly here, and then fill in more details later in the tutorial.\n\n\nPlugin Config\n\u00b6\n\n\nPlugin config is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is being used? What type(s) of environments does the plugin work with? What features does the plugin offer?...\n\n\nPlugin Operations\n\u00b6\n\n\nThe plugin will need to provide operations. These are Python functions, each of which implements one small piece of functionality. This is how the plugin customizes Delphix behavior to work with the kind of dataset you\u2019re building the plugin for. One operation will handle setting up a newly-configured virtual dataset. Another will handle copying data from a source environment, and so on.\n\n\nLater we\u2019ll provide examples for our first plugin. See \nPlugin Operations\n for full details on the operations that are available, which are required, and what each one is required to do.\n\n\nSchemas\n\u00b6\n\n\nAs part of normal operations, plugins need to generate and access certain pieces of information in order to do their job. For example, plugins that work with Postgres might need to know which port number to connect to, or which credentials to use.\n\n\nDefining your plugin\u2019s schemas will enable it to give the Delphix Engine the details it needs to run the operations we\u2019ve built into it. Different datasets can have very different needs. The \nschemas\n you provide for your plugin will tell Delphix how to operate with your dataset.\n\n\nPrerequisites\n\u00b6\n\n\nTo complete the tutorial that follows, make sure you check off the things on this list:\n\n\n\n\nDownload the SDK and get it working\n\n\nA running Delphix Engine, version x.y.z or above.\n\n\nAdd at least one Unix host\u2014but preferably three\u2014to the Delphix Engine as remote environments\n\n\nHave a tool at hand for editing text files\u2014mostly Python and JSON. A simple text editor would work fine, or you can use a full-fledged IDE.\n\n\n\n\n\n\nSurvey\n\n\nPlease fill out this \nsurvey\n to give us feedback about this section.",
            "title": "Overview"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#overview",
            "text": "In the following few pages, we will walk through an example of making a simple, working plugin.  Our plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin will not care about the contents and will treat it as a directory tree full of files.",
            "title": "Overview"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#data-flow-in-the-delphix-engine",
            "text": "Here we will briefly overview how data moves through the Delphix Engine.",
            "title": "Data Flow in the Delphix Engine"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#ingestion",
            "text": "It all begins with Delphix ingesting data\u2014copying some data from what we call a  source environment   onto the Delphix Engine.  Plugins can use either of two basic strategies to do this copying:   direct linking , where the Delphix Engine pulls data directly from the source environment.  staged linking , where the plugin is responsible for pulling data from the source environment.   Our plugin will use the staged linking strategy.  With staged linking, Delphix exposes and mounts storage to a  staging environment .  This would be an NFS share for Unix environments and iSCSI disks for Windows environments. You can use either the source environment or a different environment for staging. We will write our plugin to handle both approaches.  Once Delphix mounts the storage share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the storage share, which is backed by Delphix Engine storage.  When this initial copy is complete, Delphix will take a snapshot of the backing storage.  ( Diagram coming soon! )  This same basic operation will be repeated when Delphix mounts an NFS share: The plugin copies data onto it, then Delphix snapshots the result.",
            "title": "Ingestion"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#provisioning",
            "text": "Provisioning  is when you take a Delphix Engine snapshot and create a virtual dataset from it.  First the snapshot is cloned onto the Delphix Engine, then this newly-cloned data is mounted as a virtual dataset onto a  target environment . While this new virtual dataset gets updated by its end users, the original snapshot is persistent. You can use it in a few ways:   Provision other virtual datasets from it  Rewind the virtual dataset back to the state it represents  Create a physical database from it in what we call V2P: Virtual to Physical   ( Diagram coming soon! )",
            "title": "Provisioning"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#parts-of-a-plugin",
            "text": "A plugin consists of three main parts. We will cover them briefly here, and then fill in more details later in the tutorial.",
            "title": "Parts of a Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#plugin-config",
            "text": "Plugin config is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is being used? What type(s) of environments does the plugin work with? What features does the plugin offer?...",
            "title": "Plugin Config"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#plugin-operations",
            "text": "The plugin will need to provide operations. These are Python functions, each of which implements one small piece of functionality. This is how the plugin customizes Delphix behavior to work with the kind of dataset you\u2019re building the plugin for. One operation will handle setting up a newly-configured virtual dataset. Another will handle copying data from a source environment, and so on.  Later we\u2019ll provide examples for our first plugin. See  Plugin Operations  for full details on the operations that are available, which are required, and what each one is required to do.",
            "title": "Plugin Operations"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#schemas",
            "text": "As part of normal operations, plugins need to generate and access certain pieces of information in order to do their job. For example, plugins that work with Postgres might need to know which port number to connect to, or which credentials to use.  Defining your plugin\u2019s schemas will enable it to give the Delphix Engine the details it needs to run the operations we\u2019ve built into it. Different datasets can have very different needs. The  schemas  you provide for your plugin will tell Delphix how to operate with your dataset.",
            "title": "Schemas"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#prerequisites",
            "text": "To complete the tutorial that follows, make sure you check off the things on this list:   Download the SDK and get it working  A running Delphix Engine, version x.y.z or above.  Add at least one Unix host\u2014but preferably three\u2014to the Delphix Engine as remote environments  Have a tool at hand for editing text files\u2014mostly Python and JSON. A simple text editor would work fine, or you can use a full-fledged IDE.    Survey  Please fill out this  survey  to give us feedback about this section.",
            "title": "Prerequisites"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/",
            "text": "Initial Setup\n\u00b6\n\n\nBefore we begin to start writing plugin code, we will need to do some setup work. We will be using the \ndvp\n tool, which is described in the \nGetting Started\n section.\n\n\nThe quoted examples in this section assume you're working on a Unix-like system.\n\n\nSanity check\n\u00b6\n\n\nFirst a reminder that it's highly recommended that you develop your plugin in a \nvirtual environment\n.\n\n\nNext, make sure you have a Delphix Engine ready to use, as described in the \nPrerequisites\n section on the previous page.\n\n\nFinally, let's quickly make sure that \ndvp\n is working! Type \ndvp -h\n and you should see something like the following:\n\n\n(venv)$ dvp -h\nUsage: dvp [OPTIONS] COMMAND [ARGS]...\n\n  The tools of the Delphix Virtualization SDK that help develop, build, and\n  upload a plugin.\n\nOptions:\n  --version      Show the version and exit.\n  -v, --verbose  Enable verbose mode. Can be repeated up to three times for\n                 increased verbosity.\n  -q, --quiet    Enable quiet mode. Can be repeated up to three times for\n                 increased suppression.\n  -h, --help     Show this message and exit.\n\nCommands:\n  build          Build the plugin code and generate upload artifact file...\n  download-logs  Download plugin logs from a target Delphix Engine to a...\n  init           Create a plugin in the root directory.\n  upload         Upload the generated upload artifact (the plugin JSON\n                 file)...\n\n\n\n\n\nIf this looks good, you are ready to begin!\n\n\nIf, instead, you see something like the following, go back to \nGetting Started\n and make sure you setup everything correctly before continuing.\n\n\n(venv)$ dvp\n-bash: dvp: command not found\n\n\n\n\n\nCreating a Bare Plugin\n\u00b6\n\n\nTo start, we will create a new directory where our new plugin code will live.\n\n\n(venv)$ mkdir first_plugin\n(venv)$ cd first_plugin\n\n\n\n\n\nNow that we are in our new plugin directory, we can use the \ndvp\n tool to create a plugin for us. This plugin will be a mere skeleton -- it will not do anything useful until we modify it in the subsequent pages.\n\n\n(venv) first_plugin$ dvp init -n first_plugin -s STAGED\n\n\n\n\n\nThe \n-n\n argument here means \"plugin name.\" We are using the name \nfirst_plugin\n.\n\n\nThe \n-s\n argument tells which syncing strategy we want to use.\n\n\nYou can type \ndvp init -h\n for more information about the options available.\n\n\nAfter running this command, you should see that files have been created for you:\n\n\n(venv) first_plugin$ ls\nplugin_config.yml   schema.json     src\n\n\n\n\n\nThese files are described below:\n\n\n\n\n\n\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nplugin_config.yml\n\n\nThe \nplugin config\n file, which provides a list of plugin properties\n\n\n\n\n\n\nschema.json\n\n\nContains \nschemas\n which provide custom datatype definitions\n\n\n\n\n\n\nsrc/plugin_runner.py\n\n\nA Python file which will eventually contain code that handles plugin \noperations\n\n\n\n\n\n\n\n\nOpen these files in your editor/IDE and take a look at them. At this point they will not have a lot of content, but we will add to them as we go through the next few pages.\n\n\nBuilding The New Plugin\n\u00b6\n\n\nThe new files we created above have to get \nbuilt\n to produce a single \nartifact\n. This is done with the \ndvp\n tool.\n\n\n(venv) first_plugin$ dvp build\n\n\n\n\n\nAfter the build, you should see that the build process has created a new file called \nartifact.json\n.\n\n\n(venv) first_plugin$ ls\nartifact.json       plugin_config.yml   schema.json     src\n\n\n\n\n\nUploading The New Plugin\n\u00b6\n\n\nNow using the \ndvp\n tool we can upload the artifact onto our Delphix Engine.\n\n\n(venv) first_plugin$ dvp upload -e engine.company.com -u admin\n\n\n\n\n\nThe \n-e\n argument specifies the engine on which to install the plugin, and the \n-u\n argument gives the Delphix Engine user.\n\n\nYou will be prompted for a password.\n\n\nOnce the upload is finished, you can verify the installation from the Manage > Toolkits screen in the Delphix Engine UI.\n\n\n\n\n\n\nSurvey\n\n\nPlease fill out this \nsurvey\n to give us feedback about this section.",
            "title": "Initial Setup"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#initial-setup",
            "text": "Before we begin to start writing plugin code, we will need to do some setup work. We will be using the  dvp  tool, which is described in the  Getting Started  section.  The quoted examples in this section assume you're working on a Unix-like system.",
            "title": "Initial Setup"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#sanity-check",
            "text": "First a reminder that it's highly recommended that you develop your plugin in a  virtual environment .  Next, make sure you have a Delphix Engine ready to use, as described in the  Prerequisites  section on the previous page.  Finally, let's quickly make sure that  dvp  is working! Type  dvp -h  and you should see something like the following:  (venv)$ dvp -h\nUsage: dvp [OPTIONS] COMMAND [ARGS]...\n\n  The tools of the Delphix Virtualization SDK that help develop, build, and\n  upload a plugin.\n\nOptions:\n  --version      Show the version and exit.\n  -v, --verbose  Enable verbose mode. Can be repeated up to three times for\n                 increased verbosity.\n  -q, --quiet    Enable quiet mode. Can be repeated up to three times for\n                 increased suppression.\n  -h, --help     Show this message and exit.\n\nCommands:\n  build          Build the plugin code and generate upload artifact file...\n  download-logs  Download plugin logs from a target Delphix Engine to a...\n  init           Create a plugin in the root directory.\n  upload         Upload the generated upload artifact (the plugin JSON\n                 file)...  If this looks good, you are ready to begin!  If, instead, you see something like the following, go back to  Getting Started  and make sure you setup everything correctly before continuing.  (venv)$ dvp\n-bash: dvp: command not found",
            "title": "Sanity check"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#creating-a-bare-plugin",
            "text": "To start, we will create a new directory where our new plugin code will live.  (venv)$ mkdir first_plugin\n(venv)$ cd first_plugin  Now that we are in our new plugin directory, we can use the  dvp  tool to create a plugin for us. This plugin will be a mere skeleton -- it will not do anything useful until we modify it in the subsequent pages.  (venv) first_plugin$ dvp init -n first_plugin -s STAGED  The  -n  argument here means \"plugin name.\" We are using the name  first_plugin .  The  -s  argument tells which syncing strategy we want to use.  You can type  dvp init -h  for more information about the options available.  After running this command, you should see that files have been created for you:  (venv) first_plugin$ ls\nplugin_config.yml   schema.json     src  These files are described below:     File  Description      plugin_config.yml  The  plugin config  file, which provides a list of plugin properties    schema.json  Contains  schemas  which provide custom datatype definitions    src/plugin_runner.py  A Python file which will eventually contain code that handles plugin  operations     Open these files in your editor/IDE and take a look at them. At this point they will not have a lot of content, but we will add to them as we go through the next few pages.",
            "title": "Creating a Bare Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#building-the-new-plugin",
            "text": "The new files we created above have to get  built  to produce a single  artifact . This is done with the  dvp  tool.  (venv) first_plugin$ dvp build  After the build, you should see that the build process has created a new file called  artifact.json .  (venv) first_plugin$ ls\nartifact.json       plugin_config.yml   schema.json     src",
            "title": "Building The New Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#uploading-the-new-plugin",
            "text": "Now using the  dvp  tool we can upload the artifact onto our Delphix Engine.  (venv) first_plugin$ dvp upload -e engine.company.com -u admin  The  -e  argument specifies the engine on which to install the plugin, and the  -u  argument gives the Delphix Engine user.  You will be prompted for a password.  Once the upload is finished, you can verify the installation from the Manage > Toolkits screen in the Delphix Engine UI.    Survey  Please fill out this  survey  to give us feedback about this section.",
            "title": "Uploading The New Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/",
            "text": "Discovery\n\u00b6\n\n\nWhat is Discovery?\n\u00b6\n\n\nIn order to ingest data from a source environment, the Delphix Engine first needs to learn information about the data: Where does it live? How can it be accessed? What is it called?\n\n\nDiscovery\n is the process by which the Delphix Engine learns about remote data. Discovery can be either:\n\n\n\n\nautomatic\n \u2014 where the plugin finds the remote data on its own\n\n\nmanual\n \u2014 where the user tells us about the remote data\n\n\n\n\nFor our first plugin, we will be using a mix of these two techniques.\n\n\nSource Configs and Repositories\n\u00b6\n\n\nWhat are Source Configs and Repositories?\n\u00b6\n\n\nA \nsource config\n is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?). For our first plugin, it is simply a directory tree on the filesystem of the remote environment.\n\n\nA \nrepository\n represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you are working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we do not have any special dependencies, except for the simple existence of the unix system on which the directory lives.\n\n\nWe will be using automatic discovery for our repositories, and manual discovery for our source configs. This is the default configuration that is created by \ndvp init\n, so there is nothing further we need to do here.\n\n\nDefining Your Data Formats\n\u00b6\n\n\nBecause each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they need to collect and store.\n\n\nDelphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers?\n\n\nFor our first plugin, we do not need a lot of information. We use no special information about our repositories (except some way for the user to identify them). For source configs, all we need to know is the path to the directory from which we will be ingesting data.\n\n\nThe plugin needs to describe all of this to the Delphix Engine, and it does so using \nschemas\n.  Recall that when we ran \ndvp init\n, a file full of bare-bones schemas was created. As we build up our first toolkit, we will be augmenting these schemas to serve our needs.\n\n\nRepository Schema\n\u00b6\n\n\nOpen up the \nschema.json\n file in your editor/IDE and locate \nrepositoryDefinition\n, it should look like this:\n\n\n{\n\n    \n\"repositoryDefinition\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"properties\"\n:\n \n{\n\n            \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n        \n},\n\n        \n\"nameField\"\n:\n \n\"name\"\n,\n\n        \n\"identityFields\"\n:\n \n[\n\"name\"\n]\n\n    \n}\n\n\n}\n\n\n\n\n\n\nSince we do not have any special dependencies, we can just leave it as-is.\n\n\nFor detailed information about exactly how repository schemas work, see \nthe reference page\n.\n\n\nIn brief, what we are doing here is saying that each of our repositories will have a single property called \nname\n, which will be used both as a unique identifier and as the user-visible name of the repository.\n\n\nSource Config Schema\n\u00b6\n\n\nFor source configs, the bare-bones schema is not going to be good enough. Recall that for us, a source config represents a directory tree on a remote environment.\n\n\nLocate the \nsourceConfigDefinition\n inside the \nschema.json\n file and modify the definition so it looks like this:\n\n\n\"sourceConfigDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"name\"\n:\n \n{\n\n          \n\"type\"\n:\n \n\"string\"\n,\n\n          \n\"prettyName\"\n:\n \n\"Dataset Name\"\n,\n\n          \n\"description\"\n:\n \n\"User-visible name for this dataset\"\n\n        \n},\n\n        \n\"path\"\n:\n \n{\n\n          \n\"type\"\n:\n \n\"string\"\n,\n\n          \n\"format\"\n:\n \n\"unixpath\"\n,\n\n          \n\"prettyName\"\n:\n \n\"Path\"\n,\n\n          \n\"description\"\n:\n \n\"Full path to data location on the remote environment\"\n\n        \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"name\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n,\n\n\n\n\n\n\nNow, we have two properties, a property \nname\n serving as the user-visible name of the source config and \npath\n which tells us where the data lives on the remote host. Note  we are using \npath\n as the unique identifier.\n\n\nBecause we are using manual discovery, the end user is going to be responsible for filling in values for \nname\n and \npath\n. So, we have added some things to our schema that we did not need for repositories.\n\n\nThe \nprettyName\n and \ndescription\n entries will be used by the UI to tell the user what these fields mean.\n\n\nBecause we set \nadditionalProperties\n to \nfalse\n, this will prevent users from supplying properties other than \nname\n and \npath\n.\n\n\nFinally, we have specified that the \npath\n property must be a well-formatted Unix path. This allows the UI to enforce that the format is correct before the user is allowed to proceed. (Note this only enforces the format, and does not actually check to see if the path really exists on some remote environment!)\n\n\nRefer to the reference page for \nSchemas\n for more details about these entries, and for other things that you can do in these schemas.\n\n\nImplementing Discovery in Your Plugin\n\u00b6\n\n\nAbout Python Code\n\u00b6\n\n\nAs described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize.\n\n\nRight now, we are concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered.\n\n\nA Look at the Generated Code\n\u00b6\n\n\nRecall that the \ndvp init\n command we ran created a file called \nsrc/plugin_runner.py\n. Open this file in your editor/IDE. You will see that this file already contains a bunch of Python code. Let's take a look at the first three blocks of code in this file.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMount\n,\n \nMountSpecification\n,\n \nPlugin\n\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \n(\n\n    \nRepositoryDefinition\n,\n\n    \nSourceConfigDefinition\n,\n\n    \nSnapshotDefinition\n,\n\n\n)\n\n\n\n\n\n\nThese \nimport\n lines make certain functionality available to our Python code. Some of this functionality will\nbe used just below, as we implement discovery. Others will be used later on, as we implement\ningestion and provisioning. Later, you'll add more \nimport\ns to unlock more functionality.\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n\n\n\nThis line creates a Python object which allows us to define our plugin types. We have the ability to do this because of the \nimport Plugin\n statement above.\n\n\nThis object is stored in a variable we have elected to call \nplugin\n. We are free to call this variable anything we want, so long as we also change the \nentryPoint\n line in the \nplugin_config.yml\n file. For this example, we will just leave it as \nplugin\n.\n\n\n#\n\n\n# Below is an example of the repository discovery operation.\n\n\n#\n\n\n# NOTE: The decorators are defined on the 'plugin' object created above.\n\n\n#\n\n\n# Mark the function below as the operation that does repository discovery.\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n    \n#\n\n    \n# This is an object generated from the repositoryDefinition schema.\n\n    \n# In order to use it locally you must run the 'build -g' command provided\n\n    \n# by the SDK tools from the plugin's root directory.\n\n    \n#\n\n\n    \nreturn\n \n[\nRepositoryDefinition\n(\nname\n=\n'1e87dc30-3cdb-4f0a-9634-07ce017d20d1'\n)]\n\n\n\n\n\n\nThis is our first \nplugin operation\n. In this case, it's defining what will happen when the Delphix Engine wants to discover repositories on an environment.  Let's take a look at this code line-by-line\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n\n\n\n\n\nThis begins the definition of a function called \nrepository_discovery\n.\n\n\nWe are using a Python \ndecorator\n which signals to the Delphix Engine that this is the function which should be called when it is time to do repository discovery. The actual name of the function doesn't matter here. Note that we are using our \nplugin\n variable here as part of the decorator.\n\n\nThe Delphix Engine will pass us information about the source environment in an argument called \nsource_connection\n.\n\n\n\n\nWarning\n\n\nThe name of this input argument matters. That is, you'll always need to have an argument called\n\nsource_connection\n here. Each plugin operation has its own set of required argument names. For\ndetails on which arguments apply to which operations, see the \nreference section\n.\n\n\n\n\n    \nreturn\n \n[\nRepositoryDefinition\n(\nname\n=\n'1e87dc30-3cdb-4f0a-9634-07ce017d20d1'\n)]\n\n\n\n\n\n\nThis creates and returns a Python object that corresponds to the format defined by our repository schema. Because out repository has exactly one string property called \nname\n, therefore this Python object has one property called \nname\n.\n\n\nNotice that the code generator has filled in the value of \nname\n with a random string. This results in a plugin operation that works, but which will not be very helpful for the user. We'll change this later.\n\n\nThe rest of the file contains more plugin operations, and we'll be modifying them later.\n\n\nRepository Discovery\n\u00b6\n\n\nNow, we need to modify the provided \nrepository discovery\n operation. This operation will examine a remote environment, find any repositories, and return information about them to the Delphix Engine.\n\n\nAs a reminder, our only external dependency on the remote environment is simply the existence of a filesystem. Since every Unix host has a filesystem, that means we will have exactly one repository per remote environment. Therefore, our repository discovery operation can be very simple.\n\n\nIn fact, as we saw above, the default-generated \nrepository_discovery\n function does almost exactly what we want -- it returns one single repository for any Unix host that it is asked to work with. The only problem with it is that it uses\nunhelpful name.  That's really easy to change!\n\n\nReplace or modify \nrepository_discovery\n so it looks like this:\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n    \nrepository\n \n=\n \nRepositoryDefinition\n(\n'Repository for our First Plugin'\n)\n\n    \nreturn\n \n[\nrepository\n]\n\n\n\n\n\n\n\n\nTip\n\n\nBe careful to always use consistent indentation in Python code!\n\n\n\n\nSource Config Discovery\n\u00b6\n\n\nFor source configs, we will rely solely on manual discovery. Therefore, the user will tell us which directories they want to ingest from. We still have to define a source config discovery operation -- it just won't need to do much.\n\n\nThe job of this operation is to return only source configs associated with the given \nrepository\n. This function will be called once per repository. In our case, that means it will only be called once.\n\n\nBecause we want to supply \nno\n automatically-discovered source configs, this function should simply returns an empty list.\n\n\nIn fact, \ndvp init\n has already generated a function for us that does exactly this.\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n    \nreturn\n \n[]\n\n\n\n\n\n\nIf we wanted to do automatic discovery of source configs, we'd modify this function. But, for our purposes now, the existing code is fine and we don't need to change anything.\n\n\nHow to Run Discovery in the Delphix Engine\n\u00b6\n\n\nLet us make sure discovery works!\n\n\n\n\n\n\nRun the \ndvp build\n commands, as before. This will build the plugin, with all of the new changes, and create an artifact.\n\n\n\n\n\n\nRun \ndvp upload -e <engine> -u <user>\n, as before. This will get all the new changes onto the Delphix Engine.\n\n\n\n\n\n\nOnce the new plugin is uploaded, add a remote unix environment to your engine. To do this, go to \nManage > Environments\n, chose \nAdd Environment\n from the menu, answer the questions, and \nSubmit\n. (If you already have an environment set up, you can just refresh it instead).\n\n\n\n\n\n\nTo keep an eye on this discovery process, you may need to open the \nActions\n tab on the UI. If any errors happen, they will be reported here.\n\n\n\n\nAfter the automatic discovery process completes, go to the \nDatabases\n tab. You will see an entry for \nRepository For Our First Plugin\n. This is the repository you created in your Python code.\n\n\n\n\n\n\nNotice that it says \nNo databases found on installation\n. This is because we chose not to do automatic source config discovery.\n\n\nHowever, because we have allowed manual source config discovery, you can add your own entries by clicking the plus sign (\nAdd Database\n). Complete the information in the Add Database dialog and click Add.\n\n\n\n\nThis should all look familiar. It is precisely what we defined in our source config schema. As expected, there are two entries, one for our \nname\n property, and one for \npath\n.\n\n\nFor example, in the above screenshot, we are specifying that we want to sync the \n/bin\n directory\nfrom the remote host, and we want to call it \nBinaries\n. You can pick any directory and name that\nyou want.\n\n\nOnce you have added one or more source configs, you will be able to sync. This is covered on the next page.\n\n\n\n\nWarning\n\n\nOnce you have automatically or manually created source configs, you will not be allowed to modify your plugin's source config schema. We will cover how to deal with this later in the upgrade section. For now, if you need to change your plugin's source config schema:\n\n\n\n\nYou will have to delete any source configs you have manually added.\n\n\nDelete the plugin and its corresponding objects (dSources, Virtual Sources, etc) if the source configs were manually discovered.\n\n\n\n\n\n\n\n\nSurvey\n\n\nPlease fill out this \nsurvey\n to give us feedback about this section.",
            "title": "Discovery"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#discovery",
            "text": "",
            "title": "Discovery"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#what-is-discovery",
            "text": "In order to ingest data from a source environment, the Delphix Engine first needs to learn information about the data: Where does it live? How can it be accessed? What is it called?  Discovery  is the process by which the Delphix Engine learns about remote data. Discovery can be either:   automatic  \u2014 where the plugin finds the remote data on its own  manual  \u2014 where the user tells us about the remote data   For our first plugin, we will be using a mix of these two techniques.",
            "title": "What is Discovery?"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#source-configs-and-repositories",
            "text": "",
            "title": "Source Configs and Repositories"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#what-are-source-configs-and-repositories",
            "text": "A  source config  is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?). For our first plugin, it is simply a directory tree on the filesystem of the remote environment.  A  repository  represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you are working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we do not have any special dependencies, except for the simple existence of the unix system on which the directory lives.  We will be using automatic discovery for our repositories, and manual discovery for our source configs. This is the default configuration that is created by  dvp init , so there is nothing further we need to do here.",
            "title": "What are Source Configs and Repositories?"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#defining-your-data-formats",
            "text": "Because each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they need to collect and store.  Delphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers?  For our first plugin, we do not need a lot of information. We use no special information about our repositories (except some way for the user to identify them). For source configs, all we need to know is the path to the directory from which we will be ingesting data.  The plugin needs to describe all of this to the Delphix Engine, and it does so using  schemas .  Recall that when we ran  dvp init , a file full of bare-bones schemas was created. As we build up our first toolkit, we will be augmenting these schemas to serve our needs.",
            "title": "Defining Your Data Formats"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#repository-schema",
            "text": "Open up the  schema.json  file in your editor/IDE and locate  repositoryDefinition , it should look like this:  { \n     \"repositoryDefinition\" :   { \n         \"type\" :   \"object\" , \n         \"properties\" :   { \n             \"name\" :   {   \"type\" :   \"string\"   } \n         }, \n         \"nameField\" :   \"name\" , \n         \"identityFields\" :   [ \"name\" ] \n     }  }   Since we do not have any special dependencies, we can just leave it as-is.  For detailed information about exactly how repository schemas work, see  the reference page .  In brief, what we are doing here is saying that each of our repositories will have a single property called  name , which will be used both as a unique identifier and as the user-visible name of the repository.",
            "title": "Repository Schema"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#source-config-schema",
            "text": "For source configs, the bare-bones schema is not going to be good enough. Recall that for us, a source config represents a directory tree on a remote environment.  Locate the  sourceConfigDefinition  inside the  schema.json  file and modify the definition so it looks like this:  \"sourceConfigDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"required\" :   [ \"name\" ,   \"path\" ], \n     \"additionalProperties\" :   false , \n     \"properties\" :   { \n         \"name\" :   { \n           \"type\" :   \"string\" , \n           \"prettyName\" :   \"Dataset Name\" , \n           \"description\" :   \"User-visible name for this dataset\" \n         }, \n         \"path\" :   { \n           \"type\" :   \"string\" , \n           \"format\" :   \"unixpath\" , \n           \"prettyName\" :   \"Path\" , \n           \"description\" :   \"Full path to data location on the remote environment\" \n         } \n     }, \n     \"nameField\" :   \"name\" , \n     \"identityFields\" :   [ \"path\" ]  } ,   Now, we have two properties, a property  name  serving as the user-visible name of the source config and  path  which tells us where the data lives on the remote host. Note  we are using  path  as the unique identifier.  Because we are using manual discovery, the end user is going to be responsible for filling in values for  name  and  path . So, we have added some things to our schema that we did not need for repositories.  The  prettyName  and  description  entries will be used by the UI to tell the user what these fields mean.  Because we set  additionalProperties  to  false , this will prevent users from supplying properties other than  name  and  path .  Finally, we have specified that the  path  property must be a well-formatted Unix path. This allows the UI to enforce that the format is correct before the user is allowed to proceed. (Note this only enforces the format, and does not actually check to see if the path really exists on some remote environment!)  Refer to the reference page for  Schemas  for more details about these entries, and for other things that you can do in these schemas.",
            "title": "Source Config Schema"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#implementing-discovery-in-your-plugin",
            "text": "",
            "title": "Implementing Discovery in Your Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#about-python-code",
            "text": "As described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize.  Right now, we are concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered.",
            "title": "About Python Code"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#a-look-at-the-generated-code",
            "text": "Recall that the  dvp init  command we ran created a file called  src/plugin_runner.py . Open this file in your editor/IDE. You will see that this file already contains a bunch of Python code. Let's take a look at the first three blocks of code in this file.  from   dlpx.virtualization.platform   import   Mount ,   MountSpecification ,   Plugin  from   generated.definitions   import   ( \n     RepositoryDefinition , \n     SourceConfigDefinition , \n     SnapshotDefinition ,  )   These  import  lines make certain functionality available to our Python code. Some of this functionality will\nbe used just below, as we implement discovery. Others will be used later on, as we implement\ningestion and provisioning. Later, you'll add more  import s to unlock more functionality.  plugin   =   Plugin ()   This line creates a Python object which allows us to define our plugin types. We have the ability to do this because of the  import Plugin  statement above.  This object is stored in a variable we have elected to call  plugin . We are free to call this variable anything we want, so long as we also change the  entryPoint  line in the  plugin_config.yml  file. For this example, we will just leave it as  plugin .  #  # Below is an example of the repository discovery operation.  #  # NOTE: The decorators are defined on the 'plugin' object created above.  #  # Mark the function below as the operation that does repository discovery.  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ): \n     # \n     # This is an object generated from the repositoryDefinition schema. \n     # In order to use it locally you must run the 'build -g' command provided \n     # by the SDK tools from the plugin's root directory. \n     # \n\n     return   [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )]   This is our first  plugin operation . In this case, it's defining what will happen when the Delphix Engine wants to discover repositories on an environment.  Let's take a look at this code line-by-line  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ):   This begins the definition of a function called  repository_discovery .  We are using a Python  decorator  which signals to the Delphix Engine that this is the function which should be called when it is time to do repository discovery. The actual name of the function doesn't matter here. Note that we are using our  plugin  variable here as part of the decorator.  The Delphix Engine will pass us information about the source environment in an argument called  source_connection .   Warning  The name of this input argument matters. That is, you'll always need to have an argument called source_connection  here. Each plugin operation has its own set of required argument names. For\ndetails on which arguments apply to which operations, see the  reference section .        return   [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )]   This creates and returns a Python object that corresponds to the format defined by our repository schema. Because out repository has exactly one string property called  name , therefore this Python object has one property called  name .  Notice that the code generator has filled in the value of  name  with a random string. This results in a plugin operation that works, but which will not be very helpful for the user. We'll change this later.  The rest of the file contains more plugin operations, and we'll be modifying them later.",
            "title": "A Look at the Generated Code"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#repository-discovery",
            "text": "Now, we need to modify the provided  repository discovery  operation. This operation will examine a remote environment, find any repositories, and return information about them to the Delphix Engine.  As a reminder, our only external dependency on the remote environment is simply the existence of a filesystem. Since every Unix host has a filesystem, that means we will have exactly one repository per remote environment. Therefore, our repository discovery operation can be very simple.  In fact, as we saw above, the default-generated  repository_discovery  function does almost exactly what we want -- it returns one single repository for any Unix host that it is asked to work with. The only problem with it is that it uses\nunhelpful name.  That's really easy to change!  Replace or modify  repository_discovery  so it looks like this:  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ): \n     repository   =   RepositoryDefinition ( 'Repository for our First Plugin' ) \n     return   [ repository ]    Tip  Be careful to always use consistent indentation in Python code!",
            "title": "Repository Discovery"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#source-config-discovery",
            "text": "For source configs, we will rely solely on manual discovery. Therefore, the user will tell us which directories they want to ingest from. We still have to define a source config discovery operation -- it just won't need to do much.  The job of this operation is to return only source configs associated with the given  repository . This function will be called once per repository. In our case, that means it will only be called once.  Because we want to supply  no  automatically-discovered source configs, this function should simply returns an empty list.  In fact,  dvp init  has already generated a function for us that does exactly this.  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n     return   []   If we wanted to do automatic discovery of source configs, we'd modify this function. But, for our purposes now, the existing code is fine and we don't need to change anything.",
            "title": "Source Config Discovery"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#how-to-run-discovery-in-the-delphix-engine",
            "text": "Let us make sure discovery works!    Run the  dvp build  commands, as before. This will build the plugin, with all of the new changes, and create an artifact.    Run  dvp upload -e <engine> -u <user> , as before. This will get all the new changes onto the Delphix Engine.    Once the new plugin is uploaded, add a remote unix environment to your engine. To do this, go to  Manage > Environments , chose  Add Environment  from the menu, answer the questions, and  Submit . (If you already have an environment set up, you can just refresh it instead).    To keep an eye on this discovery process, you may need to open the  Actions  tab on the UI. If any errors happen, they will be reported here.   After the automatic discovery process completes, go to the  Databases  tab. You will see an entry for  Repository For Our First Plugin . This is the repository you created in your Python code.    Notice that it says  No databases found on installation . This is because we chose not to do automatic source config discovery.  However, because we have allowed manual source config discovery, you can add your own entries by clicking the plus sign ( Add Database ). Complete the information in the Add Database dialog and click Add.   This should all look familiar. It is precisely what we defined in our source config schema. As expected, there are two entries, one for our  name  property, and one for  path .  For example, in the above screenshot, we are specifying that we want to sync the  /bin  directory\nfrom the remote host, and we want to call it  Binaries . You can pick any directory and name that\nyou want.  Once you have added one or more source configs, you will be able to sync. This is covered on the next page.   Warning  Once you have automatically or manually created source configs, you will not be allowed to modify your plugin's source config schema. We will cover how to deal with this later in the upgrade section. For now, if you need to change your plugin's source config schema:   You will have to delete any source configs you have manually added.  Delete the plugin and its corresponding objects (dSources, Virtual Sources, etc) if the source configs were manually discovered.     Survey  Please fill out this  survey  to give us feedback about this section.",
            "title": "How to Run Discovery in the Delphix Engine"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/",
            "text": "Data Ingestion\n\u00b6\n\n\nHow Does Delphix Ingest Data?\n\u00b6\n\n\nAs \npreviously\n discussed, the Delphix Engine uses the \ndiscovery\n process to learn about datasets that live on a \nsource environment\n. In this section we will learn how the Delphix Engine uses a two-step process to ingest a dataset.\n\n\nLinking\n\u00b6\n\n\nThe first step is called \nlinking\n. This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a \ndSource\n.\n\n\nSyncing\n\u00b6\n\n\nImmediately after linking, the new dSource is \nsynced\n for the first time. Syncing is a process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date.\n\n\nThe details of how this is done varies significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from: direct and staging.\n\n\nWith the \ndirect\n strategy, the plugin is not in charge\nof the data copying. Instead the Delphix Engine directly pulls raw data from the source environment.\nThe plugin merely provides the location of the data. This is a very simple strategy, and is also\nquite limiting.\n\n\nFor our first plugin, we will be using the more flexible \nstaging\n strategy. With this strategy, the Delphix Engine uses NFS for Unix environments (or iSCSI on Windows environments) to mount storage onto a \nstaging environment\n. Our plugin will then be in full control of how to get data from the source environment onto this storage mount.\n\n\nWith the staging strategy, there are two types of syncs: sync and resync. A \nsync\n is used to ingestion incremental changes while a \nresync\n is used to re-ingest all the data for the dSource. For databases, this could mean re-ingesting from a full database backup to reset the dSource. A \nsync\n and a \nresync\n execute the same plugin operations and are differentiated by a boolean flag in the \nsnapshot_parameters\n argument passed into \nlinked.pre_snapshot\n and \nlinked.post_snapshot\n.\n\n\nA regular \nsync\n is the default and is executed as part of policy driven syncs. A \nresync\n is only executed during initial ingestion or if the Delphix user manually starts one. The customer can manually trigger a \nresync\n via the UI by selecting the dSource, going to more options and selecting \nResynchronize dSource\n. \n\n\n\n\nGotcha\n\n\nAlthough it is not common, it is entirely possible that the staging environment is the same as the source environment. Be careful not to assume otherwise in your plugins.\n\n\n\n\nOur Syncing Strategy\n\u00b6\n\n\nFor our purposes here in this intro plugin, we will use a simple strategy. We won't do anything with the resync snapshot parameter and simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We will do this by running the Unix tool \nrsync\n from our staging environment, and rely on passwordless SSH to connect to the source environment.\n\n\n\n\nInfo\n\n\nThis plugin is assuming that \nrsync\n is installed on the staging host, and that the staging\nhost user is able to SSH into the source host without having to type in a password. A more\nfull-featured plugin would test these assumptions, usually as part of discovery.\n\n\n\n\nIn the special case mentioned above, where the staging environment is the same as the source environment, we could likely do something more efficient. However, for simplicity's sake, we won't do that here.\n\n\nDefining Your Linked Source Data Format\n\u00b6\n\n\nIn order to be able to successfully do the copying required, plugins might need to get some information from the end-user of your plugin. In our case, we need to tell \nrsync\n how to access the files. This means we need to know the source environment's IP address (or domain name), the username we need to connect as, and finally the location where the files live.\n\n\nAgain, we will be using a JSON schema to define the data format. The user will be presented with a UI that lets them provide all the information our schema specifies.\n\n\nOpen up \nschema.json\n in your editor/IDE. Locate the \nLinkedSourceDefinition\n and replace it with the following schema:\n\n\n\"linkedSourceDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"required\"\n:\n \n[\n\"sourceAddress\"\n,\n \n\"username\"\n,\n \n\"mountLocation\"\n],\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"sourceAddress\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Host from which to copy\"\n,\n\n            \n\"description\"\n:\n \n\"IP or FQDN of host from which to copy\"\n\n        \n},\n\n        \n\"username\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Username on Source Host\"\n,\n\n            \n\"description\"\n:\n \n\"Username for making SSH connection to source host\"\n\n        \n},\n\n        \n\"mountLocation\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"format\"\n:\n \n\"unixpath\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Mount Location on Staging Host\"\n,\n\n            \n\"description\"\n:\n \n\"Where to mount storage onto the staging host while syncing\"\n\n        \n}\n\n    \n}\n\n\n}\n,\n\n\n\n\n\n\n\n\nInfo\n\n\nAs will be explained later, this schema will be used to generate Python code.\nAll names in the autogenerated Python code will use \nlower_case_with_underscores\n as attribute names as per Python variable naming conventions.\nThat is, if we were to use \nmountLocation\n as the schema property name, it would be called\n\nmount_location\n in the generated Python code.\n\n\n\n\nWith this schema, the user will be required to provide the source username, the source's IP address, and the staging mount location as part of the linking process.\n\n\nImplementing Syncing in Your Plugin\n\u00b6\n\n\nThere are three things we must do to implement syncing. First, we need to tell the Delphix Engine\nwhere to mount storage onto the staging environment. Next we need to actually do the work of copying\ndata onto that mounted storage. Finally, we need to generate any snapshot-related data.\n\n\nMount Specification\n\u00b6\n\n\nBefore syncing can begin, the Delphix Engine needs to mount some storage onto the staging host.\nSince different plugins can have different requirements about where exactly this mount lives, it is\nup to the plugin to specify this location. As mentioned above, our simple plugin will get this\nlocation from the user.\n\n\nOpen up the \nplugin_runner.py\n file and find the \nlinked_mount_specification\n function (which was generated by \ndvp init\n). Replace it with the following code:\n\n\n@plugin.linked.mount_specification()\ndef linked_mount_specification(staged_source, repository):\n    mount_location = staged_source.parameters.mount_location\n    mount = Mount(staged_source.staged_connection.environment, mount_location)\n    return MountSpecification([mount])\n\n\n\n\n\nLet's take this line-by-line to see what's going on here.\n\n\n@plugin.linked.mount_specification()\n\n\n\n\n\nThis \ndecorator\n announces that the following function\nis the code that handles the \nmount_specification\n operation. This is what allows the Delphix\nEngine to know which function to call when it's time to learn where to mount. Every operation\ndefinition will begin with a similar decorator.\n\n\ndef linked_mount_specification(staged_source, repository):\n\n\n\n\n\nThis begins a Python function definition. We chose to call it \nlinked_mount_specification\n, but we\ncould have chosen any name at all. This function accepts two arguments, one giving information about\nthe linked source, and one giving information about the associated repository.\n\n\n    mount_location = staged_source.parameters.mount_location\n\n\n\n\n\nThe \nstaged_source\n input argument contains an attribute called \nparameters\n. This in turn contains\nall of the properties defined by the \nlinkedSourceDefinition\n schema. So, in our case, that means\nit will contain attributes called \nsource_address\n, \nusername\n, and \nmount_location\n. Note how any attribute defined in \ncamelCase\n in the schema is converted to \nvariable_with_underscores\n. This line\nsimply retrieves the user-provided mount location and saves it in a local variable.\n\n\n    mount = Mount(staged_source.staged_connection.environment, mount_location)\n\n\n\n\n\nThis line constructs a new object from the \nMount class\n. This class\nholds details about how Delphix Engine storage is mounted onto remote environments. Here, we\ncreate a mount object that says to mount onto the staging environment, at the location specified\nby the user.\n\n\n    return MountSpecification([mount])\n\n\n\n\n\nOn the line just before this one, we created an object that describes a \nsingle\n mount. Now, we\nmust return a full \nmount specification\n. In general,\na mount specification is a collection of mounts. But, in our case, we just have one single mount.\nTherefore, we use an array with only one item it in -- namely, the one single mount object we\ncreated just above.\n\n\nData Copying\n\u00b6\n\n\nAs explained \nhere\n, the Delphix Engine will always run the plugin's \npreSnapshot\n operation just before taking a snapshot of the dsource. That means our \npreSnapshot\n operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy.\n\n\nUnlike the previous operations we've seen so far, the pre-snapshot operation will not be autogenerated by \ndvp init\n.\nSo, we will need to add one ourselves.  Open up the \nplugin_runner.py\n file.\n\n\nFirst, we'll add a new import line near the top of the file, so that we can use Delphix's platform libraries (explained below).\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\n\n\n\nNext, we'll add a new function:\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \ncopy_data_from_source\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n    \nstage_mount_path\n \n=\n \nstaged_source\n.\nmount\n.\nmount_path\n\n    \ndata_location\n \n=\n \n\"{}@{}:{}\"\n.\nformat\n(\nstaged_source\n.\nparameters\n.\nusername\n,\n\n        \nstaged_source\n.\nparameters\n.\nsource_address\n,\n\n        \nsource_config\n.\npath\n)\n\n\n    \nrsync_command\n \n=\n \n\"rsync -r {} {}\"\n.\nformat\n(\ndata_location\n,\n \nstage_mount_path\n)\n\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\nstaged_source\n.\nstaged_connection\n,\n \nrsync_command\n)\n\n\n    \nif\n \nresult\n.\nexit_code\n \n!=\n \n0\n:\n\n        \nraise\n \nRuntimeError\n(\n\"Could not copy files. Please ensure that passwordless SSH works for {}.\n\\n\n{}\"\n.\nformat\n(\nstaged_source\n.\nparameters\n.\nsource_address\n,\n \nresult\n.\nstderr\n))\n\n\n\n\n\n\nLet's walk through this function and see what's going on\n\n\n    \nstage_mount_path\n \n=\n \nstaged_source\n.\nmount\n.\nmount_path\n\n\n\n\n\n\nThe \nstaged_source\n argument contains information about the current mount location. Here we save that\nto a local variable for convenience.\n\n\n    \ndata_location\n \n=\n \n\"{}@{}:{}\"\n.\nformat\n(\nstaged_source\n.\nparameters\n.\nusername\n,\n\n        \nstaged_source\n.\nparameters\n.\nsource_address\n,\n\n        \nsource_config\n.\npath\n)\n\n\n\n\n\n\nThis code creates a Python string that represents the location of the data that we want to ingest.\nThis is in the form \n<user>@<host>:<path>\n. For example \njdoe@sourcehost.mycompany.com:/bin\n. As\nbefore with \nmountLocation\n, we have defined our schemas such that these three pieces of information\nwere provided by the user. Here we're just putting them into a format that \nrsync\n will understand.\n\n\n    \nrsync_command\n \n=\n \n\"rsync -r {} {}\"\n.\nformat\n(\ndata_location\n,\n \nstage_mount_path\n)\n\n\n\n\n\n\nThis line is the actual Bash command that we'll be running on the staging host. This will look something like \nrsync -r user@host:/source/path /staging/mount/path\n.\n\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\nstaged_source\n.\nstaged_connection\n,\n \nrsync_command\n)\n\n\n\n\n\n\nThis is an example of a \nplatform library\n function, where we ask the Virtualization Platform\nto do some work on our behalf. In this case, we're asking the platform to run our Bash command on the\nstaging environment. For full details on the \nrun_bash\n platform library function and others, see this \nreference\n.\n\n\n    \nif\n \nresult\n.\nexit_code\n \n!=\n \n0\n:\n\n        \nraise\n \nRuntimeError\n(\n\"Could not copy files. Please ensure that passwordless SSH works for {}.\n\\n\n{}\"\n.\nformat\n(\nstaged_source\n.\nparameters\n.\nsource_address\n,\n \nresult\n.\nstderr\n))\n\n\n\n\n\n\nFinally, we check to see if our Bash command actually worked okay. If not, we raise an error\nmessage, and describe one possible problem for the user to investigate.\n\n\nSaving Snapshot Data\n\u00b6\n\n\nWhenever the Delphix Engine takes a \nsnapshot\n of a dSource or VDB,\nthe plugin has the chance to save any information it likes alongside that snapshot. Later, if the\nsnapshot is ever used to provision a new VDB, the plugin can use the previously-saved information\nto help get the new VDB ready for use.\n\n\nThe format of this data is controlled by the plugin's \nsnapshotDefinition\n schema. In our case, we\ndon't have any data we need to save. So, there's not much to do here. We will not modify the blank\nschema that was created by \ndvp init\n.\n\n\nWe do still need to provide python function for the engine to call, but we don't have to do much.\nIn fact, the default implementation that was generated by \ndvp init\n will work just fine for our purposes:\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot\n(\nstaged_source\n,\n\n                         \nrepository\n,\n\n                         \nsource_config\n,\n\n                         \nsnapshot_parameters\n):\n\n    \nreturn\n \nSnapshotDefinition\n()\n\n\n\n\n\n\nThe only thing this code is doing is creating a new object using our (empty) snapshot\ndefinition, and returning that new empty object.\n\n\nHow to Link and Sync in the Delphix Engine\n\u00b6\n\n\nLet's try it out and make sure this works!\n\n\nPrerequisites\n\n\n\n\n\n\nYou should already have a repository and source config set up from the previous page.\n\n\n\n\n\n\nYou can optionally set up a new staging environment. Or, you can simply re-use your source\n    environment for staging.\n\n\n\n\n\n\nProcedure\n\n\n\n\nNote\n\n\nRecall that, for simplicity's sake, this plugin requires that passwordless SSH is set up between\nyour staging and source environments. You may want to verify this before continuing.\n\n\n\n\n\n\n\n\nAs before, use \ndvp build\n and \ndvp upload\n to get your latest plugin changes installed onto\nthe Delphix Engine.\n\n\n\n\n\n\nGo to \nManage > Environments\n, select your \nsource\n environment, and then go to the \nDatabases\n tab. Find \nRepository for our First Plugin\n, and your source config underneath it.\n\n\n\n\n\n\nFrom your source config click \nAdd dSource\n. This will begin the linking process. The first\nscreen you see should ask for the properties that you recently added to your \nlinkedSourceDefinition\n. \n\n\n\n\n\n\nWalk through the remainder of the screens and hit \nSubmit\n. This will kick off the initial link and first sync.\n\n\n\n\n\n\nYou can confirm that your new dSource was added successfully by going to \nManage > Datasets\n.\n\n\n\n\n\n\nAfter you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data.\n\n\n\n\nGotcha\n\n\nManually creating a dSource sets your plugin\u2019s linked source schema in stone, and you will have to recreate the dSource in order to modify your schema. We will cover how to deal with this correctly later, in the upgrade section. For now, if you need to change your plugin's linked source schema, you will have to first delete any dSources you have manually added.\n\n\n\n\n\n\nSurvey\n\n\nPlease fill out this \nsurvey\n to give us feedback about this section.",
            "title": "Data Ingestion"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#data-ingestion",
            "text": "",
            "title": "Data Ingestion"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#how-does-delphix-ingest-data",
            "text": "As  previously  discussed, the Delphix Engine uses the  discovery  process to learn about datasets that live on a  source environment . In this section we will learn how the Delphix Engine uses a two-step process to ingest a dataset.",
            "title": "How Does Delphix Ingest Data?"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#linking",
            "text": "The first step is called  linking . This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a  dSource .",
            "title": "Linking"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#syncing",
            "text": "Immediately after linking, the new dSource is  synced  for the first time. Syncing is a process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date.  The details of how this is done varies significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from: direct and staging.  With the  direct  strategy, the plugin is not in charge\nof the data copying. Instead the Delphix Engine directly pulls raw data from the source environment.\nThe plugin merely provides the location of the data. This is a very simple strategy, and is also\nquite limiting.  For our first plugin, we will be using the more flexible  staging  strategy. With this strategy, the Delphix Engine uses NFS for Unix environments (or iSCSI on Windows environments) to mount storage onto a  staging environment . Our plugin will then be in full control of how to get data from the source environment onto this storage mount.  With the staging strategy, there are two types of syncs: sync and resync. A  sync  is used to ingestion incremental changes while a  resync  is used to re-ingest all the data for the dSource. For databases, this could mean re-ingesting from a full database backup to reset the dSource. A  sync  and a  resync  execute the same plugin operations and are differentiated by a boolean flag in the  snapshot_parameters  argument passed into  linked.pre_snapshot  and  linked.post_snapshot .  A regular  sync  is the default and is executed as part of policy driven syncs. A  resync  is only executed during initial ingestion or if the Delphix user manually starts one. The customer can manually trigger a  resync  via the UI by selecting the dSource, going to more options and selecting  Resynchronize dSource .    Gotcha  Although it is not common, it is entirely possible that the staging environment is the same as the source environment. Be careful not to assume otherwise in your plugins.",
            "title": "Syncing"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#our-syncing-strategy",
            "text": "For our purposes here in this intro plugin, we will use a simple strategy. We won't do anything with the resync snapshot parameter and simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We will do this by running the Unix tool  rsync  from our staging environment, and rely on passwordless SSH to connect to the source environment.   Info  This plugin is assuming that  rsync  is installed on the staging host, and that the staging\nhost user is able to SSH into the source host without having to type in a password. A more\nfull-featured plugin would test these assumptions, usually as part of discovery.   In the special case mentioned above, where the staging environment is the same as the source environment, we could likely do something more efficient. However, for simplicity's sake, we won't do that here.",
            "title": "Our Syncing Strategy"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#defining-your-linked-source-data-format",
            "text": "In order to be able to successfully do the copying required, plugins might need to get some information from the end-user of your plugin. In our case, we need to tell  rsync  how to access the files. This means we need to know the source environment's IP address (or domain name), the username we need to connect as, and finally the location where the files live.  Again, we will be using a JSON schema to define the data format. The user will be presented with a UI that lets them provide all the information our schema specifies.  Open up  schema.json  in your editor/IDE. Locate the  LinkedSourceDefinition  and replace it with the following schema:  \"linkedSourceDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   false , \n     \"required\" :   [ \"sourceAddress\" ,   \"username\" ,   \"mountLocation\" ], \n     \"properties\" :   { \n         \"sourceAddress\" :   { \n             \"type\" :   \"string\" , \n             \"prettyName\" :   \"Host from which to copy\" , \n             \"description\" :   \"IP or FQDN of host from which to copy\" \n         }, \n         \"username\" :   { \n             \"type\" :   \"string\" , \n             \"prettyName\" :   \"Username on Source Host\" , \n             \"description\" :   \"Username for making SSH connection to source host\" \n         }, \n         \"mountLocation\" :   { \n             \"type\" :   \"string\" , \n             \"format\" :   \"unixpath\" , \n             \"prettyName\" :   \"Mount Location on Staging Host\" , \n             \"description\" :   \"Where to mount storage onto the staging host while syncing\" \n         } \n     }  } ,    Info  As will be explained later, this schema will be used to generate Python code.\nAll names in the autogenerated Python code will use  lower_case_with_underscores  as attribute names as per Python variable naming conventions.\nThat is, if we were to use  mountLocation  as the schema property name, it would be called mount_location  in the generated Python code.   With this schema, the user will be required to provide the source username, the source's IP address, and the staging mount location as part of the linking process.",
            "title": "Defining Your Linked Source Data Format"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#implementing-syncing-in-your-plugin",
            "text": "There are three things we must do to implement syncing. First, we need to tell the Delphix Engine\nwhere to mount storage onto the staging environment. Next we need to actually do the work of copying\ndata onto that mounted storage. Finally, we need to generate any snapshot-related data.",
            "title": "Implementing Syncing in Your Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#mount-specification",
            "text": "Before syncing can begin, the Delphix Engine needs to mount some storage onto the staging host.\nSince different plugins can have different requirements about where exactly this mount lives, it is\nup to the plugin to specify this location. As mentioned above, our simple plugin will get this\nlocation from the user.  Open up the  plugin_runner.py  file and find the  linked_mount_specification  function (which was generated by  dvp init ). Replace it with the following code:  @plugin.linked.mount_specification()\ndef linked_mount_specification(staged_source, repository):\n    mount_location = staged_source.parameters.mount_location\n    mount = Mount(staged_source.staged_connection.environment, mount_location)\n    return MountSpecification([mount])  Let's take this line-by-line to see what's going on here.  @plugin.linked.mount_specification()  This  decorator  announces that the following function\nis the code that handles the  mount_specification  operation. This is what allows the Delphix\nEngine to know which function to call when it's time to learn where to mount. Every operation\ndefinition will begin with a similar decorator.  def linked_mount_specification(staged_source, repository):  This begins a Python function definition. We chose to call it  linked_mount_specification , but we\ncould have chosen any name at all. This function accepts two arguments, one giving information about\nthe linked source, and one giving information about the associated repository.      mount_location = staged_source.parameters.mount_location  The  staged_source  input argument contains an attribute called  parameters . This in turn contains\nall of the properties defined by the  linkedSourceDefinition  schema. So, in our case, that means\nit will contain attributes called  source_address ,  username , and  mount_location . Note how any attribute defined in  camelCase  in the schema is converted to  variable_with_underscores . This line\nsimply retrieves the user-provided mount location and saves it in a local variable.      mount = Mount(staged_source.staged_connection.environment, mount_location)  This line constructs a new object from the  Mount class . This class\nholds details about how Delphix Engine storage is mounted onto remote environments. Here, we\ncreate a mount object that says to mount onto the staging environment, at the location specified\nby the user.      return MountSpecification([mount])  On the line just before this one, we created an object that describes a  single  mount. Now, we\nmust return a full  mount specification . In general,\na mount specification is a collection of mounts. But, in our case, we just have one single mount.\nTherefore, we use an array with only one item it in -- namely, the one single mount object we\ncreated just above.",
            "title": "Mount Specification"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#data-copying",
            "text": "As explained  here , the Delphix Engine will always run the plugin's  preSnapshot  operation just before taking a snapshot of the dsource. That means our  preSnapshot  operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy.  Unlike the previous operations we've seen so far, the pre-snapshot operation will not be autogenerated by  dvp init .\nSo, we will need to add one ourselves.  Open up the  plugin_runner.py  file.  First, we'll add a new import line near the top of the file, so that we can use Delphix's platform libraries (explained below).  from   dlpx.virtualization   import   libs   Next, we'll add a new function:  @plugin.linked.pre_snapshot ()  def   copy_data_from_source ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n     stage_mount_path   =   staged_source . mount . mount_path \n     data_location   =   \"{}@{}:{}\" . format ( staged_source . parameters . username , \n         staged_source . parameters . source_address , \n         source_config . path ) \n\n     rsync_command   =   \"rsync -r {} {}\" . format ( data_location ,   stage_mount_path ) \n\n     result   =   libs . run_bash ( staged_source . staged_connection ,   rsync_command ) \n\n     if   result . exit_code   !=   0 : \n         raise   RuntimeError ( \"Could not copy files. Please ensure that passwordless SSH works for {}. \\n {}\" . format ( staged_source . parameters . source_address ,   result . stderr ))   Let's walk through this function and see what's going on       stage_mount_path   =   staged_source . mount . mount_path   The  staged_source  argument contains information about the current mount location. Here we save that\nto a local variable for convenience.       data_location   =   \"{}@{}:{}\" . format ( staged_source . parameters . username , \n         staged_source . parameters . source_address , \n         source_config . path )   This code creates a Python string that represents the location of the data that we want to ingest.\nThis is in the form  <user>@<host>:<path> . For example  jdoe@sourcehost.mycompany.com:/bin . As\nbefore with  mountLocation , we have defined our schemas such that these three pieces of information\nwere provided by the user. Here we're just putting them into a format that  rsync  will understand.       rsync_command   =   \"rsync -r {} {}\" . format ( data_location ,   stage_mount_path )   This line is the actual Bash command that we'll be running on the staging host. This will look something like  rsync -r user@host:/source/path /staging/mount/path .       result   =   libs . run_bash ( staged_source . staged_connection ,   rsync_command )   This is an example of a  platform library  function, where we ask the Virtualization Platform\nto do some work on our behalf. In this case, we're asking the platform to run our Bash command on the\nstaging environment. For full details on the  run_bash  platform library function and others, see this  reference .       if   result . exit_code   !=   0 : \n         raise   RuntimeError ( \"Could not copy files. Please ensure that passwordless SSH works for {}. \\n {}\" . format ( staged_source . parameters . source_address ,   result . stderr ))   Finally, we check to see if our Bash command actually worked okay. If not, we raise an error\nmessage, and describe one possible problem for the user to investigate.",
            "title": "Data Copying"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#saving-snapshot-data",
            "text": "Whenever the Delphix Engine takes a  snapshot  of a dSource or VDB,\nthe plugin has the chance to save any information it likes alongside that snapshot. Later, if the\nsnapshot is ever used to provision a new VDB, the plugin can use the previously-saved information\nto help get the new VDB ready for use.  The format of this data is controlled by the plugin's  snapshotDefinition  schema. In our case, we\ndon't have any data we need to save. So, there's not much to do here. We will not modify the blank\nschema that was created by  dvp init .  We do still need to provide python function for the engine to call, but we don't have to do much.\nIn fact, the default implementation that was generated by  dvp init  will work just fine for our purposes:  @plugin.linked.post_snapshot ()  def   linked_post_snapshot ( staged_source , \n                          repository , \n                          source_config , \n                          snapshot_parameters ): \n     return   SnapshotDefinition ()   The only thing this code is doing is creating a new object using our (empty) snapshot\ndefinition, and returning that new empty object.",
            "title": "Saving Snapshot Data"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#how-to-link-and-sync-in-the-delphix-engine",
            "text": "Let's try it out and make sure this works!  Prerequisites    You should already have a repository and source config set up from the previous page.    You can optionally set up a new staging environment. Or, you can simply re-use your source\n    environment for staging.    Procedure   Note  Recall that, for simplicity's sake, this plugin requires that passwordless SSH is set up between\nyour staging and source environments. You may want to verify this before continuing.     As before, use  dvp build  and  dvp upload  to get your latest plugin changes installed onto\nthe Delphix Engine.    Go to  Manage > Environments , select your  source  environment, and then go to the  Databases  tab. Find  Repository for our First Plugin , and your source config underneath it.    From your source config click  Add dSource . This will begin the linking process. The first\nscreen you see should ask for the properties that you recently added to your  linkedSourceDefinition .     Walk through the remainder of the screens and hit  Submit . This will kick off the initial link and first sync.    You can confirm that your new dSource was added successfully by going to  Manage > Datasets .    After you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data.   Gotcha  Manually creating a dSource sets your plugin\u2019s linked source schema in stone, and you will have to recreate the dSource in order to modify your schema. We will cover how to deal with this correctly later, in the upgrade section. For now, if you need to change your plugin's linked source schema, you will have to first delete any dSources you have manually added.    Survey  Please fill out this  survey  to give us feedback about this section.",
            "title": "How to Link and Sync in the Delphix Engine"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/",
            "text": "Provisioning\n\u00b6\n\n\nWhat is Provisioning?\n\u00b6\n\n\nOnce Delphix has a \nsnapshot\n of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new \nvirtual dataset\n. This new virtual dataset will be made available for use on a \ntarget environment\n. This process is called \nprovisioning\n.\n\n\nOur Provisioning Strategy\n\u00b6\n\n\nFor many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment.\n\n\nIn our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling \nwhere\n that data is mounted.\n\n\nDefining our Provision-Related Data Formats\n\u00b6\n\n\nWe have already seen four custom data formats: for repositories, source configs, snapshots and\nlinked sources. The final one is used for \nvirtual sources\n.\n\n\nRecall that, for our plugin, a VDB is just a directory full of files. There is no special\nprocedure needed to enable it, no DBMS to coordinate with, etc. All we need to do is make the files\navailable on the target environment.\n\n\nSo, the only question for the user is \"Where should these files live?\"\n\n\nOpen up \nschema.json\n, locate the \nvirtualSourceDefintion\n section, and change it to look like this:\n\n\n\"virtualSourceDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n \n:\n \nfalse\n,\n\n    \n\"required\"\n:\n \n[\n\"mountLocation\"\n],\n\n    \n\"properties\"\n \n:\n \n{\n\n        \n\"mountLocation\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"format\"\n:\n \n\"unixpath\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Mount Location on Target Host\"\n,\n\n            \n\"description\"\n:\n \n\"Where to mount VDB onto the target host\"\n\n        \n}\n\n    \n}\n\n\n}\n,\n\n\n\n\n\n\nThis should look familiar from the source config schema that we did earlier. We only have one\nproperty, and it represents the mount location on the target environment.\n\n\nImplementing Provisioning\n\u00b6\n\n\nThere are numerous ways for a plugin to customize the provisioning process.\nFor our example plugin, we just need to do a few things:\n\n\n\n\nTell Delphix where to mount the virtual dataset.\n\n\nCreate a \nsourceConfig\n to represent each newly-provisioned virtual dataset.\n\n\nModify an existing \nsourceConfig\n, if necessary, when the virtual dataset is refreshed or rewound.\n\n\nConstruct snapshot-related data any time a snapshot is taken of the virtual dataset.\n\n\n\n\nControlling Mounting\n\u00b6\n\n\nAs we saw previously with linked sources, we need to tell Delphix where to mount the dataset. Open\nup \nplugin_runner.py\n and find the \nplugin.virtual.mount_specification\n decorator. Change that function so that\nit looks like this:\n\n\n@plugin.virtual.mount_specification\n()\n\n\ndef\n \nvdb_mount_spec\n(\nvirtual_source\n,\n \nrepository\n):\n\n    \nmount_location\n \n=\n \nvirtual_source\n.\nparameters\n.\nmount_location\n\n    \nmount\n \n=\n \nMount\n(\nvirtual_source\n.\nconnection\n.\nenvironment\n,\n \nmount_location\n)\n\n    \nreturn\n \nMountSpecification\n([\nmount\n])\n\n\n\n\n\n\nAs we did with linked sources, we just look up what the user told us, and then package that up\nand return it to Delphix.\n\n\nCreating a Source Config for a new VDB\n\u00b6\n\n\nJust like we saw earlier with \nlinked datasets\n, each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config at provision time\n\n\nAs a reminder, here is what our schema looks like for source configs:\n\n\n\"sourceConfigDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"name\"\n:\n \n{\n\n          \n\"type\"\n:\n \n\"string\"\n,\n\n          \n\"prettyName\"\n:\n \n\"Dataset Name\"\n,\n\n          \n\"description\"\n:\n \n\"User-visible name for this dataset\"\n\n        \n},\n\n        \n\"path\"\n:\n \n{\n\n          \n\"type\"\n:\n \n\"string\"\n,\n\n          \n\"format\"\n:\n \n\"unixpath\"\n,\n\n          \n\"prettyName\"\n:\n \n\"Path\"\n,\n\n          \n\"description\"\n:\n \n\"Full path to data location on the remote environment\"\n\n        \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"name\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n,\n\n\n\n\n\n\nThus, for each newly-cloned virtual dataset, we create a new source config object with a name and a\npath. This is done by the \nconfigure\n plugin operation.\n\n\nIn addition to generating a new source config, the configure operation is also tasked with getting\nthe newly-cloned dataset ready for use on the target environment. What this means exactly will vary\nfrom plugin to plugin. For our simple plugin, the dataset does not require any setup work, and so we\nonly have to worry about the source config.\n\n\nFind the \nplugin.virtual.configure\n decorator and change the function to look like this:\n\n\n@plugin.virtual.configure\n()\n\n\ndef\n \nconfigure_new_vdb\n(\nvirtual_source\n,\n \nsnapshot\n,\n \nrepository\n):\n\n    \nmount_location\n \n=\n \nvirtual_source\n.\nparameters\n.\nmount_location\n\n    \nname\n \n=\n \n\"VDB mounted at {}\"\n.\nformat\n(\nmount_location\n)\n\n    \nreturn\n \nSourceConfigDefinition\n(\npath\n=\nmount_location\n,\n \nname\n=\nname\n)\n\n\n\n\n\n\nModifying a Source Config after Rewind or Refresh\n\u00b6\n\n\nJust as a new VDB might need to be configured, a refreshed or rewound VDB might need to be\n\"reconfigured\" to handle the new post-refresh (or post-rewind) state of the VDB. So, just as there\nis a \nconfigure\n operation, there is also a \nreconfigure\n operation.\n\n\nThe main difference between the two is that \nconfigure\n must \ncreate\n a source config, but\n\nreconfigure\n needs to \nmodify\n a pre-existing source config.\n\n\nIn our simple plugin, there is no special work to do at reconfigure time, and there is no reason\nto modify anything about the source config. We just need to write a \nreconfigure\n operation that\nreturns the existing source config without making any changes. Find the \nplugin.virtual.reconfigure\n decorator and modify the function as follows.\n\n\n@plugin.virtual.reconfigure\n()\n\n\ndef\n \nreconfigure_existing_vdb\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot\n):\n\n    \nreturn\n \nsource_config\n\n\n\n\n\n\nSaving Snapshot Data\n\u00b6\n\n\nAs with our linked sources, we don't actually have anything we need to save when VDB snapshots are\ntaken. And, again, \ndvp init\n has created a post-snapshot operation that will work just fine for us without modification:\n\n\n@plugin.virtual.post_snapshot\n()\n\n\ndef\n \nvirtual_post_snapshot\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n    \nreturn\n \nSnapshotDefinition\n()\n\n\n\n\n\n\nHow To Provision in the Delphix Engine\n\u00b6\n\n\nFinally, let us try it out to make sure provisioning works!\n\n\n\n\nAgain, use \ndvp build\n and \ndvp upload\n to get your new changes onto your Delphix Engine.\n\n\nClick \nManage > Datasets\n.\n\n\nSelect the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the \nProvision vFiles\n icon.\n\n\nThis will open the Provision VDB wizard. Complete the steps and select \nSubmit\n.\n  During VDB provisioning one of the things you will have to do is to provide the data required by your virtual source schema. In our case, that means you will be asked to provide a value for \nmountLocation\n. You will also be asked to choose a target environment on which the new VDB will live. After the wizard finishes, you will see a job appear in the \nActions\n tab on the right-hand side of the screen. When that job completes, your new VDB should be ready.\n\n\nTo ensure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the \nmountLocation\n. What you should see is a copy of the directory that you linked to with your dSource.\n\n\n\n\n\n\nSurvey\n\n\nPlease fill out this \nsurvey\n to give us feedback about this section.",
            "title": "Provisioning"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#provisioning",
            "text": "",
            "title": "Provisioning"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#what-is-provisioning",
            "text": "Once Delphix has a  snapshot  of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new  virtual dataset . This new virtual dataset will be made available for use on a  target environment . This process is called  provisioning .",
            "title": "What is Provisioning?"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#our-provisioning-strategy",
            "text": "For many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment.  In our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling  where  that data is mounted.",
            "title": "Our Provisioning Strategy"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#defining-our-provision-related-data-formats",
            "text": "We have already seen four custom data formats: for repositories, source configs, snapshots and\nlinked sources. The final one is used for  virtual sources .  Recall that, for our plugin, a VDB is just a directory full of files. There is no special\nprocedure needed to enable it, no DBMS to coordinate with, etc. All we need to do is make the files\navailable on the target environment.  So, the only question for the user is \"Where should these files live?\"  Open up  schema.json , locate the  virtualSourceDefintion  section, and change it to look like this:  \"virtualSourceDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\"   :   false , \n     \"required\" :   [ \"mountLocation\" ], \n     \"properties\"   :   { \n         \"mountLocation\" :   { \n             \"type\" :   \"string\" , \n             \"format\" :   \"unixpath\" , \n             \"prettyName\" :   \"Mount Location on Target Host\" , \n             \"description\" :   \"Where to mount VDB onto the target host\" \n         } \n     }  } ,   This should look familiar from the source config schema that we did earlier. We only have one\nproperty, and it represents the mount location on the target environment.",
            "title": "Defining our Provision-Related Data Formats"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#implementing-provisioning",
            "text": "There are numerous ways for a plugin to customize the provisioning process.\nFor our example plugin, we just need to do a few things:   Tell Delphix where to mount the virtual dataset.  Create a  sourceConfig  to represent each newly-provisioned virtual dataset.  Modify an existing  sourceConfig , if necessary, when the virtual dataset is refreshed or rewound.  Construct snapshot-related data any time a snapshot is taken of the virtual dataset.",
            "title": "Implementing Provisioning"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#controlling-mounting",
            "text": "As we saw previously with linked sources, we need to tell Delphix where to mount the dataset. Open\nup  plugin_runner.py  and find the  plugin.virtual.mount_specification  decorator. Change that function so that\nit looks like this:  @plugin.virtual.mount_specification ()  def   vdb_mount_spec ( virtual_source ,   repository ): \n     mount_location   =   virtual_source . parameters . mount_location \n     mount   =   Mount ( virtual_source . connection . environment ,   mount_location ) \n     return   MountSpecification ([ mount ])   As we did with linked sources, we just look up what the user told us, and then package that up\nand return it to Delphix.",
            "title": "Controlling Mounting"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#creating-a-source-config-for-a-new-vdb",
            "text": "Just like we saw earlier with  linked datasets , each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config at provision time  As a reminder, here is what our schema looks like for source configs:  \"sourceConfigDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"required\" :   [ \"name\" ,   \"path\" ], \n     \"additionalProperties\" :   false , \n     \"properties\" :   { \n         \"name\" :   { \n           \"type\" :   \"string\" , \n           \"prettyName\" :   \"Dataset Name\" , \n           \"description\" :   \"User-visible name for this dataset\" \n         }, \n         \"path\" :   { \n           \"type\" :   \"string\" , \n           \"format\" :   \"unixpath\" , \n           \"prettyName\" :   \"Path\" , \n           \"description\" :   \"Full path to data location on the remote environment\" \n         } \n     }, \n     \"nameField\" :   \"name\" , \n     \"identityFields\" :   [ \"path\" ]  } ,   Thus, for each newly-cloned virtual dataset, we create a new source config object with a name and a\npath. This is done by the  configure  plugin operation.  In addition to generating a new source config, the configure operation is also tasked with getting\nthe newly-cloned dataset ready for use on the target environment. What this means exactly will vary\nfrom plugin to plugin. For our simple plugin, the dataset does not require any setup work, and so we\nonly have to worry about the source config.  Find the  plugin.virtual.configure  decorator and change the function to look like this:  @plugin.virtual.configure ()  def   configure_new_vdb ( virtual_source ,   snapshot ,   repository ): \n     mount_location   =   virtual_source . parameters . mount_location \n     name   =   \"VDB mounted at {}\" . format ( mount_location ) \n     return   SourceConfigDefinition ( path = mount_location ,   name = name )",
            "title": "Creating a Source Config for a new VDB"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#modifying-a-source-config-after-rewind-or-refresh",
            "text": "Just as a new VDB might need to be configured, a refreshed or rewound VDB might need to be\n\"reconfigured\" to handle the new post-refresh (or post-rewind) state of the VDB. So, just as there\nis a  configure  operation, there is also a  reconfigure  operation.  The main difference between the two is that  configure  must  create  a source config, but reconfigure  needs to  modify  a pre-existing source config.  In our simple plugin, there is no special work to do at reconfigure time, and there is no reason\nto modify anything about the source config. We just need to write a  reconfigure  operation that\nreturns the existing source config without making any changes. Find the  plugin.virtual.reconfigure  decorator and modify the function as follows.  @plugin.virtual.reconfigure ()  def   reconfigure_existing_vdb ( virtual_source ,   repository ,   source_config ,   snapshot ): \n     return   source_config",
            "title": "Modifying a Source Config after Rewind or Refresh"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#saving-snapshot-data",
            "text": "As with our linked sources, we don't actually have anything we need to save when VDB snapshots are\ntaken. And, again,  dvp init  has created a post-snapshot operation that will work just fine for us without modification:  @plugin.virtual.post_snapshot ()  def   virtual_post_snapshot ( virtual_source ,   repository ,   source_config ): \n     return   SnapshotDefinition ()",
            "title": "Saving Snapshot Data"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#how-to-provision-in-the-delphix-engine",
            "text": "Finally, let us try it out to make sure provisioning works!   Again, use  dvp build  and  dvp upload  to get your new changes onto your Delphix Engine.  Click  Manage > Datasets .  Select the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the  Provision vFiles  icon.  This will open the Provision VDB wizard. Complete the steps and select  Submit .\n  During VDB provisioning one of the things you will have to do is to provide the data required by your virtual source schema. In our case, that means you will be asked to provide a value for  mountLocation . You will also be asked to choose a target environment on which the new VDB will live. After the wizard finishes, you will see a job appear in the  Actions  tab on the right-hand side of the screen. When that job completes, your new VDB should be ready.  To ensure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the  mountLocation . What you should see is a copy of the directory that you linked to with your dSource.    Survey  Please fill out this  survey  to give us feedback about this section.",
            "title": "How To Provision in the Delphix Engine"
        },
        {
            "location": "/References/CLI/",
            "text": "CLI\n\u00b6\n\n\nThe CLI is installed with the SDK. To install the SDK, refer to the \nGetting Started\n section.\n\n\nHelp\n\u00b6\n\n\nEvery command has a \n-h\n flag including the CLI itself. This will print the help menu.\n\n\nExamples\n\u00b6\n\n\nGet the CLI's help menu.\n\n\n$ dvp -h\nUsage: dvp [OPTIONS] COMMAND [ARGS]...\n\n  The tools of the Delphix Virtualization SDK that help develop, build, and\n  upload a plugin.\n\nOptions:\n  --version      Show the version and exit.\n  -v, --verbose  Enable verbose mode. Can be repeated up to three times for\n                 increased verbosity.\n  -q, --quiet    Enable quiet mode. Can be repeated up to three times for\n                 increased suppression.\n  -h, --help     Show this message and exit.\n\nCommands:\n  build   Build the plugin code and generate upload artifact file using the...\n  init    Create a plugin in the root directory.\n  upload  Upload the generated upload artifact (the plugin JSON file) that...\n\n\n\n\n\nGet the \nbuild\n command's help menu.\n\n\n$ dvp build -h\nUsage: dvp build [OPTIONS]\n\n  Build the plugin code and generate upload artifact file using the\n  configuration provided in the plugin config file.\n\nOptions:\n  -c, --plugin-config FILE    Set the path to plugin config file.This file\n                              contains the configuration required to build the\n                              plugin.  [default: plugin_config.yml]\n  -a, --upload-artifact FILE  Set the upload artifact.The upload artifact file\n                              generated by build process will be writtento\n                              this file and later used by upload command.\n                              [default: artifact.json]\n  -g, --generate-only         Only generate the Python classes from the schema\n                              definitions. Do not do a full build or create an\n                              upload artifact.  [default: False]\n  -h, --help                  Show this message and exit.\n\n\n\n\n\nVerbosity\n\u00b6\n\n\nTo change the verbosity level of the CLI you can specify up to three \n-v\n (to increase) or \n-q\n (to decrease) the amount that is printed to the console. This is an option on the CLI itself and can be used with any command.\n\n\n\n\n\n\n\n\nOption\n\n\nOutput\n\n\n\n\n\n\n\n\n\n\n-qqq\n\n\nNone\n\n\n\n\n\n\n-qq\n\n\nCritical\n\n\n\n\n\n\n-q\n\n\nError\n\n\n\n\n\n\n-v\n\n\nInfo\n\n\n\n\n\n\n-vv\n\n\nDebug\n\n\n\n\n\n\n-vvv\n\n\nAll\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nPrint everything to the console.\n\n\n$ dvp -vvv build\n\n\n\n\n\nPrint nothing to the console.\n\n\n$ dvp -qqq upload -e engine.example.com -u admin\n\n\n\n\n\nCommands\n\u00b6\n\n\ninit\n\u00b6\n\n\nDescription\n\u00b6\n\n\nCreate a plugin in the root directory. The plugin will be valid but have no functionality.\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\nOption\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\nDescription\n\n\nRequired\n\n\nDefault\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\n-r,\n--root-dir\nDIRECTORY\n\n\nSet the plugin root directory.\n\n\nN\n\n\nos.cwd()\n\n\n\n\n\n\n-n,\n--plugin-name\nTEXT\n\n\nSet the name of the plugin that will be used to identify it.\n\n\nN\n\n\nid\n\n\n\n\n\n\n-s,\n--ingestion-strategy\n[DIRECT|STAGED]\n\n\nSet the ingestion strategy of the plugin. A \"direct\" plugin ingests without a staging server while a \"staged\" plugin requires a staging server.\n\n\nN\n\n\nDIRECT\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nCreate a plugin in the current working directory with the \nDIRECT\n ingestion strategy. Here the name of the plugin will be equal to the id that is generated.\n\n\n$ dvp init\n\n\n\n\n\nCreate a plugin in the current working directory with the \nDIRECT\n ingestion strategy and use \npostgres\n as the display name.\n\n\n$ dvp init -n postgres\n\n\n\n\n\nCreate a plugin called \nmongodb\n in a custom location with the \nSTAGED\n ingestion strategy.\n\n\n$ dvp init -n mongodb -s STAGED\n\n\n\n\n\n\n\nbuild\n\u00b6\n\n\nDescription\n\u00b6\n\n\nBuild the plugin code and generate upload artifact file using the configuration provided in the plugin config file.\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\nOption \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\nDescription\n\n\nRequired\n\n\nDefault\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\n-c,\n--plugin-config\nFILE\n\n\nSet the path to plugin config file.This file contains the configuration required to build the plugin.\n\n\nN\n\n\nplugin_config.yml\n\n\n\n\n\n\n-a,\n--upload-artifact\nFILE\n\n\nSet the upload artifact.The upload artifact file generated by build process will be written to this file and later used by upload command.\n\n\nN\n\n\nartifact.json\n\n\n\n\n\n\n-g,\n--generate-only\n\n\nOnly generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact.\n\n\nN\n\n\nFalse\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nDo a full build of the plugin and write the upload artifact to \n./artifact.json\n.\n\n\nThis assumes current working directory contains a plugin config file named \nplugin_config.yml\n.\n\n\n$ dvp build\n\n\n\n\n\nDo a partial build and just generate the Python classes from the schema definitions.\n\n\nThis assumes current working directory contains ad plugin config file named \nplugin_config.yml\n.\n\n\n$ dvp build -g\n\n\n\n\n\nDo a full build of a plugin and write the artifact file to a custom location.\n\n\n$ dvp build -c config.yml -a build/artifact.json\n\n\n\n\n\n\n\nupload\n\u00b6\n\n\nDescription\n\u00b6\n\n\nUpload the generated upload artifact (the plugin JSON file) that was built to a target Delphix Engine. Note that the upload artifact should be the file created after running the build command and will fail if it's not readable or valid.\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\nOption \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\nDescription\n\n\nRequired\n\n\nDefault \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\n-e,\n--delphix-engine\nTEXT\n\n\nUpload plugin to the provided engine. This should be either the hostname or IP address.\n\n\nY\n\n\nNone\n\n\n\n\n\n\n-u,\n--user\nTEXT\n\n\nAuthenticate to the Delphix Engine with the provided user.\n\n\nY\n\n\nNone\n\n\n\n\n\n\n-a,\n--upload-artifact FILE\n\n\nPath to the upload artifact that was generated through build.\n\n\nN\n\n\nartifact.json\n\n\n\n\n\n\n--password\nTEXT\n\n\nAuthenticate using the provided password. If ommitted, the password will be requested through a secure prompt.\n\n\nN\n\n\nNone\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nUpload artifact \nbuild/artifact.json\n to \ndelphix-engine.domain\n using the user \nadmin\n. Since the password option is ommitted, a secure password prompt is used instead.\n\n\n$ dvp upload -a build/artifact -e engine.example.com -u admin\nPassword:\n\n\n\n\n\n\n\ndownload-logs\n\u00b6\n\n\nDescription\n\u00b6\n\n\nDownload plugin logs from a Delphix Engine to a local directory.\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\nOption \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\nDescription\n\n\nRequired\n\n\nDefault\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\n-e,\n--delphix-engine\nTEXT\n\n\nDownload plugin logs from the provided Delphix engine. This should be either the hostname or IP address.\n\n\nY\n\n\nNone\n\n\n\n\n\n\n-c,\n--plugin-config FILE\n\n\nSet the path to plugin config file. This file contains the plugin name to download logs for.\n\n\nN\n\n\nplugin_config.yml\n\n\n\n\n\n\n-u,\n--user\nTEXT\n\n\nAuthenticate to the Delphix Engine with the provided user.\n\n\nY\n\n\nNone\n\n\n\n\n\n\n-d,\n--directory DIRECTORY\n\n\nSpecify the directory of where to download the plugin logs.\n\n\nN\n\n\nos.cwd()\n\n\n\n\n\n\n--password\nTEXT\n\n\nAuthenticate using the provided password. If ommitted, the password will be requested through a secure prompt.\n\n\nN\n\n\nNone\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nDownload plugin logs from \nengine.example.com\n using the user \nadmin\n. Since the password option is ommitted, a secure password prompt is used instead.\n\n\n$ dvp download-logs -e engine.example.com -u admin\nPassword:",
            "title": "CLI"
        },
        {
            "location": "/References/CLI/#cli",
            "text": "The CLI is installed with the SDK. To install the SDK, refer to the  Getting Started  section.",
            "title": "CLI"
        },
        {
            "location": "/References/CLI/#help",
            "text": "Every command has a  -h  flag including the CLI itself. This will print the help menu.",
            "title": "Help"
        },
        {
            "location": "/References/CLI/#examples",
            "text": "Get the CLI's help menu.  $ dvp -h\nUsage: dvp [OPTIONS] COMMAND [ARGS]...\n\n  The tools of the Delphix Virtualization SDK that help develop, build, and\n  upload a plugin.\n\nOptions:\n  --version      Show the version and exit.\n  -v, --verbose  Enable verbose mode. Can be repeated up to three times for\n                 increased verbosity.\n  -q, --quiet    Enable quiet mode. Can be repeated up to three times for\n                 increased suppression.\n  -h, --help     Show this message and exit.\n\nCommands:\n  build   Build the plugin code and generate upload artifact file using the...\n  init    Create a plugin in the root directory.\n  upload  Upload the generated upload artifact (the plugin JSON file) that...  Get the  build  command's help menu.  $ dvp build -h\nUsage: dvp build [OPTIONS]\n\n  Build the plugin code and generate upload artifact file using the\n  configuration provided in the plugin config file.\n\nOptions:\n  -c, --plugin-config FILE    Set the path to plugin config file.This file\n                              contains the configuration required to build the\n                              plugin.  [default: plugin_config.yml]\n  -a, --upload-artifact FILE  Set the upload artifact.The upload artifact file\n                              generated by build process will be writtento\n                              this file and later used by upload command.\n                              [default: artifact.json]\n  -g, --generate-only         Only generate the Python classes from the schema\n                              definitions. Do not do a full build or create an\n                              upload artifact.  [default: False]\n  -h, --help                  Show this message and exit.",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#verbosity",
            "text": "To change the verbosity level of the CLI you can specify up to three  -v  (to increase) or  -q  (to decrease) the amount that is printed to the console. This is an option on the CLI itself and can be used with any command.     Option  Output      -qqq  None    -qq  Critical    -q  Error    -v  Info    -vv  Debug    -vvv  All",
            "title": "Verbosity"
        },
        {
            "location": "/References/CLI/#examples_1",
            "text": "Print everything to the console.  $ dvp -vvv build  Print nothing to the console.  $ dvp -qqq upload -e engine.example.com -u admin",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#commands",
            "text": "",
            "title": "Commands"
        },
        {
            "location": "/References/CLI/#init",
            "text": "",
            "title": "init"
        },
        {
            "location": "/References/CLI/#description",
            "text": "Create a plugin in the root directory. The plugin will be valid but have no functionality.",
            "title": "Description"
        },
        {
            "location": "/References/CLI/#options",
            "text": "Option\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Description  Required  Default\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      -r, --root-dir DIRECTORY  Set the plugin root directory.  N  os.cwd()    -n, --plugin-name TEXT  Set the name of the plugin that will be used to identify it.  N  id    -s, --ingestion-strategy [DIRECT|STAGED]  Set the ingestion strategy of the plugin. A \"direct\" plugin ingests without a staging server while a \"staged\" plugin requires a staging server.  N  DIRECT",
            "title": "Options"
        },
        {
            "location": "/References/CLI/#examples_2",
            "text": "Create a plugin in the current working directory with the  DIRECT  ingestion strategy. Here the name of the plugin will be equal to the id that is generated.  $ dvp init  Create a plugin in the current working directory with the  DIRECT  ingestion strategy and use  postgres  as the display name.  $ dvp init -n postgres  Create a plugin called  mongodb  in a custom location with the  STAGED  ingestion strategy.  $ dvp init -n mongodb -s STAGED",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#build",
            "text": "",
            "title": "build"
        },
        {
            "location": "/References/CLI/#description_1",
            "text": "Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file.",
            "title": "Description"
        },
        {
            "location": "/References/CLI/#options_1",
            "text": "Option \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Description  Required  Default\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      -c, --plugin-config FILE  Set the path to plugin config file.This file contains the configuration required to build the plugin.  N  plugin_config.yml    -a, --upload-artifact FILE  Set the upload artifact.The upload artifact file generated by build process will be written to this file and later used by upload command.  N  artifact.json    -g, --generate-only  Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact.  N  False",
            "title": "Options"
        },
        {
            "location": "/References/CLI/#examples_3",
            "text": "Do a full build of the plugin and write the upload artifact to  ./artifact.json .  This assumes current working directory contains a plugin config file named  plugin_config.yml .  $ dvp build  Do a partial build and just generate the Python classes from the schema definitions.  This assumes current working directory contains ad plugin config file named  plugin_config.yml .  $ dvp build -g  Do a full build of a plugin and write the artifact file to a custom location.  $ dvp build -c config.yml -a build/artifact.json",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#upload",
            "text": "",
            "title": "upload"
        },
        {
            "location": "/References/CLI/#description_2",
            "text": "Upload the generated upload artifact (the plugin JSON file) that was built to a target Delphix Engine. Note that the upload artifact should be the file created after running the build command and will fail if it's not readable or valid.",
            "title": "Description"
        },
        {
            "location": "/References/CLI/#options_2",
            "text": "Option \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Description  Required  Default \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      -e, --delphix-engine TEXT  Upload plugin to the provided engine. This should be either the hostname or IP address.  Y  None    -u, --user TEXT  Authenticate to the Delphix Engine with the provided user.  Y  None    -a, --upload-artifact FILE  Path to the upload artifact that was generated through build.  N  artifact.json    --password TEXT  Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt.  N  None",
            "title": "Options"
        },
        {
            "location": "/References/CLI/#examples_4",
            "text": "Upload artifact  build/artifact.json  to  delphix-engine.domain  using the user  admin . Since the password option is ommitted, a secure password prompt is used instead.  $ dvp upload -a build/artifact -e engine.example.com -u admin\nPassword:",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#download-logs",
            "text": "",
            "title": "download-logs"
        },
        {
            "location": "/References/CLI/#description_3",
            "text": "Download plugin logs from a Delphix Engine to a local directory.",
            "title": "Description"
        },
        {
            "location": "/References/CLI/#options_3",
            "text": "Option \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Description  Required  Default\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      -e, --delphix-engine TEXT  Download plugin logs from the provided Delphix engine. This should be either the hostname or IP address.  Y  None    -c, --plugin-config FILE  Set the path to plugin config file. This file contains the plugin name to download logs for.  N  plugin_config.yml    -u, --user TEXT  Authenticate to the Delphix Engine with the provided user.  Y  None    -d, --directory DIRECTORY  Specify the directory of where to download the plugin logs.  N  os.cwd()    --password TEXT  Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt.  N  None",
            "title": "Options"
        },
        {
            "location": "/References/CLI/#examples_5",
            "text": "Download plugin logs from  engine.example.com  using the user  admin . Since the password option is ommitted, a secure password prompt is used instead.  $ dvp download-logs -e engine.example.com -u admin\nPassword:",
            "title": "Examples"
        },
        {
            "location": "/References/Plugin_Config/",
            "text": "Plugin Config\n\u00b6\n\n\nThe plugin config is a \nYAML\n file that marks the root of a plugin and defines metadata about the plugin and its structure. The config file is read at build time to generate the upload artifact.\n\n\nThe name of the file does not matter and can be specified during the build, but, by default, the build looks for \nplugin_config.yml\n.\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField Name\n\n\nRequired\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nid\n\n\nY\n\n\nstring\n\n\nThe unique id of the plugin. This will be autogenerated by \ndvp init\n.\n\n\n\n\n\n\nname\n\n\nN\n\n\nstring\n\n\nThe display name of the plugin. This will be used in the UI. If it is not specified name will be equal to id.\n\n\n\n\n\n\nversion\n\n\nY\n\n\nstring\n\n\nThe \nplugin's version\n in the format \nx.y.z\n.\n\n\n\n\n\n\nhostTypes\n\n\nY\n\n\nlist\n\n\nThe host type that the plugin supports. Either \nUNIX\n or \nWINDOWS\n.\n\n\n\n\n\n\nschemaFile\n\n\nY\n\n\nstring\n\n\nThe path to the JSON file that contains the \nplugin's schema definitions\n.\nThis path can be absolute or relative to the directory containing the plugin config file.\n\n\n\n\n\n\nsrcDir\n\n\nY\n\n\nstring\n\n\nThe path to the directory that contains the source code for the plugin. During execution of a plugin operation, this directory will be the current working directory of the Python interpreter. Any modules or resources defined outside of this directory will be inaccessible at runtime.\nThis path can be absolute or relative to the directory containing the plugin config file.\n\n\n\n\n\n\nentryPoint\n\n\nY\n\n\nstring\n\n\nA fully qualified Python symbol that points to the \ndlpx.virtualization.platform.Plugin\n object that defines the plugin.\nIt must be in the form \nimportable.module:object_name\n where \nimportable.module\n is in \nsrcDir\n.\n\n\n\n\n\n\nmanualDiscovery\n\n\nN\n\n\nboolean\n\n\nTrue if the plugin supports manual discovery of source config objects. The default value is \ntrue\n.\n\n\n\n\n\n\npluginType\n\n\nY\n\n\nenum\n\n\nThe ingestion strategy of the plugin. Can be either \nSTAGED\n or \nDIRECT\n.\n\n\n\n\n\n\nlanguage\n\n\nY\n\n\nenum\n\n\nMust be \nPYTHON27\n.\n\n\n\n\n\n\ndefaultLocale\n\n\nN\n\n\nenum\n\n\nThe locale to be used by the plugin if the Delphix user does not specify one. Plugin messages will be displayed in this locale by default. The default value is \nen-us\n.\n\n\n\n\n\n\nrootSquashEnabled\n\n\nN\n\n\nboolean\n\n\nThis dictates whether \"root squash\" is enabled on NFS mounts for the plugin (i.e. whether the \nroot\n user on remote hosts has access to the NFS mounts). Setting this to \nfalse\n allows processes usually run as \nroot\n, like Docker daemons, access to the NFS mounts. The default value is \ntrue\n. This field only applies to Unix hosts.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nAssume the following basic plugin structure:\n\n\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 mongo_runner.py\n\n\n\n\n\nmongo_runner.py\n contains:\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\n\nmongodb\n \n=\n \nPlugin\n()\n\n\n\n\n\n\nThis is a valid plugin config for the plugin:\n\n\nid\n:\n \n7cf830f2-82f3-4d5d-a63c-7bbe50c22b32\n\n\nname\n:\n \nMongoDB\n\n\nversion\n:\n \n2.0.0\n\n\nhostTypes\n:\n\n  \n-\n \nUNIX\n\n\nentryPoint\n:\n \nmongo_runner:mongodb\n\n\nsrcDir\n:\n \nsrc/\n\n\nschemaFile\n:\n \nschema.json\n\n\npluginType\n:\n \nDIRECT\n\n\nlanguage\n:\n \nPYTHON27\n\n\n\n\n\n\nThis is a valid plugin config for the plugin with manualDiscovery set to false:\n\n\nid\n:\n \n7cf830f2-82f3-4d5d-a63c-7bbe50c22b32\n\n\nname\n:\n \nMongoDB\n\n\nversion\n:\n \n2.0.0\n\n\nhostTypes\n:\n\n  \n-\n \nUNIX\n\n\nentryPoint\n:\n \nmongo_runner:mongodb\n\n\nsrcDir\n:\n \nsrc/\n\n\nschemaFile\n:\n \nschema.json\n\n\nmanualDiscovery\n:\n \nfalse\n\n\npluginType\n:\n \nDIRECT\n\n\nlanguage\n:\n \nPYTHON27",
            "title": "Plugin Config"
        },
        {
            "location": "/References/Plugin_Config/#plugin-config",
            "text": "The plugin config is a  YAML  file that marks the root of a plugin and defines metadata about the plugin and its structure. The config file is read at build time to generate the upload artifact.  The name of the file does not matter and can be specified during the build, but, by default, the build looks for  plugin_config.yml .",
            "title": "Plugin Config"
        },
        {
            "location": "/References/Plugin_Config/#fields",
            "text": "Field Name  Required  Type  Description      id  Y  string  The unique id of the plugin. This will be autogenerated by  dvp init .    name  N  string  The display name of the plugin. This will be used in the UI. If it is not specified name will be equal to id.    version  Y  string  The  plugin's version  in the format  x.y.z .    hostTypes  Y  list  The host type that the plugin supports. Either  UNIX  or  WINDOWS .    schemaFile  Y  string  The path to the JSON file that contains the  plugin's schema definitions . This path can be absolute or relative to the directory containing the plugin config file.    srcDir  Y  string  The path to the directory that contains the source code for the plugin. During execution of a plugin operation, this directory will be the current working directory of the Python interpreter. Any modules or resources defined outside of this directory will be inaccessible at runtime. This path can be absolute or relative to the directory containing the plugin config file.    entryPoint  Y  string  A fully qualified Python symbol that points to the  dlpx.virtualization.platform.Plugin  object that defines the plugin. It must be in the form  importable.module:object_name  where  importable.module  is in  srcDir .    manualDiscovery  N  boolean  True if the plugin supports manual discovery of source config objects. The default value is  true .    pluginType  Y  enum  The ingestion strategy of the plugin. Can be either  STAGED  or  DIRECT .    language  Y  enum  Must be  PYTHON27 .    defaultLocale  N  enum  The locale to be used by the plugin if the Delphix user does not specify one. Plugin messages will be displayed in this locale by default. The default value is  en-us .    rootSquashEnabled  N  boolean  This dictates whether \"root squash\" is enabled on NFS mounts for the plugin (i.e. whether the  root  user on remote hosts has access to the NFS mounts). Setting this to  false  allows processes usually run as  root , like Docker daemons, access to the NFS mounts. The default value is  true . This field only applies to Unix hosts.",
            "title": "Fields"
        },
        {
            "location": "/References/Plugin_Config/#example",
            "text": "Assume the following basic plugin structure:  \u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 mongo_runner.py  mongo_runner.py  contains:  from   dlpx.virtualization.platform   import   Plugin  mongodb   =   Plugin ()   This is a valid plugin config for the plugin:  id :   7cf830f2-82f3-4d5d-a63c-7bbe50c22b32  name :   MongoDB  version :   2.0.0  hostTypes : \n   -   UNIX  entryPoint :   mongo_runner:mongodb  srcDir :   src/  schemaFile :   schema.json  pluginType :   DIRECT  language :   PYTHON27   This is a valid plugin config for the plugin with manualDiscovery set to false:  id :   7cf830f2-82f3-4d5d-a63c-7bbe50c22b32  name :   MongoDB  version :   2.0.0  hostTypes : \n   -   UNIX  entryPoint :   mongo_runner:mongodb  srcDir :   src/  schemaFile :   schema.json  manualDiscovery :   false  pluginType :   DIRECT  language :   PYTHON27",
            "title": "Example"
        },
        {
            "location": "/References/Decorators/",
            "text": "Decorators\n\u00b6\n\n\nThe Virtualization SDK exposes decorators to be able to annotate functions that correspond to each \nPlugin Operation\n.\nIn the example below, it first instantiates a \nPlugin()\n object, that can then be used to tag plugin operations.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\n# Initialize a plugin object\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n# Use the decorator to annotate the function that corresponds to the \"Virtual Source Start\" Plugin Operation\n\n\n@plugin.virtual_source.start\n()\n\n\ndef\n \nmy_start\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nprint\n \n\"running start\"\n \n\n\n\n\n\n\n\nInfo\n\n\nDecorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses \n()\n appended at the end.\n\n\n\n\nAssuming the name of the object, is \nplugin\n as above, the table below lists the corresponding decorators for each plugin operation.\n\n\n\n\n\n\n\n\nPlugin Operation\n\n\nDecorator\n\n\n\n\n\n\n\n\n\n\nRepository Discovey\n\n\n@plugin.discovery.repository()\n\n\n\n\n\n\nSource Config Discovey\n\n\n@plugin.discovery.source_config()\n\n\n\n\n\n\nDirect Linked Source Pre-Snapshot\n\n\n@plugin.linked.pre_snapshot()\n\n\n\n\n\n\nDirect Linked Source Post-Snapshot\n\n\n@plugin.linked.post_snapshot()\n\n\n\n\n\n\nStaged Linked Source Pre-Snapshot\n\n\n@plugin.linked.pre_snapshot()\n\n\n\n\n\n\nStaged Linked Source Post-Snapshot\n\n\n@plugin.linked.post_snapshot()\n\n\n\n\n\n\nStaged Linked Source Start-Staging\n\n\n@plugin.linked.start_staging()\n\n\n\n\n\n\nStaged Linked Source Stop-Staging\n\n\n@plugin.linked.stop_staging()\n\n\n\n\n\n\nStaged Linked Source Status\n\n\n@plugin.linked.status()\n\n\n\n\n\n\nStaged Linked Source Worker\n\n\n@plugin.linked.worker()\n\n\n\n\n\n\nStaged Linked Source Mount Specification\n\n\n@plugin.linked.mount_specification()\n\n\n\n\n\n\nVirtual Source Configure\n\n\n@plugin.virtual.configure()\n\n\n\n\n\n\nVirtual Source Unconfigure\n\n\n@plugin.virtual.unconfigure()\n\n\n\n\n\n\nVirtual Source Reconfigure\n\n\n@plugin.virtual.reconfigure()\n\n\n\n\n\n\nVirtual Source Start\n\n\n@plugin.virtual.start()\n\n\n\n\n\n\nVirtual Source Stop\n\n\n@plugin.virtual.stop()\n\n\n\n\n\n\nVirtualSource Pre-Snapshot\n\n\n@plugin.virtual.pre_snapshot()\n\n\n\n\n\n\nVirtual Source Post-Snapshot\n\n\n@plugin.virtual.post_snapshot()\n\n\n\n\n\n\nVirtual Source Mount Specification\n\n\n@plugin.virtual.mount_specification()\n\n\n\n\n\n\nVirtual Source Status\n\n\n@plugin.virtual.status()\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nA plugin should only implement the \ndirect\n operations or the \nstaged\n operations based on the \nplugin type",
            "title": "Decorators"
        },
        {
            "location": "/References/Decorators/#decorators",
            "text": "The Virtualization SDK exposes decorators to be able to annotate functions that correspond to each  Plugin Operation .\nIn the example below, it first instantiates a  Plugin()  object, that can then be used to tag plugin operations.  from   dlpx.virtualization.platform   import   Plugin  # Initialize a plugin object  plugin   =   Plugin ()  # Use the decorator to annotate the function that corresponds to the \"Virtual Source Start\" Plugin Operation  @plugin.virtual_source.start ()  def   my_start ( virtual_source ,   repository ,   source_config ): \n   print   \"running start\"     Info  Decorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses  ()  appended at the end.   Assuming the name of the object, is  plugin  as above, the table below lists the corresponding decorators for each plugin operation.     Plugin Operation  Decorator      Repository Discovey  @plugin.discovery.repository()    Source Config Discovey  @plugin.discovery.source_config()    Direct Linked Source Pre-Snapshot  @plugin.linked.pre_snapshot()    Direct Linked Source Post-Snapshot  @plugin.linked.post_snapshot()    Staged Linked Source Pre-Snapshot  @plugin.linked.pre_snapshot()    Staged Linked Source Post-Snapshot  @plugin.linked.post_snapshot()    Staged Linked Source Start-Staging  @plugin.linked.start_staging()    Staged Linked Source Stop-Staging  @plugin.linked.stop_staging()    Staged Linked Source Status  @plugin.linked.status()    Staged Linked Source Worker  @plugin.linked.worker()    Staged Linked Source Mount Specification  @plugin.linked.mount_specification()    Virtual Source Configure  @plugin.virtual.configure()    Virtual Source Unconfigure  @plugin.virtual.unconfigure()    Virtual Source Reconfigure  @plugin.virtual.reconfigure()    Virtual Source Start  @plugin.virtual.start()    Virtual Source Stop  @plugin.virtual.stop()    VirtualSource Pre-Snapshot  @plugin.virtual.pre_snapshot()    Virtual Source Post-Snapshot  @plugin.virtual.post_snapshot()    Virtual Source Mount Specification  @plugin.virtual.mount_specification()    Virtual Source Status  @plugin.virtual.status()      Warning  A plugin should only implement the  direct  operations or the  staged  operations based on the  plugin type",
            "title": "Decorators"
        },
        {
            "location": "/References/Plugin_Operations/",
            "text": "Plugin Operations\n\u00b6\n\n\n\n\nWarning\n\n\nIf a Plugin Operations is \nRequired\n and is not present, the corresponding Delphix Engine Operation will fail when invoked. The plugin can still be built and uploaded to the Delphix Engine.\n\n\n\n\n\n\nWarning\n\n\nFor each operation, the argument names must match exactly. For example, the Repository Discovery\noperation must have a single argument named \nsource_connection\n.\n\n\n\n\n\n\n\n\n\n\nPlugin Operation\n\n\nRequired\n\n\nDecorator\n\n\nDelphix Engine Operations\n\n\n\n\n\n\n\n\n\n\nRepository\nDiscovery\n\n\nYes\n\n\ndiscovery.repository()\n\n\nEnvironment Discovery\nEnvironment Refresh\n\n\n\n\n\n\nSource Config\nDiscovery\n\n\nYes\n\n\ndiscovery.source_config()\n\n\nEnvironment Discovery\nEnvironment Refresh\n\n\n\n\n\n\nDirect Linked Source\nPre-Snapshot\n\n\nNo\n\n\nlinked.pre_snapshot()\n\n\nLinked Source Sync\n\n\n\n\n\n\nDirect Linked Source\nPost-Snapshot\n\n\nYes\n\n\nlinked.post_snapshot()\n\n\nLinked Source Sync\n\n\n\n\n\n\nStaged Linked Source\nPre-Snapshot\n\n\nNo\n\n\nlinked.pre_snapshot()\n\n\nLinked Source Sync\n\n\n\n\n\n\nStaged Linked Source\nPost-Snapshot\n\n\nYes\n\n\nlinked.post_snapshot()\n\n\nLinked Source Sync\n\n\n\n\n\n\nStaged Linked Source\nStart-Staging\n\n\nNo\n\n\nlinked.start_staging()\n\n\nLinked Source Enable\n\n\n\n\n\n\nStaged Linked Source\nStop-Staging\n\n\nNo\n\n\nlinked.stop_staging()\n\n\nLinked Source Disable\nLinked Source Delete\n\n\n\n\n\n\nStaged Linked Source\nStatus\n\n\nNo\n\n\nlinked.status()\n\n\nN/A\n\n\n\n\n\n\nStaged Linked Source\nWorker\n\n\nNo\n\n\nlinked.worker()\n\n\nN/A\n\n\n\n\n\n\nStaged Linked Source\nMount Specification\n\n\nYes\n\n\nlinked.mount_specification()\n\n\nLinked Source Sync\nLinked Source Enable\n\n\n\n\n\n\nVirtual Source\nConfigure\n\n\nYes\n\n\nvirtual.configure()\n\n\nVirtual Source Provision\nVirtual Source Refresh\n\n\n\n\n\n\nVirtual Source\nUnconfigure\n\n\nNo\n\n\nvirtual.unconfigure()\n\n\nVirtual Source Refresh\nVirtual Source Delete\n\n\n\n\n\n\nVirtual Source\nReconfigure\n\n\nYes\n\n\nvirtual.reconfigure()\n\n\nVirtual Source Rollback\nVirtual Source Enable\n\n\n\n\n\n\nVirtual Source\nStart\n\n\nNo\n\n\nvirtual.start()\n\n\nVirtual Source Start\n\n\n\n\n\n\nVirtual Source\nStop\n\n\nNo\n\n\nvirtual.stop()\n\n\nVirtual Source Stop\n\n\n\n\n\n\nVirtual Source\nPre-Snapshot\n\n\nNo\n\n\nvirtual.pre_snapshot()\n\n\nVirtual Source Snapshot\n\n\n\n\n\n\nVirtual Source\nPost-Snapshot\n\n\nYes\n\n\nvirtual.post_snapshot()\n\n\nVirtual Source Snapshot\n\n\n\n\n\n\nVirtual Source\nMount Specification\n\n\nYes\n\n\nvirtual.mount_specification()\n\n\nVirtual Source Enable\nVirtual Source Provision\nVirtual Source Refresh\nVirtual Source Rollback\nVirtual Source Start\n\n\n\n\n\n\nVirtual Source\nStatus\n\n\nNo\n\n\nvirtual.status()\n\n\nVirtual Source Enable\n\n\n\n\n\n\n\n\nRepository Discovery\n\u00b6\n\n\nDiscovers the set of \nrepositories\n for a plugin on an \nenvironment\n. For a DBMS, this can correspond to the set of binaries installed on a Unix host.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nEnvironment Refresh\n\n\nEnvironment Discovery\n\n\n\n\nSignature\n\u00b6\n\n\ndef repository_discovery(source_connection)\n\n\nDecorator\n\u00b6\n\n\ndiscovery.repository()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource_connection\n\n\nRemoteConnection\n\n\nThe connection associated with the remote environment to run repository discovery\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nA list of \nRepositoryDefinition\n objects.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n  \n  \nrepository\n \n=\n \nRepositoryDefinition\n()\n\n  \nrepository\n.\ninstallPath\n \n=\n \n\"/usr/bin/install\"\n\n  \nrepository\n.\nversion\n \n=\n \n\"1.2.3\"\n\n  \nreturn\n \n[\nrepository\n]\n\n\n\n\n\n\n\n\nThe above command assumes a \nRepository Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"installPath\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"version\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"installPath\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"installPath\"\n]\n    \n\n}\n\n\n\n\n\n\nSource Config Discovery\n\u00b6\n\n\nDiscovers the set of \nsource configs\n for a plugin for a \nrepository\n. For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nEnvironment Refresh\n\n\nEnvironment Discovery\n\n\n\n\nSignature\n\u00b6\n\n\ndef source_config_discovery(source_connection, repository)\n\n\nDecorator\n\u00b6\n\n\ndiscovery.source_config()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource_connection\n\n\nRemoteConnection\n\n\nThe connection to the remote environment the corresponds to the repository.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository to discover source configs for.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nA list of \nSourceConfigDefinition\n objects.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n  \nsource_config\n \n=\n \nSourceConfigDefinition\n()\n\n  \nsource_config\n.\nname\n \n=\n \n\"my_name\"\n\n  \nsource_config\n.\nport\n \n=\n \n10000\n\n  \nreturn\n \n[\nsource_config\n]\n\n\n\n\n\n\n\n\nThe above command assumes a \nSource Config Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"number\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n    \n\n}\n\n\n\n\n\n\nDirect Linked Source Pre-Snapshot\n\u00b6\n\n\nSets up a \ndSource\n to ingest data. Only applies when using a \nDirect Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_pre_snapshot(direct_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.pre_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndirect_source\n\n\nDirectSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot\n(\ndirect_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nDirect Linked Source Post-Snapshot\n\u00b6\n\n\nCaptures metadata from a \ndSource\n once data has been ingested. Only applies when using a \nDirect Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_post_snapshot(direct_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.post_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndirect_source\n\n\nDirectSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSnapshotDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot\n(\ndirect_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nsnapshot\n \n=\n \nSnapshotDefinition\n()\n\n  \nsnapshot\n.\ntransaction_id\n \n=\n \n1000\n\n  \nreturn\n \nsnapshot\n\n\n\n\n\n\n\n\nThe above command assumes a \nSnapshot Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"transactionId\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nStaged Linked Source Pre-Snapshot\n\u00b6\n\n\nSets up a \ndSource\n to ingest data. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_pre_snapshot(staged_source, repository, source_config, snapshot_parameters)\n\n\nDecorator\n\u00b6\n\n\nlinked.pre_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\nsnapshot_parameters\n\n\nSnapshotParametersDefinition\n\n\nThe snapshot parameters.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \npass\n\n\n\n\n\n\nStaged Linked Source Post-Snapshot\n\u00b6\n\n\nCaptures metadata from a \ndSource\n once data has been ingested. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_post_snapshot(staged_source, repository, source_config, snapshot_parameters)\n\n\nDecorator\n\u00b6\n\n\nlinked.post_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\nsnapshot_parameters\n\n\nSnapshotParametersDefinition\n\n\nThe snapshot parameters.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSnapshotDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \nsnapshot\n \n=\n \nSnapshotDefinition\n()\n\n  \nif\n \nsnapshot_parameters\n.\nresync\n:\n\n    \nsnapshot\n.\ntransaction_id\n \n=\n \n1000\n\n  \nelse\n:\n\n    \nsnapshot\n.\ntransaction_id\n \n=\n \n10\n\n  \nreturn\n \nsnapshot\n\n\n\n\n\n\n\n\nThe above command assumes a \nSnapshot Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"transactionId\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nStaged Linked Source Start-Staging\n\u00b6\n\n\nSets up a \nStaging Source\n to ingest data. Only applies when using a \nStaged Linking\n strategy.\nRequired to implement for Delphix Engine operations:\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Enable\n\n\n\n\nSignature\n\u00b6\n\n\ndef start_staging(staged_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.start_staging()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.start_staging\n()\n\n\ndef\n \nstart_staging\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nStaged Linked Source Stop-Staging\n\u00b6\n\n\nQuiesces a \nStaging Source\n to pause ingestion. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Disable\n\n\nLinked Source Delete\n\n\n\n\nSignature\n\u00b6\n\n\ndef stop_staging(staged_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.stop_staging()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExamples\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.stop_staging\n()\n\n\ndef\n \nstop_staging\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nStaged Linked Source Status\n\u00b6\n\n\nDetermines the status of a \nStaging Source\n to show end users whether it is healthy or not. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\nIf not implemented, the platform assumes that the status is \nStatus.ACTIVE\n\n\nDelphix Engine Operations\n\u00b6\n\n\nN/A\n\n\nSignature\n\u00b6\n\n\ndef linked_status(staged_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.status()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nStatus\n\n\nStatus.ACTIVE\n if the plugin operation is not implemented.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nStatus\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.status\n()\n\n\ndef\n \nlinked_status\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nreturn\n \nStatus\n.\nACTIVE\n\n\n\n\n\n\nStaged Linked Source Worker\n\u00b6\n\n\nMonitors the status of a \nStaging Source\n on a reqular interval. It can be used to fix up any errors on staging if it is not functioning as expected. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\nN/A\n\n\nSignature\n\u00b6\n\n\ndef worker(staged_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.worker()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.worker\n()\n\n\ndef\n \nworker\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nStaged Linked Source Mount Specification\n\u00b6\n\n\nReturns configurations for the mounts associated for data in staged source. The \nownership_specification\n is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\nLinked Source Enable\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_mount_specification(staged_source, repository)\n\n\nDecorator\n\u00b6\n\n\nlinked.mount_specification()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nMountSpecification\n\n\nExample\n\u00b6\n\n\n\n\nInfo\n\n\nownership_specification\n only applies to Unix hosts.\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMount\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMountSpecification\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nOwenershipSpecification\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.mount_specification\n()\n\n\ndef\n \nlinked_mount_specification\n(\nstaged_source\n,\n \nrepository\n):\n\n  \nmount\n \n=\n \nMount\n(\nstaged_source\n.\nstaged_connection\n.\nenvironment\n,\n \n\"/some/path\"\n)\n\n  \nownership_spec\n \n=\n \nOwenershipSpecification\n(\nrepository\n.\nuid\n,\n \nrepository\n.\ngid\n)\n\n\n  \nreturn\n \nMountSpecification\n([\nmount\n],\n \nownership_spec\n)\n\n\n\n\n\n\n\n\nThe above command assumes a \nRepository Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"uid\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n},\n\n    \n\"gid\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nVirtual Source Configure\n\u00b6\n\n\nConfigures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Provision\n\n\nVirtual Source Refresh\n\n\n\n\nSignature\n\u00b6\n\n\ndef configure(virtual_source, snapshot, repository)\n\n\nDecorator\n\u00b6\n\n\nvirtual.configure()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nsnapshot\n\n\nSnapshotDefinition\n\n\nThe snapshot of the data set to configure.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSourceConfigDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.configure\n()\n\n\ndef\n \nconfigure\n(\nvirtual_source\n,\n \nrepository\n,\n \nsnapshot\n):\n\n  \nname\n \n=\n \n\"config_name\"\n\n  \nsource_config\n \n=\n \nSourceConfigDefinition\n()\n\n  \nsource_config\n.\nname\n \n=\n \nname\n\n  \nreturn\n \nsource_config\n\n\n\n\n\n\n\n\nThe above command assumes a \nSourceConfig Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n\n\n}\n\n\n\n\n\n\nVirtual Source Unconfigure\n\u00b6\n\n\nQuiesces the virtual source on a target environment. For database data files, shutting down and unregistering a database on a host.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Refresh\n\n\nVirtual Source Delete\n\n\n\n\nSignature\n\u00b6\n\n\ndef unconfigure(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.unconfigure()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.unconfigure\n()\n\n\ndef\n \nunconfigure\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nVirtual Source Reconfigure\n\u00b6\n\n\nRe-configures the data for a virtual source to point to the data in a prior snapshot for the virtual source. For database data files, this may mean recovering from a crash consistent format or backup of a new snapshot. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Rollback\n\n\nVirtual Source Enable\n\n\n\n\nSignature\n\u00b6\n\n\ndef reconfigure(virtual_source, repository, source_config, snapshot)\n\n\nDecorator\n\u00b6\n\n\nvirtual.reconfigure()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nsnapshot\n\n\nSnapshotDefinition\n\n\nThe snapshot of the data set to configure.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSourceConfigDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.reconfigure\n()\n\n\ndef\n \nconfigure\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot\n):\n\n  \nname\n \n=\n \n\"updated_config_name\"\n\n  \nsource_config\n \n=\n \nSourceConfigDefinition\n()\n\n  \nsource_config\n.\nname\n \n=\n \nname\n\n  \nreturn\n \nsource_config\n\n\n\n\n\n\n\n\nThe above command assumes a \nSourceConfig Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n\n\n}\n\n\n\n\n\n\nVirtual Source Start\n\u00b6\n\n\nExecuted whenever the data should be placed in a \"running\" state.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Start\n\n\n\n\nSignature\n\u00b6\n\n\ndef start(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.start()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.start\n()\n\n\ndef\n \nstart\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nVirtual Source Stop\n\u00b6\n\n\nExecuted whenever the data needs to be shut down.\nRequired to implement for Delphix Engine operations:\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Stop\n\n\n\n\nSignature\n\u00b6\n\n\ndef stop(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.stop()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.stop\n()\n\n\ndef\n \nstop\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nVirtual Source Pre-Snapshot\n\u00b6\n\n\nPrepares the virtual source for taking a snapshot of the data.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Snapshot\n\n\n\n\nSignature\n\u00b6\n\n\ndef virtual_pre_snapshot(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.pre_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.pre_snapshot\n()\n\n\ndef\n \nvirtual_pre_snapshot\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nVirtual Source Post-Snapshot\n\u00b6\n\n\nCaptures metadata after a snapshot.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Snapshot\n\n\n\n\nSignature\n\u00b6\n\n\ndef virtual_post_snapshot(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.post_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSnapshotDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.post_snapshot\n()\n\n\ndef\n \nvirtual_post_snapshot\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nsnapshot\n \n=\n \nSnapshotDefinition\n()\n\n  \nsnapshot\n.\ntransaction_id\n \n=\n \n1000\n\n  \nreturn\n \nsnapshot\n\n\n\n\n\n\n\n\nThe above command assumes a \nSnapshot Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"transactionId\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nVirtual Source Mount Specification\n\u00b6\n\n\nReturns configurations for the mounts associated for data in virtual source.\nThe \nownership_specification\n is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Enable\n\n\nVirtual Source Provision\n\n\nVirtual Source Refresh\n\n\nVirtual Source Rollback\n\n\nVirtual Source Start\n\n\n\n\nSignature\n\u00b6\n\n\ndef virtual_mount_specification(virtual_source, repository)\n\n\nDecorator\n\u00b6\n\n\nvirtual.mount_specification()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nMountSpecification\n\n\nExample\n\u00b6\n\n\n\n\nInfo\n\n\nownership_specification\n only applies to Unix hosts.\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMount\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMountSpecification\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nOwenershipSpecification\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.mount_specification\n()\n\n\ndef\n \nvirtual_mount_specification\n(\nvirtual_source\n,\n \nrepository\n):\n\n  \nmount\n \n=\n \nMount\n(\nvirtual_source\n.\nconnection\n.\nenvironment\n,\n \n\"/some/path\"\n)\n\n  \nownership_spec\n \n=\n \nOwenershipSpecification\n(\nrepository\n.\nuid\n,\n \nrepository\n.\ngid\n)\n\n\n  \nreturn\n \nMountSpecification\n([\nmount\n],\n \nownership_spec\n)\n\n\n\n\n\n\n\n\nThe above command assumes a \nRepository Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"uid\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n},\n\n    \n\"gid\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nVirtual Source Status\n\u00b6\n\n\nDetermines the status of a \nVirtual Source\n to show end users whether it is healthy or not.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\nIf not implemented, the platform assumes that the status is \nStatus.ACTIVE\n.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Enable\n\n\n\n\nSignature\n\u00b6\n\n\ndef virtual_status(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.status()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nStatus\n\n\nStatus.ACTIVE\n if the plugin operation is not implemented.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nStatus\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.status\n()\n\n\ndef\n \nvirtual_status\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nreturn\n \nStatus\n.\nACTIVE",
            "title": "Plugin Operations"
        },
        {
            "location": "/References/Plugin_Operations/#plugin-operations",
            "text": "Warning  If a Plugin Operations is  Required  and is not present, the corresponding Delphix Engine Operation will fail when invoked. The plugin can still be built and uploaded to the Delphix Engine.    Warning  For each operation, the argument names must match exactly. For example, the Repository Discovery\noperation must have a single argument named  source_connection .      Plugin Operation  Required  Decorator  Delphix Engine Operations      Repository Discovery  Yes  discovery.repository()  Environment Discovery Environment Refresh    Source Config Discovery  Yes  discovery.source_config()  Environment Discovery Environment Refresh    Direct Linked Source Pre-Snapshot  No  linked.pre_snapshot()  Linked Source Sync    Direct Linked Source Post-Snapshot  Yes  linked.post_snapshot()  Linked Source Sync    Staged Linked Source Pre-Snapshot  No  linked.pre_snapshot()  Linked Source Sync    Staged Linked Source Post-Snapshot  Yes  linked.post_snapshot()  Linked Source Sync    Staged Linked Source Start-Staging  No  linked.start_staging()  Linked Source Enable    Staged Linked Source Stop-Staging  No  linked.stop_staging()  Linked Source Disable Linked Source Delete    Staged Linked Source Status  No  linked.status()  N/A    Staged Linked Source Worker  No  linked.worker()  N/A    Staged Linked Source Mount Specification  Yes  linked.mount_specification()  Linked Source Sync Linked Source Enable    Virtual Source Configure  Yes  virtual.configure()  Virtual Source Provision Virtual Source Refresh    Virtual Source Unconfigure  No  virtual.unconfigure()  Virtual Source Refresh Virtual Source Delete    Virtual Source Reconfigure  Yes  virtual.reconfigure()  Virtual Source Rollback Virtual Source Enable    Virtual Source Start  No  virtual.start()  Virtual Source Start    Virtual Source Stop  No  virtual.stop()  Virtual Source Stop    Virtual Source Pre-Snapshot  No  virtual.pre_snapshot()  Virtual Source Snapshot    Virtual Source Post-Snapshot  Yes  virtual.post_snapshot()  Virtual Source Snapshot    Virtual Source Mount Specification  Yes  virtual.mount_specification()  Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start    Virtual Source Status  No  virtual.status()  Virtual Source Enable",
            "title": "Plugin Operations"
        },
        {
            "location": "/References/Plugin_Operations/#repository-discovery",
            "text": "Discovers the set of  repositories  for a plugin on an  environment . For a DBMS, this can correspond to the set of binaries installed on a Unix host.",
            "title": "Repository Discovery"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations",
            "text": "Environment Refresh  Environment Discovery",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature",
            "text": "def repository_discovery(source_connection)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator",
            "text": "discovery.repository()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments",
            "text": "Argument  Type  Description      source_connection  RemoteConnection  The connection associated with the remote environment to run repository discovery",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns",
            "text": "A list of  RepositoryDefinition  objects.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.defintions   import   RepositoryDefinition  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ):   \n   repository   =   RepositoryDefinition () \n   repository . installPath   =   \"/usr/bin/install\" \n   repository . version   =   \"1.2.3\" \n   return   [ repository ]    The above command assumes a  Repository Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"installPath\" :   {   \"type\" :   \"string\"   }, \n     \"version\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"installPath\" ], \n   \"nameField\" :   [ \"installPath\" ]      }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#source-config-discovery",
            "text": "Discovers the set of  source configs  for a plugin for a  repository . For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host.",
            "title": "Source Config Discovery"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_1",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_1",
            "text": "Environment Refresh  Environment Discovery",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_1",
            "text": "def source_config_discovery(source_connection, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_1",
            "text": "discovery.source_config()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_1",
            "text": "Argument  Type  Description      source_connection  RemoteConnection  The connection to the remote environment the corresponds to the repository.    repository  RepositoryDefinition  The repository to discover source configs for.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_1",
            "text": "A list of  SourceConfigDefinition  objects.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_1",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SourceConfigDefinition  plugin   =   Plugin ()  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n   source_config   =   SourceConfigDefinition () \n   source_config . name   =   \"my_name\" \n   source_config . port   =   10000 \n   return   [ source_config ]    The above command assumes a  Source Config Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"number\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]      }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#direct-linked-source-pre-snapshot",
            "text": "Sets up a  dSource  to ingest data. Only applies when using a  Direct Linking  strategy.",
            "title": "Direct Linked Source Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_2",
            "text": "Optional",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_2",
            "text": "Linked Source Sync",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_2",
            "text": "def linked_pre_snapshot(direct_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_2",
            "text": "linked.pre_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_2",
            "text": "Argument  Type  Description      direct_source  DirectSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_2",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_2",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SourceConfigDefinition  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot ( direct_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#direct-linked-source-post-snapshot",
            "text": "Captures metadata from a  dSource  once data has been ingested. Only applies when using a  Direct Linking  strategy.",
            "title": "Direct Linked Source Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_3",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_3",
            "text": "Linked Source Sync",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_3",
            "text": "def linked_post_snapshot(direct_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_3",
            "text": "linked.post_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_3",
            "text": "Argument  Type  Description      direct_source  DirectSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_3",
            "text": "SnapshotDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_3",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.linked.post_snapshot ()  def   linked_post_snapshot ( direct_source ,   repository ,   source_config ): \n   snapshot   =   SnapshotDefinition () \n   snapshot . transaction_id   =   1000 \n   return   snapshot    The above command assumes a  Snapshot Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"transactionId\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-pre-snapshot",
            "text": "Sets up a  dSource  to ingest data. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_4",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_4",
            "text": "Linked Source Sync",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_4",
            "text": "def linked_pre_snapshot(staged_source, repository, source_config, snapshot_parameters)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_4",
            "text": "linked.pre_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_4",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.    snapshot_parameters  SnapshotParametersDefinition  The snapshot parameters.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_4",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_4",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-post-snapshot",
            "text": "Captures metadata from a  dSource  once data has been ingested. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_5",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_5",
            "text": "Linked Source Sync",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_5",
            "text": "def linked_post_snapshot(staged_source, repository, source_config, snapshot_parameters)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_5",
            "text": "linked.post_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_5",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.    snapshot_parameters  SnapshotParametersDefinition  The snapshot parameters.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_5",
            "text": "SnapshotDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_5",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.linked.post_snapshot ()  def   linked_post_snapshot ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   snapshot   =   SnapshotDefinition () \n   if   snapshot_parameters . resync : \n     snapshot . transaction_id   =   1000 \n   else : \n     snapshot . transaction_id   =   10 \n   return   snapshot    The above command assumes a  Snapshot Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"transactionId\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-start-staging",
            "text": "Sets up a  Staging Source  to ingest data. Only applies when using a  Staged Linking  strategy.\nRequired to implement for Delphix Engine operations:",
            "title": "Staged Linked Source Start-Staging"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_6",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_6",
            "text": "Linked Source Enable",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_6",
            "text": "def start_staging(staged_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_6",
            "text": "linked.start_staging()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_6",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_6",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_6",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.start_staging ()  def   start_staging ( staged_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-stop-staging",
            "text": "Quiesces a  Staging Source  to pause ingestion. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Stop-Staging"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_7",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_7",
            "text": "Linked Source Disable  Linked Source Delete",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_7",
            "text": "def stop_staging(staged_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_7",
            "text": "linked.stop_staging()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_7",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_7",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#examples",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.stop_staging ()  def   stop_staging ( staged_source ,   repository ,   source_config ): \n   pass",
            "title": "Examples"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-status",
            "text": "Determines the status of a  Staging Source  to show end users whether it is healthy or not. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Status"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_8",
            "text": "Optional. \nIf not implemented, the platform assumes that the status is  Status.ACTIVE",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_8",
            "text": "N/A",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_8",
            "text": "def linked_status(staged_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_8",
            "text": "linked.status()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_8",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_8",
            "text": "Status  Status.ACTIVE  if the plugin operation is not implemented.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_7",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform   import   Status  plugin   =   Plugin ()  @plugin.linked.status ()  def   linked_status ( staged_source ,   repository ,   source_config ): \n   return   Status . ACTIVE",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-worker",
            "text": "Monitors the status of a  Staging Source  on a reqular interval. It can be used to fix up any errors on staging if it is not functioning as expected. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Worker"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_9",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_9",
            "text": "N/A",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_9",
            "text": "def worker(staged_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_9",
            "text": "linked.worker()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_9",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_9",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_8",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.worker ()  def   worker ( staged_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-mount-specification",
            "text": "Returns configurations for the mounts associated for data in staged source. The  ownership_specification  is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.",
            "title": "Staged Linked Source Mount Specification"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_10",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_10",
            "text": "Linked Source Sync  Linked Source Enable",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_10",
            "text": "def linked_mount_specification(staged_source, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_10",
            "text": "linked.mount_specification()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_10",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_10",
            "text": "MountSpecification",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_9",
            "text": "Info  ownership_specification  only applies to Unix hosts.   from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform   import   Mount  from   dlpx.virtualization.platform   import   MountSpecification  from   dlpx.virtualization.platform   import   OwenershipSpecification  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.linked.mount_specification ()  def   linked_mount_specification ( staged_source ,   repository ): \n   mount   =   Mount ( staged_source . staged_connection . environment ,   \"/some/path\" ) \n   ownership_spec   =   OwenershipSpecification ( repository . uid ,   repository . gid ) \n\n   return   MountSpecification ([ mount ],   ownership_spec )    The above command assumes a  Repository Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"uid\" :   {   \"type\" :   \"integer\"   }, \n     \"gid\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-configure",
            "text": "Configures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.",
            "title": "Virtual Source Configure"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_11",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_11",
            "text": "Virtual Source Provision  Virtual Source Refresh",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_11",
            "text": "def configure(virtual_source, snapshot, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_11",
            "text": "virtual.configure()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_11",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    snapshot  SnapshotDefinition  The snapshot of the data set to configure.    repository  RepositoryDefinition  The repository associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_11",
            "text": "SourceConfigDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_10",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.defintions   import   SourceConfigDefinition  plugin   =   Plugin ()  @plugin.virtual.configure ()  def   configure ( virtual_source ,   repository ,   snapshot ): \n   name   =   \"config_name\" \n   source_config   =   SourceConfigDefinition () \n   source_config . name   =   name \n   return   source_config    The above command assumes a  SourceConfig Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-unconfigure",
            "text": "Quiesces the virtual source on a target environment. For database data files, shutting down and unregistering a database on a host.",
            "title": "Virtual Source Unconfigure"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_12",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_12",
            "text": "Virtual Source Refresh  Virtual Source Delete",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_12",
            "text": "def unconfigure(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_12",
            "text": "virtual.unconfigure()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_12",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_12",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_11",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.unconfigure ()  def   unconfigure ( virtual_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-reconfigure",
            "text": "Re-configures the data for a virtual source to point to the data in a prior snapshot for the virtual source. For database data files, this may mean recovering from a crash consistent format or backup of a new snapshot. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.",
            "title": "Virtual Source Reconfigure"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_13",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_13",
            "text": "Virtual Source Rollback  Virtual Source Enable",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_13",
            "text": "def reconfigure(virtual_source, repository, source_config, snapshot)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_13",
            "text": "virtual.reconfigure()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_13",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    snapshot  SnapshotDefinition  The snapshot of the data set to configure.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_13",
            "text": "SourceConfigDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_12",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SourceConfigDefinition  plugin   =   Plugin ()  @plugin.virtual.reconfigure ()  def   configure ( virtual_source ,   repository ,   source_config ,   snapshot ): \n   name   =   \"updated_config_name\" \n   source_config   =   SourceConfigDefinition () \n   source_config . name   =   name \n   return   source_config    The above command assumes a  SourceConfig Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-start",
            "text": "Executed whenever the data should be placed in a \"running\" state.",
            "title": "Virtual Source Start"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_14",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_14",
            "text": "Virtual Source Start",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_14",
            "text": "def start(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_14",
            "text": "virtual.start()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_14",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_14",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_13",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.start ()  def   start ( virtual_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-stop",
            "text": "Executed whenever the data needs to be shut down.\nRequired to implement for Delphix Engine operations:",
            "title": "Virtual Source Stop"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_15",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_15",
            "text": "Virtual Source Stop",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_15",
            "text": "def stop(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_15",
            "text": "virtual.stop()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_15",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_15",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_14",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.stop ()  def   stop ( virtual_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-pre-snapshot",
            "text": "Prepares the virtual source for taking a snapshot of the data.",
            "title": "Virtual Source Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_16",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_16",
            "text": "Virtual Source Snapshot",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_16",
            "text": "def virtual_pre_snapshot(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_16",
            "text": "virtual.pre_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_16",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_16",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_15",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.pre_snapshot ()  def   virtual_pre_snapshot ( virtual_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-post-snapshot",
            "text": "Captures metadata after a snapshot.",
            "title": "Virtual Source Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_17",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_17",
            "text": "Virtual Source Snapshot",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_17",
            "text": "def virtual_post_snapshot(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_17",
            "text": "virtual.post_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_17",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_17",
            "text": "SnapshotDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_16",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.defintions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.virtual.post_snapshot ()  def   virtual_post_snapshot ( virtual_source ,   repository ,   source_config ): \n   snapshot   =   SnapshotDefinition () \n   snapshot . transaction_id   =   1000 \n   return   snapshot    The above command assumes a  Snapshot Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"transactionId\" :   {   \"type\" :   \"string\"   } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-mount-specification",
            "text": "Returns configurations for the mounts associated for data in virtual source.\nThe  ownership_specification  is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.",
            "title": "Virtual Source Mount Specification"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_18",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_18",
            "text": "Virtual Source Enable  Virtual Source Provision  Virtual Source Refresh  Virtual Source Rollback  Virtual Source Start",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_18",
            "text": "def virtual_mount_specification(virtual_source, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_18",
            "text": "virtual.mount_specification()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_18",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_18",
            "text": "MountSpecification",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_17",
            "text": "Info  ownership_specification  only applies to Unix hosts.   from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform   import   Mount  from   dlpx.virtualization.platform   import   MountSpecification  from   dlpx.virtualization.platform   import   OwenershipSpecification  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.virtual.mount_specification ()  def   virtual_mount_specification ( virtual_source ,   repository ): \n   mount   =   Mount ( virtual_source . connection . environment ,   \"/some/path\" ) \n   ownership_spec   =   OwenershipSpecification ( repository . uid ,   repository . gid ) \n\n   return   MountSpecification ([ mount ],   ownership_spec )    The above command assumes a  Repository Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"uid\" :   {   \"type\" :   \"integer\"   }, \n     \"gid\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-status",
            "text": "Determines the status of a  Virtual Source  to show end users whether it is healthy or not.",
            "title": "Virtual Source Status"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_19",
            "text": "Optional. \nIf not implemented, the platform assumes that the status is  Status.ACTIVE .",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_19",
            "text": "Virtual Source Enable",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_19",
            "text": "def virtual_status(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_19",
            "text": "virtual.status()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_19",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_19",
            "text": "Status  Status.ACTIVE  if the plugin operation is not implemented.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_18",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform   import   Status  plugin   =   Plugin ()  @plugin.virtual.status ()  def   virtual_status ( virtual_source ,   repository ,   source_config ): \n   return   Status . ACTIVE",
            "title": "Example"
        },
        {
            "location": "/References/Schemas/",
            "text": "Schemas\n\u00b6\n\n\nAbout Schemas\n\u00b6\n\n\nAny time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data:\n\n\n\n\nWhat is the set of data needed and what should they be called?\n\n\nWhat is the type of each piece of data: Strings? Integers? Booleans?\n\n\n\n\nPlugins use \nschemas\n to describe the format of such data. Once a schema is defined, it is used in three ways\n\n\n\n\nIt tells the Delphix Engine how to store the data for later use.\n\n\nIt is used to autogenerate a custom user interface, and to validate user inputs.\n\n\nIt is used to \nautogenerate Python classes\n that can be used by plugin code to access and manipulate user input and stored data.\n\n\n\n\nThere are five plugin-customizable data formats:\n\n\n\n\n\n\n\n\nDelphix Object\n\n\nSchema\n\n\nAutogenerated Class\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nRepositoryDefinition\n\n\nRepositoryDefinition\n\n\n\n\n\n\nSource Config\n\n\nSourceConfigDefinition\n\n\nSourceConfigDefinition\n\n\n\n\n\n\nLinked Source\n\n\nLinkedSourceDefinition\n\n\nLinkedSourceDefinition\n\n\n\n\n\n\nVirtual Source\n\n\nVirtualSourceDefinition\n\n\nVirtualSourceDefinition\n\n\n\n\n\n\nSnapshot\n\n\nSnapshotDefinition\n\n\nSnapshotDefinition\n\n\n\n\n\n\n\n\nJSON Schemas\n\u00b6\n\n\nPlugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below:\n\n\n\n\nWhat is JSON?\n\n\nWhat is a JSON schema?\n\n\nHow has Delphix augmented JSON schemas?\n\n\n\n\nJSON\n\u00b6\n\n\nJSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format:\n\n\n\n\n\n\n\n\nJSON\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\"hello\"\n\n\nA string. Note the double quotes.\n\n\n\n\n\n\n17\n\n\nAn integer\n\n\n\n\n\n\ntrue\n\n\nA boolean\n\n\n\n\n\n\n{\"name\": \"Julie\", \"age\": 37}\n\n\nA JSON object with two fields, \nname\n (a string), and \nage\n (an integer). Objects are denoted with curly braces.\n\n\n\n\n\n\n[ true, false, true]\n\n\nA JSON array with three booleans. Arrays are denoted with square brackets.\n\n\n\n\n\n\n\n\nFor more details on JSON, please see https://www.json.org/.\n\n\nJSON Schemas\n\u00b6\n\n\nThe \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the \ndescription\n of the format of data (whereas \"raw\" JSON is intended for storing data).\n\n\nHere is an example of a JSON schema that defines a (simplified) US address:\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"streetNumber\"\n,\n \n\"street\"\n,\n \n\"city\"\n,\n \n\"state\"\n,\n \n\"zip5\"\n],\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"streetNumber\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"street\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"unit\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"city\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[A-Z][A-Za-z ]*$\"\n \n},\n\n        \n\"state\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[A-Z]{2}$\"\n \n},\n\n        \n\"zip5\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[0-9]{5}\"\n},\n\n        \n\"zipPlus4\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[0-9]{4}\"\n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nNote that this is perfectly valid JSON data. It's a JSON object with four fields: \ntype\n (a JSON string), \nrequired\n (A JSON array), \nadditionalProperties\n (a JSON boolean), and \nproperties\n. \nproperties\n, in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc.\n\n\nBut, this isn't \njust\n a JSON object. This is a JSON schema. It uses special keywords like \ntype\n \nrequired\n, and \nadditionalProperties\n. These have specially-defined meanings in the context of JSON schemas.\n\n\nHere is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords.\n\n\n\n\n\n\n\n\nkeyword\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nadditionalProperties\n\n\nDetermines whether the schema allows properties that are not explicitly listed in the \nproperties\n specification. Must be a \ntrue\n or \nfalse\n.\n\n\n\n\n\n\npattern\n\n\nUsed with string types to specify a regular expression that the property must conform to.\n\n\n\n\n\n\nrequired\n\n\nA list of required properties. Properties not listed in this list are optional.\n\n\n\n\n\n\nstring\n\n\nUsed with \ntype\n to declare that a property must be a string.\n\n\n\n\n\n\ntype\n\n\nSpecifies a datatype. Common values are \nobject\n, \narray\n, \nnumber\n, \ninteger\n, \nboolean\n, and \nstring\n.\n\n\n\n\n\n\n\n\nSome points to note about the address schema above:\n\n\n\n\nBecause of the \nrequired\n list, all valid addresses must have fields called \nname\n, \nstreetNumber\n and so on.\n\n\nunit\n and \nzipPlus4\n do not appear in the \nrequired\n list, and therefore are optional.\n\n\nBecause of \nadditionalProperties\n being \nfalse\n, valid addresses cannot make up their own fields like \nnickname\n or \ndoorbellLocation\n.\n\n\nBecause of the \npattern\n, any \nstate\n field in a valid address must consist of exactly two capital letters.\n\n\nSimilarly, \ncity\n must only contain letters and spaces, and \nzip\n and \nzipPlus4\n must only contain digits.\n\n\nEach property has its own valid subschema that describes its own type definition.\n\n\n\n\nHere is a JSON object that conforms to the above schema:\n\n\n{\n\n  \n\"name\"\n:\n \n\"Delphix\"\n,\n\n  \n\"streetNumber\"\n:\n \n\"220\"\n,\n\n  \n\"street\"\n:\n \n\"Congress St.\"\n,\n\n  \n\"unit\"\n:\n \n\"200\"\n,\n\n  \n\"city\"\n:\n \n\"Boston\"\n,\n\n  \n\"state\"\n:\n \n\"MA\"\n,\n\n  \n\"zip\"\n:\n \n\"02210\"\n\n\n}\n\n\n\n\n\n\n\n\nInfo\n\n\nA common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema \ndescribes\n what an address looks like. The address itself is not a schema.\n\n\n\n\nFor much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see https://json-schema.org/understanding-json-schema/\n\n\nDelphix-specific Extensions to JSON Schema\n\u00b6\n\n\nThe JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords.\n\n\nThe list below outlines each of these keywords, and provides minimal examples of how they might be used.\n\n\ndescription\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nIn any property subschema, at the same level as \ntype\n.\n\n\n\n\n\n\n\n\nThe \ndescription\n keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown.\n\n\nIn this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"description\"\n:\n \n\"User-readable name for the provisioned database\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nidentityFields\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nRequired (for repository and source config schemas only)\n\n\n\n\n\n\nWhere?\n\n\nAt the top level of a repository or source config schema, at the same level as \ntype\n and \nproperties\n.\n\n\n\n\n\n\n\n\nThe \nidentityFields\n is a list of property names that, together, serve as a unique identifier for a repository or source config.\n\n\nWhen a plugin's \nautomatic discovery\n code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about.\n\n\nFor example, suppose the engine already knows about a single repository with data \n{\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"}\n (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data \n{ \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"}\n.\n\n\nWhat should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name?\n\n\nidentityFields\n is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if \nall\n of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data.\n\n\nidentityFields\n is \nrequired\n for \nRepositoryDefinition\n and \nSourceConfigDefinition\n schemas, and may not be used in any other schemas.\n\n\nIn this example, we'll tell the Delphix Engine that \npath\n is the sole unique identifier.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n      \n\"dbname\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n      \n\"path\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n\n\n\n\n\n\nnameField\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nRequired (for repository and source config schemas only)\n\n\n\n\n\n\nWhere?\n\n\nAt the top level of a repository or source config schema, at the same level as \ntype\n and \nproperties\n.\n\n\n\n\n\n\n\n\nThe \nnameField\n keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as \nproperties\n. It is \nrequired\n for \nRepositoryDefinition\n and \nSourceConfigDefinition\n schemas, and may not be used in any other schemas.\n\n\nIn this example, we will use the \npath\n property as the user-visible name.\n\n\n{\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"path\"\n\n\n}\n\n\n\n\n\n\nSo, if we have an repository object that looks like\n\n\n{\n\n  \n\"path\"\n:\n \n\"/usr/bin\"\n,\n\n  \n\"port\"\n:\n \n8800\n\n\n}\n\n\n\n\n\n\nthen the user will be able to refer to this object as \n/usr/bin\n.\n\n\npassword\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nAs the value for the \nformat\n keyword in any string property's subschema.\n\n\n\n\n\n\n\n\nThe \npassword\n keyword can be used to specify the \nformat\n of a \nstring\n. (Note that \nformat\n is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described \nhere\n.\n\n\nIn this example, the \ndbPass\n field on any object will be treated as a password.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"dbPass\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"format\"\n:\n \n\"password\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nprettyName\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nIn any property subschema, at the same level as \ntype\n.\n\n\n\n\n\n\n\n\nThe \nprettyName\n keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used.\n\n\nIn this example, the user would see \"Name of Database\" on the UI, instead of just \"name\".\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"prettyName\"\n:\n \n\"Name of Database\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nunixpath\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nAs the value for the \nformat\n keyword in any string property's subschema.\n\n\n\n\n\n\n\n\nThe \nunixpath\n keyword is used to specify the format of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"datapath\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"format\"\n:\n \n\"unixpath\"\n\n    \n}\n\n  \n}\n\n\n}",
            "title": "Schemas"
        },
        {
            "location": "/References/Schemas/#schemas",
            "text": "",
            "title": "Schemas"
        },
        {
            "location": "/References/Schemas/#about-schemas",
            "text": "Any time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data:   What is the set of data needed and what should they be called?  What is the type of each piece of data: Strings? Integers? Booleans?   Plugins use  schemas  to describe the format of such data. Once a schema is defined, it is used in three ways   It tells the Delphix Engine how to store the data for later use.  It is used to autogenerate a custom user interface, and to validate user inputs.  It is used to  autogenerate Python classes  that can be used by plugin code to access and manipulate user input and stored data.   There are five plugin-customizable data formats:     Delphix Object  Schema  Autogenerated Class      Repository  RepositoryDefinition  RepositoryDefinition    Source Config  SourceConfigDefinition  SourceConfigDefinition    Linked Source  LinkedSourceDefinition  LinkedSourceDefinition    Virtual Source  VirtualSourceDefinition  VirtualSourceDefinition    Snapshot  SnapshotDefinition  SnapshotDefinition",
            "title": "About Schemas"
        },
        {
            "location": "/References/Schemas/#json-schemas",
            "text": "Plugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below:   What is JSON?  What is a JSON schema?  How has Delphix augmented JSON schemas?",
            "title": "JSON Schemas"
        },
        {
            "location": "/References/Schemas/#json",
            "text": "JSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format:     JSON  Description      \"hello\"  A string. Note the double quotes.    17  An integer    true  A boolean    {\"name\": \"Julie\", \"age\": 37}  A JSON object with two fields,  name  (a string), and  age  (an integer). Objects are denoted with curly braces.    [ true, false, true]  A JSON array with three booleans. Arrays are denoted with square brackets.     For more details on JSON, please see https://www.json.org/.",
            "title": "JSON"
        },
        {
            "location": "/References/Schemas/#json-schemas_1",
            "text": "The \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the  description  of the format of data (whereas \"raw\" JSON is intended for storing data).  Here is an example of a JSON schema that defines a (simplified) US address:  { \n     \"type\" :   \"object\" , \n     \"required\" :   [ \"name\" ,   \"streetNumber\" ,   \"street\" ,   \"city\" ,   \"state\" ,   \"zip5\" ], \n     \"additionalProperties\" :   false , \n     \"properties\" :   { \n         \"name\" :   {   \"type\" :   \"string\"   }, \n         \"streetNumber\" :   {   \"type\" :   \"string\"   }, \n         \"street\" :   {   \"type\" :   \"string\"   }, \n         \"unit\" :   {   \"type\" :   \"string\"   }, \n         \"city\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[A-Z][A-Za-z ]*$\"   }, \n         \"state\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[A-Z]{2}$\"   }, \n         \"zip5\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[0-9]{5}\" }, \n         \"zipPlus4\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[0-9]{4}\" } \n     }  }   Note that this is perfectly valid JSON data. It's a JSON object with four fields:  type  (a JSON string),  required  (A JSON array),  additionalProperties  (a JSON boolean), and  properties .  properties , in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc.  But, this isn't  just  a JSON object. This is a JSON schema. It uses special keywords like  type   required , and  additionalProperties . These have specially-defined meanings in the context of JSON schemas.  Here is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords.     keyword  description      additionalProperties  Determines whether the schema allows properties that are not explicitly listed in the  properties  specification. Must be a  true  or  false .    pattern  Used with string types to specify a regular expression that the property must conform to.    required  A list of required properties. Properties not listed in this list are optional.    string  Used with  type  to declare that a property must be a string.    type  Specifies a datatype. Common values are  object ,  array ,  number ,  integer ,  boolean , and  string .     Some points to note about the address schema above:   Because of the  required  list, all valid addresses must have fields called  name ,  streetNumber  and so on.  unit  and  zipPlus4  do not appear in the  required  list, and therefore are optional.  Because of  additionalProperties  being  false , valid addresses cannot make up their own fields like  nickname  or  doorbellLocation .  Because of the  pattern , any  state  field in a valid address must consist of exactly two capital letters.  Similarly,  city  must only contain letters and spaces, and  zip  and  zipPlus4  must only contain digits.  Each property has its own valid subschema that describes its own type definition.   Here is a JSON object that conforms to the above schema:  { \n   \"name\" :   \"Delphix\" , \n   \"streetNumber\" :   \"220\" , \n   \"street\" :   \"Congress St.\" , \n   \"unit\" :   \"200\" , \n   \"city\" :   \"Boston\" , \n   \"state\" :   \"MA\" , \n   \"zip\" :   \"02210\"  }    Info  A common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema  describes  what an address looks like. The address itself is not a schema.   For much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see https://json-schema.org/understanding-json-schema/",
            "title": "JSON Schemas"
        },
        {
            "location": "/References/Schemas/#delphix-specific-extensions-to-json-schema",
            "text": "The JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords.  The list below outlines each of these keywords, and provides minimal examples of how they might be used.",
            "title": "Delphix-specific Extensions to JSON Schema"
        },
        {
            "location": "/References/Schemas/#description",
            "text": "Summary       Required or Optional?  Optional    Where?  In any property subschema, at the same level as  type .     The  description  keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown.  In this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget.  { \n   \"properties\" :   { \n     \"name\" :   { \n       \"type\" :   \"string\" , \n       \"description\" :   \"User-readable name for the provisioned database\" \n     } \n   }  }",
            "title": "description"
        },
        {
            "location": "/References/Schemas/#identityfields",
            "text": "Summary       Required or Optional?  Required (for repository and source config schemas only)    Where?  At the top level of a repository or source config schema, at the same level as  type  and  properties .     The  identityFields  is a list of property names that, together, serve as a unique identifier for a repository or source config.  When a plugin's  automatic discovery  code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about.  For example, suppose the engine already knows about a single repository with data  {\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"}  (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data  { \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"} .  What should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name?  identityFields  is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if  all  of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data.  identityFields  is  required  for  RepositoryDefinition  and  SourceConfigDefinition  schemas, and may not be used in any other schemas.  In this example, we'll tell the Delphix Engine that  path  is the sole unique identifier.  { \n   \"properties\" :   { \n       \"dbname\" :   { \"type\" :   \"string\" }, \n       \"path\" :   { \"type\" :   \"string\" } \n   }, \n   \"identityFields\" :   [ \"path\" ]  }",
            "title": "identityFields"
        },
        {
            "location": "/References/Schemas/#namefield",
            "text": "Summary       Required or Optional?  Required (for repository and source config schemas only)    Where?  At the top level of a repository or source config schema, at the same level as  type  and  properties .     The  nameField  keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as  properties . It is  required  for  RepositoryDefinition  and  SourceConfigDefinition  schemas, and may not be used in any other schemas.  In this example, we will use the  path  property as the user-visible name.  { \n     \"properties\" :   { \n         \"path\" :   {   \"type\" :   \"string\"   }, \n         \"port\" :   {   \"type\" :   \"integer\"   } \n     }, \n     \"nameField\" :   \"path\"  }   So, if we have an repository object that looks like  { \n   \"path\" :   \"/usr/bin\" , \n   \"port\" :   8800  }   then the user will be able to refer to this object as  /usr/bin .",
            "title": "nameField"
        },
        {
            "location": "/References/Schemas/#password",
            "text": "Summary       Required or Optional?  Optional    Where?  As the value for the  format  keyword in any string property's subschema.     The  password  keyword can be used to specify the  format  of a  string . (Note that  format  is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described  here .  In this example, the  dbPass  field on any object will be treated as a password.  { \n   \"properties\" :   { \n     \"dbPass\" :   { \n       \"type\" :   \"string\" , \n       \"format\" :   \"password\" \n     } \n   }  }",
            "title": "password"
        },
        {
            "location": "/References/Schemas/#prettyname",
            "text": "Summary       Required or Optional?  Optional    Where?  In any property subschema, at the same level as  type .     The  prettyName  keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used.  In this example, the user would see \"Name of Database\" on the UI, instead of just \"name\".  { \n   \"properties\" :   { \n     \"name\" :   { \n       \"type\" :   \"string\" , \n       \"prettyName\" :   \"Name of Database\" \n     } \n   }  }",
            "title": "prettyName"
        },
        {
            "location": "/References/Schemas/#unixpath",
            "text": "Summary       Required or Optional?  Optional    Where?  As the value for the  format  keyword in any string property's subschema.     The  unixpath  keyword is used to specify the format of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path.  { \n   \"properties\" :   { \n     \"datapath\" :   { \n       \"type\" :   \"string\" , \n       \"format\" :   \"unixpath\" \n     } \n   }  }",
            "title": "unixpath"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/",
            "text": "Schemas and Autogenerated Classes\n\u00b6\n\n\nPlugin operations\n will sometimes need to work with data in these custom formats. For example, the \nconfigure\n operation will accept snapshot data as an input, and must produce source config data as an output.\n\n\nTo enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes.\n\n\n\n\nInfo\n\n\nAutogenerated Python code will use \nlower_case_with_underscores\n as attribute names as per Python variable naming conventions.\nThat is, if we were to use \nmountLocation\n as the schema property name, it would be called\n\nmount_location\n in the generated Python code.\n\n\n\n\nRepositoryDefinition\n\u00b6\n\n\nDefines properties used to identify a \nRepository\n.\n\n\nRepositoryDefinition Schema\n\u00b6\n\n\nThe plugin must also decide on a \nname\n field and a set of \nidentityFields\n to display and uniquely identify the \nrepository\n.\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n  \n\"nameField\"\n:\n \n\"name\"\n\n\n}\n\n\n\n\n\n\nRepositoryDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nRepositoryDefinition Schema\n.\n\n\nclass\n \nRepositoryDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nrepository_name\n,\n \nrepository_path\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nrepository_name\n,\n \n\"path\"\n:\n \nrepository_path\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nrepository\n \n=\n \nRepositoryDefinition\n()\n\n\nrepository\n.\nname\n \n=\n \n\"name\"\n\n\nrepository\n.\npath\n \n=\n \n\"/some/path\"\n\n\n\n\n\n\nSourceConfigDefinition\n\u00b6\n\n\nDefines properties used to identify a \nSource Config\n.\n\n\nSourceConfigDefinition Schema\n\u00b6\n\n\nThe plugin must also decide on a \nname\n field and a set of \nidentityFields\n to display and uniquely identify the \nsource config\n.\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n\"name\"\n\n\n}\n\n\n\n\n\n\nSourceConfigDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nSourceConfigDefinition Schema\n.\n\n\nclass\n \nSourceConfigDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nconfig_name\n,\n \nconfig_path\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nconfig_name\n,\n \n\"path\"\n:\n \nconfig_path\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nSourceConfigDefinition\n\n\n\nsource_config\n \n=\n \nSourceConfigDefinition\n()\n\n\nsource_config\n.\nname\n \n=\n \n\"name\"\n\n\nsource_config\n.\npath\n \n=\n \n\"/some/path\"\n\n\n\n\n\n\nLinkedSourceDefinition\n\u00b6\n\n\nDefines properties used to identify \nlinked sources\n.\n\n\nLinkedSourceDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"port\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n\n}\n\n\n\n\n\n\nLinkedSourceDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nLinkedSourceDefinition Schema\n.\n\n\nclass\n \nLinkedSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nsource_name\n,\n \nsource_port\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nsource_name\n,\n \n\"port\"\n:\n \nsource_port\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nLinkedSourceDefinition\n\n\n\nsource\n \n=\n \nLinkedSourceDefinition\n(\n\"name\"\n,\n \n1000\n)\n\n\nname\n \n=\n \nsource\n.\nname\n\n\nport\n \n=\n \nsource\n.\nport\n\n\n\n\n\n\nVirtualSourceDefinition\n\u00b6\n\n\nDefines properties used to identify \nvirtual sources\n.\n\n\nVirtualSourceDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"port\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n\n}\n\n\n\n\n\n\nVirtualSourceDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nVirtualSourceDefinition Schema\n.\n\n\nclass\n \nVirtualSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nsource_name\n,\n \nsource_port\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nsource_name\n,\n \n\"port\"\n:\n \nsource_port\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nVirtualSourceDefinition\n\n\n\nsource\n \n=\n \nVirtualSourceDefinition\n(\n\"name\"\n,\n \n1000\n)\n\n\nname\n \n=\n \nsource\n.\nname\n\n\nport\n \n=\n \nsource\n.\nport\n\n\n\n\n\n\nSnapshotDefinition\n\u00b6\n\n\nDefines properties used to \nsnapshots\n.\n\n\nSnapshotDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"version\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"version\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n\n}\n\n\n\n\n\n\nSnapshotDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nVirtualSourceDefinition Schema\n.\n\n\nclass\n \nVirtualSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nsnapshot_version\n,\n \nsnapshot_transaction_id\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n      \n{\n\n        \n\"version\"\n:\n \nsnapshot_version\n,\n \n        \n\"transaction_id\"\n:\n \nsnapshot_transaction_id\n\n      \n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nSnapshotDefinition\n\n\n\nsnapshot\n \n=\n \nSnapshotDefinition\n()\n\n\nsnapshot\n.\nversion\n \n=\n \n\"1.2.3\"\n\n\nsnapshot\n.\ntransaction_id\n \n=\n \n1000",
            "title": "Schemas and Autogenerated Classes"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#schemas-and-autogenerated-classes",
            "text": "Plugin operations  will sometimes need to work with data in these custom formats. For example, the  configure  operation will accept snapshot data as an input, and must produce source config data as an output.  To enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes.   Info  Autogenerated Python code will use  lower_case_with_underscores  as attribute names as per Python variable naming conventions.\nThat is, if we were to use  mountLocation  as the schema property name, it would be called mount_location  in the generated Python code.",
            "title": "Schemas and Autogenerated Classes"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition",
            "text": "Defines properties used to identify a  Repository .",
            "title": "RepositoryDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition-schema",
            "text": "The plugin must also decide on a  name  field and a set of  identityFields  to display and uniquely identify the  repository .  { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"path\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ,   \"path\" ], \n   \"nameField\" :   \"name\"  }",
            "title": "RepositoryDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition-class",
            "text": "Autogenerated based on the  RepositoryDefinition Schema .  class   RepositoryDefinition : \n\n   def   __init__ ( self ,   repository_name ,   repository_path ): \n     self . _inner_dict   =   { \"name\" :   repository_name ,   \"path\" :   repository_path }    To use the class:   from   generated.defintions   import   RepositoryDefinition  repository   =   RepositoryDefinition ()  repository . name   =   \"name\"  repository . path   =   \"/some/path\"",
            "title": "RepositoryDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition",
            "text": "Defines properties used to identify a  Source Config .",
            "title": "SourceConfigDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-schema",
            "text": "The plugin must also decide on a  name  field and a set of  identityFields  to display and uniquely identify the  source config .  { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"path\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   \"name\"  }",
            "title": "SourceConfigDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-class",
            "text": "Autogenerated based on the  SourceConfigDefinition Schema .  class   SourceConfigDefinition : \n\n   def   __init__ ( self ,   config_name ,   config_path ): \n     self . _inner_dict   =   { \"name\" :   config_name ,   \"path\" :   config_path }    To use the class:   from   generated.defintions   import   SourceConfigDefinition  source_config   =   SourceConfigDefinition ()  source_config . name   =   \"name\"  source_config . path   =   \"/some/path\"",
            "title": "SourceConfigDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition",
            "text": "Defines properties used to identify  linked sources .",
            "title": "LinkedSourceDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ,   \"port\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"integer\"   }  }",
            "title": "LinkedSourceDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-class",
            "text": "Autogenerated based on the  LinkedSourceDefinition Schema .  class   LinkedSourceDefinition : \n\n   def   __init__ ( self ,   source_name ,   source_port ): \n     self . _inner_dict   =   { \"name\" :   source_name ,   \"port\" :   source_port }    To use the class:   from   generated.defintions   import   LinkedSourceDefinition  source   =   LinkedSourceDefinition ( \"name\" ,   1000 )  name   =   source . name  port   =   source . port",
            "title": "LinkedSourceDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition",
            "text": "Defines properties used to identify  virtual sources .",
            "title": "VirtualSourceDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ,   \"port\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"integer\"   }  }",
            "title": "VirtualSourceDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-class",
            "text": "Autogenerated based on the  VirtualSourceDefinition Schema .  class   VirtualSourceDefinition : \n\n   def   __init__ ( self ,   source_name ,   source_port ): \n     self . _inner_dict   =   { \"name\" :   source_name ,   \"port\" :   source_port }    To use the class:   from   generated.defintions   import   VirtualSourceDefinition  source   =   VirtualSourceDefinition ( \"name\" ,   1000 )  name   =   source . name  port   =   source . port",
            "title": "VirtualSourceDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition",
            "text": "Defines properties used to  snapshots .",
            "title": "SnapshotDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"version\" :   {   \"type\" :   \"string\"   }, \n     \"version\" :   {   \"type\" :   \"integer\"   }  }",
            "title": "SnapshotDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-class",
            "text": "Autogenerated based on the  VirtualSourceDefinition Schema .  class   VirtualSourceDefinition : \n\n   def   __init__ ( self ,   snapshot_version ,   snapshot_transaction_id ): \n     self . _inner_dict   =  \n       { \n         \"version\" :   snapshot_version ,  \n         \"transaction_id\" :   snapshot_transaction_id \n       }    To use the class:   from   generated.defintions   import   SnapshotDefinition  snapshot   =   SnapshotDefinition ()  snapshot . version   =   \"1.2.3\"  snapshot . transaction_id   =   1000",
            "title": "SnapshotDefinition Class"
        },
        {
            "location": "/References/Platform_Libraries/",
            "text": "Platform Libraries\n\u00b6\n\n\nSet of functions that plugins can use these for executing remote commands, etc.\n\n\nrun_bash\n\u00b6\n\n\nExecutes a bash command on a remote Unix host.\n\n\nSignature\n\u00b6\n\n\ndef run_bash(remote_connection, command, variables=None, use_login_shell=False, check=False)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_connection\n\n\nRemoteConnection\n\n\nConnection associated with the remote host to run the command on.\n\n\n\n\n\n\ncommand\n\n\nString\n\n\nCommand to run on the host.\n\n\n\n\n\n\nvariables\n\n\ndict[String, String]\n\n\nOptional\n. Environement variables to set when running the command.\n\n\n\n\n\n\nuse_login_shell\n\n\nboolean\n\n\nOptional\n. Whether to use a login shell.\n\n\n\n\n\n\ncheck\n\n\nboolean\n\n\nOptional\n. Whether or not to raise an exception if the \nexit_code\n in the \nRunBashResponse\n is non-zero.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nAn object of \nRunBashResponse\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nexit_code\n\n\nInteger\n\n\nExit code from the command.\n\n\n\n\n\n\nstdout\n\n\nString\n\n\nStdout from the command.\n\n\n\n\n\n\nstderr\n\n\nString\n\n\nStderr from the command.\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nCalling bash with an inline command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\ncommand\n \n=\n \n\"echo 'Hi' >> /tmp/debug.log\"\n\n\nvars\n \n=\n \n{\n\"var\"\n:\n \n\"val\"\n}\n\n\n\nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\nconnection\n,\n \ncommand\n,\n \nvars\n)\n\n\n\nprint\n \nresponse\n.\nexit_code\n\n\nprint\n \nresponse\n.\nstdout\n\n\nprint\n \nresponse\n.\nstderr\n\n\n\n\n\n\nUsing parameters to construct a bash command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nname\n \n=\n \nvirtual_source\n.\nparameters\n.\nusername\n\n\nport\n \n=\n \nvirtual_source\n.\nparameters\n.\nport\n\n\ncommand\n \n=\n \n\"mysqldump -u {} -p {}\"\n.\nformat\n(\nname\n,\nport\n)\n\n\n\nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\nconnection\n,\n \ncommand\n)\n\n\n\n\n\n\nRunning a bash script that is saved in a directory.\n\n\n \nimport\n \npkgutil\n\n \nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n \nscript_content\n \n=\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_date.sh'\n)\n\n\n \n# Execute script on remote host\n\n \nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\ndirect_source\n.\nconnection\n,\n \nscript_content\n)\n\n\n\n\n\n\nFor more information please go to \nManaging Scripts for Remote Execution\n section.\n\n\nrun_expect\n\u00b6\n\n\nExecutes a tcl command or script on a remote Unix host.\n\n\nSignature\n\u00b6\n\n\ndef run_expect(remote_connection, command, variables=None)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_connection\n\n\nRemoteConnection\n\n\nConnection associated with the remote host to run the command on.\n\n\n\n\n\n\ncommand\n\n\nString\n\n\nExpect(Tcl) command to run.\n\n\n\n\n\n\nvariables\n\n\ndict[String, String]\n\n\nOptional\n. Environement variables to set when running the command.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nAn object of \nRunExpectResponse\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nexit_code\n\n\nInteger\n\n\nExit code from the command.\n\n\n\n\n\n\nstdout\n\n\nString\n\n\nStdout from the command.\n\n\n\n\n\n\nstderr\n\n\nString\n\n\nStderr from the command.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nCalling expect  with an inline command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\ncommand\n \n=\n \n\"puts 'Hi'\"\n\n\nvars\n \n=\n \n{\n\"var\"\n:\n \n\"val\"\n}\n\n\n\nrepsonse\n \n=\n \nlibs\n.\nrun_expect\n(\nconnection\n,\n \ncommand\n,\n \nvars\n)\n\n\n\nprint\n \nresponse\n.\nexit_code\n\n\nprint\n \nresponse\n.\nstdout\n\n\nprint\n \nresponse\n.\nstderr\n\n\n\n\n\n\nrun_powershell\n\u00b6\n\n\nExecutes a powershell command on a remote Windows host.\n\n\nSignature\n\u00b6\n\n\ndef run_powershell(remote_connection, command, variables=None, check=False)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_connection\n\n\nRemoteConnection\n\n\nConnection associated with the remote host to run the command on.\n\n\n\n\n\n\ncommand\n\n\nString\n\n\nCommand to run to the remote host.\n\n\n\n\n\n\nvariables\n\n\ndict[String, String]\n\n\nOptional\n. Environement variables to set when running the command.\n\n\n\n\n\n\ncheck\n\n\nboolean\n\n\nOptional\n. Whether or not to raise an exception if the \nexit_code\n in the \nRunPowershellResponse\n is non-zero.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nAn object of \nRunPowershellResponse\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nexit_code\n\n\nInteger\n\n\nExit code from the command.\n\n\n\n\n\n\nstdout\n\n\nString\n\n\nStdout from the command.\n\n\n\n\n\n\nstderr\n\n\nString\n\n\nStderr from the command.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nCalling powershell with an inline command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\ncommand\n \n=\n \n\"Write-Output 'Hi'\"\n\n\nvars\n \n=\n \n{\n\"var\"\n:\n \n\"val\"\n}\n\n\n\nresponse\n \n=\n \nlibs\n.\nrun_powershell\n(\nconnection\n,\n \ncommand\n,\n \nvars\n)\n\n\n\nprint\n \nresponse\n.\nexit_code\n\n\nprint\n \nresponse\n.\nstdout\n\n\nprint\n \nresponse\n.\nstderr\n\n\n\n\n\n\nrun_sync\n\u00b6\n\n\nCopies files from the remote source host directly into the dSource, without involving a staging host.\n\n\nSignature\n\u00b6\n\n\ndef run_sync(remote_connection, source_directory, rsync_user=None, exclude_paths=None, sym_links_to_follow=None)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_connection\n\n\nRemoteConnection\n\n\nConnection associated with the remote host to run the command on.\n\n\n\n\n\n\nsource_directory\n\n\nString\n\n\nDirectory of files to be synced.\n\n\n\n\n\n\nrsync_user\n\n\nString\n\n\nOptional\n User who has access to the directory to be synced.\n\n\n\n\n\n\nexclude_paths\n\n\nlist[String]\n\n\nOptional\n Paths to be excluded.\n\n\n\n\n\n\nsym_links_to_follow\n\n\nlist[String]\n\n\nOptional\n Symbollic links to follow if any.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nsource_directory\n \n=\n \n\"sourceDirectory\"\n\n\nrsync_user\n \n=\n \n\"rsyncUser\"\n\n\nexclude_paths\n \n=\n \n[\n\"/path1\"\n,\n \n\"/path2\"\n]\n\n\nsym_links_to_follow\n \n=\n \n[\n\"/path3\"\n,\n \n\"/path4\"\n]\n\n\n\nlibs\n.\nrun_sync\n(\nconnection\n,\n \nsource_directory\n,\n \nrsync_user\n,\n \nexclude_paths\n,\n \nsym_links_to_follow\n)",
            "title": "Platform Libraries"
        },
        {
            "location": "/References/Platform_Libraries/#platform-libraries",
            "text": "Set of functions that plugins can use these for executing remote commands, etc.",
            "title": "Platform Libraries"
        },
        {
            "location": "/References/Platform_Libraries/#run_bash",
            "text": "Executes a bash command on a remote Unix host.",
            "title": "run_bash"
        },
        {
            "location": "/References/Platform_Libraries/#signature",
            "text": "def run_bash(remote_connection, command, variables=None, use_login_shell=False, check=False)",
            "title": "Signature"
        },
        {
            "location": "/References/Platform_Libraries/#arguments",
            "text": "Argument  Type  Description      remote_connection  RemoteConnection  Connection associated with the remote host to run the command on.    command  String  Command to run on the host.    variables  dict[String, String]  Optional . Environement variables to set when running the command.    use_login_shell  boolean  Optional . Whether to use a login shell.    check  boolean  Optional . Whether or not to raise an exception if the  exit_code  in the  RunBashResponse  is non-zero.",
            "title": "Arguments"
        },
        {
            "location": "/References/Platform_Libraries/#returns",
            "text": "An object of  RunBashResponse     Field  Type  Description      exit_code  Integer  Exit code from the command.    stdout  String  Stdout from the command.    stderr  String  Stderr from the command.",
            "title": "Returns"
        },
        {
            "location": "/References/Platform_Libraries/#examples",
            "text": "Calling bash with an inline command.  from   dlpx.virtualization   import   libs  command   =   \"echo 'Hi' >> /tmp/debug.log\"  vars   =   { \"var\" :   \"val\" }  response   =   libs . run_bash ( connection ,   command ,   vars )  print   response . exit_code  print   response . stdout  print   response . stderr   Using parameters to construct a bash command.  from   dlpx.virtualization   import   libs  name   =   virtual_source . parameters . username  port   =   virtual_source . parameters . port  command   =   \"mysqldump -u {} -p {}\" . format ( name , port )  response   =   libs . run_bash ( connection ,   command )   Running a bash script that is saved in a directory.    import   pkgutil \n  from   dlpx.virtualization   import   libs \n\n  script_content   =   pkgutil . get_data ( 'resources' ,   'get_date.sh' ) \n\n  # Execute script on remote host \n  response   =   libs . run_bash ( direct_source . connection ,   script_content )   For more information please go to  Managing Scripts for Remote Execution  section.",
            "title": "Examples"
        },
        {
            "location": "/References/Platform_Libraries/#run_expect",
            "text": "Executes a tcl command or script on a remote Unix host.",
            "title": "run_expect"
        },
        {
            "location": "/References/Platform_Libraries/#signature_1",
            "text": "def run_expect(remote_connection, command, variables=None)",
            "title": "Signature"
        },
        {
            "location": "/References/Platform_Libraries/#arguments_1",
            "text": "Argument  Type  Description      remote_connection  RemoteConnection  Connection associated with the remote host to run the command on.    command  String  Expect(Tcl) command to run.    variables  dict[String, String]  Optional . Environement variables to set when running the command.",
            "title": "Arguments"
        },
        {
            "location": "/References/Platform_Libraries/#returns_1",
            "text": "An object of  RunExpectResponse     Field  Type  Description      exit_code  Integer  Exit code from the command.    stdout  String  Stdout from the command.    stderr  String  Stderr from the command.",
            "title": "Returns"
        },
        {
            "location": "/References/Platform_Libraries/#example",
            "text": "Calling expect  with an inline command.  from   dlpx.virtualization   import   libs  command   =   \"puts 'Hi'\"  vars   =   { \"var\" :   \"val\" }  repsonse   =   libs . run_expect ( connection ,   command ,   vars )  print   response . exit_code  print   response . stdout  print   response . stderr",
            "title": "Example"
        },
        {
            "location": "/References/Platform_Libraries/#run_powershell",
            "text": "Executes a powershell command on a remote Windows host.",
            "title": "run_powershell"
        },
        {
            "location": "/References/Platform_Libraries/#signature_2",
            "text": "def run_powershell(remote_connection, command, variables=None, check=False)",
            "title": "Signature"
        },
        {
            "location": "/References/Platform_Libraries/#arguments_2",
            "text": "Argument  Type  Description      remote_connection  RemoteConnection  Connection associated with the remote host to run the command on.    command  String  Command to run to the remote host.    variables  dict[String, String]  Optional . Environement variables to set when running the command.    check  boolean  Optional . Whether or not to raise an exception if the  exit_code  in the  RunPowershellResponse  is non-zero.",
            "title": "Arguments"
        },
        {
            "location": "/References/Platform_Libraries/#returns_2",
            "text": "An object of  RunPowershellResponse     Field  Type  Description      exit_code  Integer  Exit code from the command.    stdout  String  Stdout from the command.    stderr  String  Stderr from the command.",
            "title": "Returns"
        },
        {
            "location": "/References/Platform_Libraries/#example_1",
            "text": "Calling powershell with an inline command.  from   dlpx.virtualization   import   libs  command   =   \"Write-Output 'Hi'\"  vars   =   { \"var\" :   \"val\" }  response   =   libs . run_powershell ( connection ,   command ,   vars )  print   response . exit_code  print   response . stdout  print   response . stderr",
            "title": "Example"
        },
        {
            "location": "/References/Platform_Libraries/#run_sync",
            "text": "Copies files from the remote source host directly into the dSource, without involving a staging host.",
            "title": "run_sync"
        },
        {
            "location": "/References/Platform_Libraries/#signature_3",
            "text": "def run_sync(remote_connection, source_directory, rsync_user=None, exclude_paths=None, sym_links_to_follow=None)",
            "title": "Signature"
        },
        {
            "location": "/References/Platform_Libraries/#arguments_3",
            "text": "Argument  Type  Description      remote_connection  RemoteConnection  Connection associated with the remote host to run the command on.    source_directory  String  Directory of files to be synced.    rsync_user  String  Optional  User who has access to the directory to be synced.    exclude_paths  list[String]  Optional  Paths to be excluded.    sym_links_to_follow  list[String]  Optional  Symbollic links to follow if any.",
            "title": "Arguments"
        },
        {
            "location": "/References/Platform_Libraries/#returns_3",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Platform_Libraries/#example_2",
            "text": "from   dlpx.virtualization   import   libs  source_directory   =   \"sourceDirectory\"  rsync_user   =   \"rsyncUser\"  exclude_paths   =   [ \"/path1\" ,   \"/path2\" ]  sym_links_to_follow   =   [ \"/path3\" ,   \"/path4\" ]  libs . run_sync ( connection ,   source_directory ,   rsync_user ,   exclude_paths ,   sym_links_to_follow )",
            "title": "Example"
        },
        {
            "location": "/References/Logging/",
            "text": "Logging\n\u00b6\n\n\nWhat is logging?\n\u00b6\n\n\nThe Virtualization Platform keeps plugin-specific log files. A plugin can, at any point in any of its \nplugin operations\n, write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin.\n\n\nOverview\n\u00b6\n\n\nThe Virtualization Platform integrates with Python's built-in \nlogging framework\n. A special \nHandler\n is exposed by the platform at \ndlpx.virtualization.libs.PlatformHandler\n. This handler needs to be added to the Python logger your plugin creates. Logging statements made through Python's logging framework will then be routed to the platform.\n\n\nBasic Setup\n\u00b6\n\n\nBelow is the absolute minimum needed to setup logging for the platform. Please refer to Python's \nlogging documentation\n and the \nexample below\n to better understand how it can be customized.\n\n\nimport\n \nlogging\n\n\n\nfrom\n \ndlpx.virtualization.libs\n \nimport\n \nPlatformHandler\n\n\n\n# Get the root logger.\n\n\nlogger\n \n=\n \nlogging\n.\ngetLogger\n()\n\n\nlogger\n.\naddHandler\n(\nPlatformHandler\n())\n\n\n\n# The root logger's default level is logging.WARNING.\n\n\n# Without the line below, logging statements of levels\n\n\n# lower than logging.WARNING will be suppressed.\n\n\nlogger\n.\nsetLevel\n(\nlogging\n.\nDEBUG\n)\n\n\n\n\n\n\n\n\nLogging Setup\n\n\nPython's logging framework is global. Setup only needs to happen once, but where it happens is important. Any logging statements that occur before the \nPlatformHandler\n is added will not be logged by the platform.\n\n\nIt is highly recommended that the logging setup is done in the plugin's entry point module before any operations are ran.\n\n\n\n\n\n\nAdd the PlatformHandler to the root logger\n\n\nLoggers in Python have a hierarchy and all loggers are children of a special logger called the \"root logger\". Logging hierarchy is not always intuitive and depends on how modules are structured.\n\n\nTo avoid this complexity, add the \nPlatformHandler\n to the root logger. The root logger can be retrieved with \nlogging.getLogger()\n.\n\n\n\n\nUsage\n\u00b6\n\n\nOnce the \nPlatformHandler\n has been added to the logger, logging is done with Python's \nLogger\n object. Below is a simple example including the basic setup code used above:\n\n\nimport\n \nlogging\n\n\n\nfrom\n \ndlpx.virtualization.libs\n \nimport\n \nPlatformHandler\n\n\n\nlogger\n \n=\n \nlogging\n.\ngetLogger\n()\n\n\nlogger\n.\naddHandler\n(\nPlatformHandler\n())\n\n\n\n# The root logger's default level is logging.WARNING.\n\n\n# Without the line below, logging statements of levels\n\n\n# lower than logging.WARNING will be suppressed.\n\n\nlogger\n.\nsetLevel\n(\nlogging\n.\nDEBUG\n)\n\n\n\nlogger\n.\ndebug\n(\n'debug'\n)\n\n\nlogger\n.\ninfo\n(\n'info'\n)\n\n\nlogger\n.\nerror\n(\n'error'\n)\n\n\n\n\n\n\nExample\n\u00b6\n\n\nImagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why.\n\n\n\n\nInfo\n\n\nRefer to \nManaging Scripts for Remote Execution\n for how remote scripts can be stored and retrieved.\n\n\n\n\nSuppose your plugin has a source config discovery operation that looks like this (code is abbreviated to be easier to follow):\n\n\nimport\n \npkgutil\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n \n  \nreturn\n \n[\nRepositoryDefinition\n(\n'Logging Example'\n)]\n\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n  \nversion_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_db_version.sh'\n))\n\n  \nusers_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_db_users.sh'\n))\n\n  \ndb_results\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_databases.sh'\n))\n\n  \nstatus_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_database_statuses.sh'\n))\n\n\n  \n# Return an empty list for simplicity. In reality\n\n  \n# something would be done with the results above.\n\n  \nreturn\n \n[]\n\n\n\n\n\n\nNow, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this:\n\n\nimport\n \nlogging\n\n\nimport\n \npkgutil\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nRepositoryDefinition\n\n\n\n# This should probably be defined in its own module outside\n\n\n# of the plugin's entry point file. It is here for simplicity.\n\n\ndef\n \n_setup_logger\n():\n\n    \n# This will log the time, level, filename, line number, and log message.\n\n    \nlog_message_format\n \n=\n \n'[\n%(asctime)s\n] [\n%(levelname)s\n] [\n%(filename)s\n:\n%(lineno)d\n] \n%(message)s\n'\n\n    \nlog_message_date_format\n \n=\n \n'%Y-%m-\n%d\n %H:%M:%S'\n\n\n    \n# Create a custom formatter. This will help with diagnosability.\n\n    \nformatter\n \n=\n \nlogging\n.\nFormatter\n(\nlog_message_format\n,\n \ndatefmt\n=\n \nlog_message_date_format\n)\n\n\n    \nplatform_handler\n \n=\n \nlibs\n.\nPlatformHandler\n()\n\n    \nplatform_handler\n.\nsetFormatter\n(\nformatter\n)\n\n\n    \nlogger\n \n=\n \nlogging\n.\ngetLogger\n()\n\n    \nlogger\n.\naddHandler\n(\nplatform_handler\n)\n\n\n    \n# By default the root logger's level is logging.WARNING.\n\n    \nlogger\n.\nsetLevel\n(\nlogging\n.\nDEBUG\n)\n\n\n\n\n# Setup the logger.\n\n\n_setup_logger\n()\n\n\n\n# logging.getLogger(__name__) is the convention way to get a logger in Python.\n\n\n# It returns a new logger per module and will be a child of the root logger.\n\n\n# Since we setup the root logger, nothing else needs to be done to set this\n\n\n# one up.\n\n\nlogger\n \n=\n \nlogging\n.\ngetLogger\n(\n__name__\n)\n\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n \n  \nreturn\n \n[\nRepositoryDefinition\n(\n'Logging Example'\n)]\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n  \nlogger\n.\ndebug\n(\n'About to get DB version'\n)\n\n  \nversion_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_db_version.sh'\n))\n\n  \nlogger\n.\ndebug\n(\n'About to get DB users'\n)\n\n  \nusers_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_db_users.sh'\n))\n\n  \nlogger\n.\ndebug\n(\n'About to get databases'\n)\n\n  \ndb_results\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_databases.sh'\n))\n\n  \nlogger\n.\ndebug\n(\n'About to get DB statuses'\n)\n\n  \nstatus_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_database_statuses.sh'\n))\n\n  \nlogger\n.\ndebug\n(\n'Done collecting data'\n)\n\n\n  \n# Return an empty list for simplicity. In reality\n\n  \n# something would be done with the results above.\n\n  \nreturn\n \n[]\n\n\n\n\n\n\nWhen you look at the log file, perhaps you'll see something like this:\n\n\n[Worker-360|JOB-315|ENVIRONMENT_DISCOVER(UNIX_HOST_ENVIRONMENT-5)] [2019-04-30 12:10:42] [DEBUG] [python_runner.py:44] About to get DB version\n[Worker-360|JOB-316|DB_SYNC(APPDATA_CONTAINER-21)] [2019-04-30 12:19:35] [DEBUG] [python_runner.py:49] About to get DB users\n[Worker-325|JOB-280|ENVIRONMENT_REFRESH(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:51] About to get databases\n[Worker-326|JOB-281|SOURCES_DISABLE(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:53] About to get DB statuses\n\n\n\n\n\nYou can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes!\n\n\nWe now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem.\n\n\nHow to retrieve logs\n\u00b6\n\n\nDownload a support bundle by going to \nHelp\n > \nSupport Logs\n  and select \nDownload\n. The logs will be in a the support bundle under \nlog/mgmt_log/plugin_log/<plugin name>\n.\n\n\nLogging Levels\n\u00b6\n\n\nPython has a number of \npreset logging levels\n and allows for custom ones as well. Since logging on the Virtualization Platform uses the \nlogging\n framework, log statements of all levels are supported.\n\n\nHowever, the Virtualization Platform will map all logging levels into three files: \ndebug.log\n, \ninfo.log\n, and \nerror.log\n in the following way:\n\n\n\n\n\n\n\n\nPython Logging Level\n\n\nLogging File\n\n\n\n\n\n\n\n\n\n\nDEBUG\n\n\ndebug.log\n\n\n\n\n\n\nINFO\n\n\ninfo.log\n\n\n\n\n\n\nWARN\n\n\nerror.log\n\n\n\n\n\n\nWARNING\n\n\nerror.log\n\n\n\n\n\n\nERROR\n\n\nerror.log\n\n\n\n\n\n\nCRITICAL\n\n\nerror.log\n\n\n\n\n\n\n\n\nAs is the case with the \nlogging\n framework, logging statements are hierarchical: logging statements made at the \nlogging.DEBUG\n level will be written only to \ndebug.log\n while logging statements made at the \nlogging.ERROR\n level will be written to \ndebug.log\n, \ninfo.log\n, and \nerror.log\n.\n\n\nSensitive data\n\u00b6\n\n\nRemember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on \nsensitive data",
            "title": "Logging"
        },
        {
            "location": "/References/Logging/#logging",
            "text": "",
            "title": "Logging"
        },
        {
            "location": "/References/Logging/#what-is-logging",
            "text": "The Virtualization Platform keeps plugin-specific log files. A plugin can, at any point in any of its  plugin operations , write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin.",
            "title": "What is logging?"
        },
        {
            "location": "/References/Logging/#overview",
            "text": "The Virtualization Platform integrates with Python's built-in  logging framework . A special  Handler  is exposed by the platform at  dlpx.virtualization.libs.PlatformHandler . This handler needs to be added to the Python logger your plugin creates. Logging statements made through Python's logging framework will then be routed to the platform.",
            "title": "Overview"
        },
        {
            "location": "/References/Logging/#basic-setup",
            "text": "Below is the absolute minimum needed to setup logging for the platform. Please refer to Python's  logging documentation  and the  example below  to better understand how it can be customized.  import   logging  from   dlpx.virtualization.libs   import   PlatformHandler  # Get the root logger.  logger   =   logging . getLogger ()  logger . addHandler ( PlatformHandler ())  # The root logger's default level is logging.WARNING.  # Without the line below, logging statements of levels  # lower than logging.WARNING will be suppressed.  logger . setLevel ( logging . DEBUG )    Logging Setup  Python's logging framework is global. Setup only needs to happen once, but where it happens is important. Any logging statements that occur before the  PlatformHandler  is added will not be logged by the platform.  It is highly recommended that the logging setup is done in the plugin's entry point module before any operations are ran.    Add the PlatformHandler to the root logger  Loggers in Python have a hierarchy and all loggers are children of a special logger called the \"root logger\". Logging hierarchy is not always intuitive and depends on how modules are structured.  To avoid this complexity, add the  PlatformHandler  to the root logger. The root logger can be retrieved with  logging.getLogger() .",
            "title": "Basic Setup"
        },
        {
            "location": "/References/Logging/#usage",
            "text": "Once the  PlatformHandler  has been added to the logger, logging is done with Python's  Logger  object. Below is a simple example including the basic setup code used above:  import   logging  from   dlpx.virtualization.libs   import   PlatformHandler  logger   =   logging . getLogger ()  logger . addHandler ( PlatformHandler ())  # The root logger's default level is logging.WARNING.  # Without the line below, logging statements of levels  # lower than logging.WARNING will be suppressed.  logger . setLevel ( logging . DEBUG )  logger . debug ( 'debug' )  logger . info ( 'info' )  logger . error ( 'error' )",
            "title": "Usage"
        },
        {
            "location": "/References/Logging/#example",
            "text": "Imagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why.   Info  Refer to  Managing Scripts for Remote Execution  for how remote scripts can be stored and retrieved.   Suppose your plugin has a source config discovery operation that looks like this (code is abbreviated to be easier to follow):  import   pkgutil  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ):  \n   return   [ RepositoryDefinition ( 'Logging Example' )]  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n   version_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_db_version.sh' )) \n   users_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_db_users.sh' )) \n   db_results   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_databases.sh' )) \n   status_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_database_statuses.sh' )) \n\n   # Return an empty list for simplicity. In reality \n   # something would be done with the results above. \n   return   []   Now, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this:  import   logging  import   pkgutil  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   RepositoryDefinition  # This should probably be defined in its own module outside  # of the plugin's entry point file. It is here for simplicity.  def   _setup_logger (): \n     # This will log the time, level, filename, line number, and log message. \n     log_message_format   =   '[ %(asctime)s ] [ %(levelname)s ] [ %(filename)s : %(lineno)d ]  %(message)s ' \n     log_message_date_format   =   '%Y-%m- %d  %H:%M:%S' \n\n     # Create a custom formatter. This will help with diagnosability. \n     formatter   =   logging . Formatter ( log_message_format ,   datefmt =   log_message_date_format ) \n\n     platform_handler   =   libs . PlatformHandler () \n     platform_handler . setFormatter ( formatter ) \n\n     logger   =   logging . getLogger () \n     logger . addHandler ( platform_handler ) \n\n     # By default the root logger's level is logging.WARNING. \n     logger . setLevel ( logging . DEBUG )  # Setup the logger.  _setup_logger ()  # logging.getLogger(__name__) is the convention way to get a logger in Python.  # It returns a new logger per module and will be a child of the root logger.  # Since we setup the root logger, nothing else needs to be done to set this  # one up.  logger   =   logging . getLogger ( __name__ )  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ):  \n   return   [ RepositoryDefinition ( 'Logging Example' )]  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n   logger . debug ( 'About to get DB version' ) \n   version_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_db_version.sh' )) \n   logger . debug ( 'About to get DB users' ) \n   users_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_db_users.sh' )) \n   logger . debug ( 'About to get databases' ) \n   db_results   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_databases.sh' )) \n   logger . debug ( 'About to get DB statuses' ) \n   status_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_database_statuses.sh' )) \n   logger . debug ( 'Done collecting data' ) \n\n   # Return an empty list for simplicity. In reality \n   # something would be done with the results above. \n   return   []   When you look at the log file, perhaps you'll see something like this:  [Worker-360|JOB-315|ENVIRONMENT_DISCOVER(UNIX_HOST_ENVIRONMENT-5)] [2019-04-30 12:10:42] [DEBUG] [python_runner.py:44] About to get DB version\n[Worker-360|JOB-316|DB_SYNC(APPDATA_CONTAINER-21)] [2019-04-30 12:19:35] [DEBUG] [python_runner.py:49] About to get DB users\n[Worker-325|JOB-280|ENVIRONMENT_REFRESH(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:51] About to get databases\n[Worker-326|JOB-281|SOURCES_DISABLE(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:53] About to get DB statuses  You can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes!  We now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem.",
            "title": "Example"
        },
        {
            "location": "/References/Logging/#how-to-retrieve-logs",
            "text": "Download a support bundle by going to  Help  >  Support Logs   and select  Download . The logs will be in a the support bundle under  log/mgmt_log/plugin_log/<plugin name> .",
            "title": "How to retrieve logs"
        },
        {
            "location": "/References/Logging/#logging-levels",
            "text": "Python has a number of  preset logging levels  and allows for custom ones as well. Since logging on the Virtualization Platform uses the  logging  framework, log statements of all levels are supported.  However, the Virtualization Platform will map all logging levels into three files:  debug.log ,  info.log , and  error.log  in the following way:     Python Logging Level  Logging File      DEBUG  debug.log    INFO  info.log    WARN  error.log    WARNING  error.log    ERROR  error.log    CRITICAL  error.log     As is the case with the  logging  framework, logging statements are hierarchical: logging statements made at the  logging.DEBUG  level will be written only to  debug.log  while logging statements made at the  logging.ERROR  level will be written to  debug.log ,  info.log , and  error.log .",
            "title": "Logging Levels"
        },
        {
            "location": "/References/Logging/#sensitive-data",
            "text": "Remember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on  sensitive data",
            "title": "Sensitive data"
        },
        {
            "location": "/References/Workflows/",
            "text": "Workflows\n\u00b6\n\n\nLegend\n\u00b6\n\n\n\n\nEnvironment Discovery / Refresh\n\u00b6\n\n\n\n\nLinked Source Sync\n\u00b6\n\n\n\n\nLinked Source Enable\n\u00b6\n\n\n\n\nLinked Source Disable\n\u00b6\n\n\n\n\nLinked Source Delete\n\u00b6\n\n\n\n\nVirtual Source Provision\n\u00b6\n\n\n\n\nVirtual Source Snapshot\n\u00b6\n\n\n\n\nVirtual Source Refresh\n\u00b6\n\n\n\n\nVirtual Source Rollback\n\u00b6\n\n\n\n\nVirtual Source Delete\n\u00b6\n\n\n\n\nVirtual Source Start\n\u00b6\n\n\n\n\nVirtual Source Stop\n\u00b6\n\n\n\n\nVirtual Source Enable\n\u00b6\n\n\n\n\nVirtual Source Disable\n\u00b6",
            "title": "Workflows"
        },
        {
            "location": "/References/Workflows/#workflows",
            "text": "",
            "title": "Workflows"
        },
        {
            "location": "/References/Workflows/#legend",
            "text": "",
            "title": "Legend"
        },
        {
            "location": "/References/Workflows/#environment-discovery-refresh",
            "text": "",
            "title": "Environment Discovery / Refresh"
        },
        {
            "location": "/References/Workflows/#linked-source-sync",
            "text": "",
            "title": "Linked Source Sync"
        },
        {
            "location": "/References/Workflows/#linked-source-enable",
            "text": "",
            "title": "Linked Source Enable"
        },
        {
            "location": "/References/Workflows/#linked-source-disable",
            "text": "",
            "title": "Linked Source Disable"
        },
        {
            "location": "/References/Workflows/#linked-source-delete",
            "text": "",
            "title": "Linked Source Delete"
        },
        {
            "location": "/References/Workflows/#virtual-source-provision",
            "text": "",
            "title": "Virtual Source Provision"
        },
        {
            "location": "/References/Workflows/#virtual-source-snapshot",
            "text": "",
            "title": "Virtual Source Snapshot"
        },
        {
            "location": "/References/Workflows/#virtual-source-refresh",
            "text": "",
            "title": "Virtual Source Refresh"
        },
        {
            "location": "/References/Workflows/#virtual-source-rollback",
            "text": "",
            "title": "Virtual Source Rollback"
        },
        {
            "location": "/References/Workflows/#virtual-source-delete",
            "text": "",
            "title": "Virtual Source Delete"
        },
        {
            "location": "/References/Workflows/#virtual-source-start",
            "text": "",
            "title": "Virtual Source Start"
        },
        {
            "location": "/References/Workflows/#virtual-source-stop",
            "text": "",
            "title": "Virtual Source Stop"
        },
        {
            "location": "/References/Workflows/#virtual-source-enable",
            "text": "",
            "title": "Virtual Source Enable"
        },
        {
            "location": "/References/Workflows/#virtual-source-disable",
            "text": "",
            "title": "Virtual Source Disable"
        },
        {
            "location": "/References/Classes/",
            "text": "Classes\n\u00b6\n\n\nDirectSource\n\u00b6\n\n\nRepresents a Linked Source object and its properties when using a \nDirect Linking\n strategy.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nDirectSource\n\n\n\ndirect_source\n \n=\n \nDirectSource\n(\nguid\n,\n \nconnection\n,\n \nparameters\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nguid\n\n\nString\n\n\nUnique Identifier for the source.\n\n\n\n\n\n\nconnection\n\n\nRemoteConnection\n\n\nConnection for the source environment.\n\n\n\n\n\n\nparameters\n\n\nLinkedSourceDefinition\n\n\nUser input as per the \nLinkedSource Schema\n.\n\n\n\n\n\n\n\n\nStagedSource\n\u00b6\n\n\nRepresents a Linked Source object and its properties when using a \nStaged Linking\n strategy.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nStagedSource\n\n\n\nstaged_source\n \n=\n \nStagedSource\n(\nguid\n,\n \nsource_connection\n,\n \nparameters\n,\n \nmount\n,\n \nstaged_connection\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nguid\n\n\nString\n\n\nUnique Identifier for the source.\n\n\n\n\n\n\nsource_connection\n\n\nRemoteConnection\n\n\nConnection for the source environment.\n\n\n\n\n\n\nparameters\n\n\nLinkedSourceDefinition\n\n\nUser input as per the \nLinkedSource Schema\n.\n\n\n\n\n\n\nmount\n\n\nMount\n\n\nMount point associated with the source.\n\n\n\n\n\n\nstaged_connection\n\n\nRemoteConnection\n\n\nConnection for the staging environment.\n\n\n\n\n\n\n\n\nVirtualSource\n\u00b6\n\n\nRepresents a Virtual Source object and its properties.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nVirtualSource\n\n\n\nvirtual_source\n \n=\n \nVirtualSource\n(\nguid\n,\n \nconnection\n,\n \nparameters\n,\n \nmounts\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nguid\n\n\nString\n\n\nUnique Identifier for the source.\n\n\n\n\n\n\nconnection\n\n\nRemoteConnection\n\n\nConnection for the source environment.\n\n\n\n\n\n\nparameters\n\n\nVirtualSourceDefinition\n\n\nUser input as per the \nVirtualSource Schema\n.\n\n\n\n\n\n\nmounts\n\n\nlist[\nMount\n]\n\n\nMount points associated with the source.\n\n\n\n\n\n\n\n\nRemoteConnection\n\u00b6\n\n\nRepresents a connection to a source.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nRemoteConnection\n\n\n\nconnection\n \n=\n \nRemoteConnection\n(\nenvironment\n,\n \nuser\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nenvironment\n\n\nRemoteEnvironment\n\n\nEnvironment for the connection. Internal virtualization platform object.\n\n\n\n\n\n\nuser\n\n\nRemoteUser\n\n\nUser for the connection. Internal virtualization platform object.\n\n\n\n\n\n\n\n\nStatus\n\u00b6\n\n\nAn enum used to represent the state of a linked or virtual source and whether is functioning as expected.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nStatus\n\n\n\nstatus\n \n=\n \nStatus\n.\nACTIVE\n\n\n\n\n\n\nValues\n\u00b6\n\n\n\n\n\n\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nACTIVE\n\n\nSource is healthy and functioning as expected.\n\n\n\n\n\n\nINACTIVE\n\n\nSource is not functioning as expected.\n\n\n\n\n\n\n\n\nMount\n\u00b6\n\n\nRepresents a mount exported and mounted to a remote host.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMount\n\n\n\nmount\n \n=\n \nMount\n(\nenvironment\n,\n \npath\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_environment\n\n\nRemoteEnvironment\n\n\nEnvironment for the connection. Internal virtualization platform object.\n\n\n\n\n\n\nmount_path\n\n\nString\n\n\nThe path on the remote host that has the mounted data set.\n\n\n\n\n\n\nshared_path\n\n\nString\n\n\nOptional.\n The path of the subdirectory of the data set to mount to the remote host.\n\n\n\n\n\n\n\n\nOwnershipSpecification\n\u00b6\n\n\nRepresents how to set the onwership for a data set. This only applies to Unix Hosts.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nOwnershipSpecification\n\n\n\nownership_specification\n \n=\n \nOwnershipSpecification\n(\nuid\n,\n \ngid\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nuid\n\n\nInteger\n\n\nThe user id to set the ownership of the data set to.\n\n\n\n\n\n\ngid\n\n\nInteger\n\n\nThe group id to set the ownership of the data set to.\n\n\n\n\n\n\n\n\nMountSpecification\n\u00b6\n\n\nRepresents properties for the mount associated with an exported data set.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMountSpecification\n\n\n\nmount_specification\n \n=\n \nMountSpecification\n([\nmount\n],\n \nownership_specification\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmounts\n\n\nlist[\nMount\n]\n\n\nThe list of mounts to export the data sets to.\n\n\n\n\n\n\nownership_specification\n\n\nOwnershipSpecification\n\n\nOptional.\n Control the ownership attributes for the data set. It defaults to the environment user of the remote environment if it is not specified.\n\n\n\n\n\n\n\n\nSnapshotParametersDefinition\n\u00b6\n\n\nUser provided parameters for the snapshot operation. It includes a boolean property named \nresync\n that can be used to indicate to the plugin whether or not to initiate a full ingestion of the dSource. The parameters are only be set during a manual snapshot. When using a sync policy \nresync\n defaults to \nfalse\n.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \nif\n \nsnapshot_parameter\n.\nresync\n:\n\n      \nprint\n(\nsnapshot_parameter\n.\nresync\n)\n\n\n\n\n\n\n\n\nThis class will be generated during build and is located with the autogenerated classes. As it is passed into the operation, importing it is not neccessary.\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nresync\n\n\nBoolean\n\n\nDetermines if this snapshot should ingest the dSource from scratch.\n\n\n\n\n\n\n\n\nRemoteEnvironment\n\u00b6\n\n\nRepresents a remote environment.\n\n\n\n\nWarning\n\n\nObjects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nName of the environment.\n\n\n\n\n\n\nhost\n\n\nRemoteHost\n\n\nHost that belongs to the environment. Internal virtualization platform object.\n\n\n\n\n\n\nreference\n\n\nString\n\n\nUnique identifier for the environment.\n\n\n\n\n\n\n\n\nRemoteHost\n\u00b6\n\n\nRepresents a remote host, can we Unix or Windows.\n\n\n\n\nWarning\n\n\nObjects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nHost address.\n\n\n\n\n\n\nbinary_path\n\n\nString\n\n\nPath to Delphix provided binaries on the host, which are present in the toolkit pushed to the remote host like \ndlpx_db_exec\n, \ndlpx_pfexec\n, etc. This property is only available for Unix hosts.\n\n\n\n\n\n\nreference\n\n\nString\n\n\nUnique identifier for the host.\n\n\n\n\n\n\n\n\nRemoteUser\n\u00b6\n\n\nRepresents a user on a remote host.\n\n\n\n\nWarning\n\n\nObjects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nUser name.\n\n\n\n\n\n\nreference\n\n\nString\n\n\nUnique identifier for the user.",
            "title": "Classes"
        },
        {
            "location": "/References/Classes/#classes",
            "text": "",
            "title": "Classes"
        },
        {
            "location": "/References/Classes/#directsource",
            "text": "Represents a Linked Source object and its properties when using a  Direct Linking  strategy.  from   dlpx.virtualization.platform   import   DirectSource  direct_source   =   DirectSource ( guid ,   connection ,   parameters )",
            "title": "DirectSource"
        },
        {
            "location": "/References/Classes/#fields",
            "text": "Field  Type  Description      guid  String  Unique Identifier for the source.    connection  RemoteConnection  Connection for the source environment.    parameters  LinkedSourceDefinition  User input as per the  LinkedSource Schema .",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#stagedsource",
            "text": "Represents a Linked Source object and its properties when using a  Staged Linking  strategy.  from   dlpx.virtualization.platform   import   StagedSource  staged_source   =   StagedSource ( guid ,   source_connection ,   parameters ,   mount ,   staged_connection )",
            "title": "StagedSource"
        },
        {
            "location": "/References/Classes/#fields_1",
            "text": "Field  Type  Description      guid  String  Unique Identifier for the source.    source_connection  RemoteConnection  Connection for the source environment.    parameters  LinkedSourceDefinition  User input as per the  LinkedSource Schema .    mount  Mount  Mount point associated with the source.    staged_connection  RemoteConnection  Connection for the staging environment.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#virtualsource",
            "text": "Represents a Virtual Source object and its properties.  from   dlpx.virtualization.platform   import   VirtualSource  virtual_source   =   VirtualSource ( guid ,   connection ,   parameters ,   mounts )",
            "title": "VirtualSource"
        },
        {
            "location": "/References/Classes/#fields_2",
            "text": "Field  Type  Description      guid  String  Unique Identifier for the source.    connection  RemoteConnection  Connection for the source environment.    parameters  VirtualSourceDefinition  User input as per the  VirtualSource Schema .    mounts  list[ Mount ]  Mount points associated with the source.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remoteconnection",
            "text": "Represents a connection to a source.  from   dlpx.virtualization.platform   import   RemoteConnection  connection   =   RemoteConnection ( environment ,   user )",
            "title": "RemoteConnection"
        },
        {
            "location": "/References/Classes/#fields_3",
            "text": "Field  Type  Description      environment  RemoteEnvironment  Environment for the connection. Internal virtualization platform object.    user  RemoteUser  User for the connection. Internal virtualization platform object.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#status",
            "text": "An enum used to represent the state of a linked or virtual source and whether is functioning as expected.  from   dlpx.virtualization.platform   import   Status  status   =   Status . ACTIVE",
            "title": "Status"
        },
        {
            "location": "/References/Classes/#values",
            "text": "Value  Description      ACTIVE  Source is healthy and functioning as expected.    INACTIVE  Source is not functioning as expected.",
            "title": "Values"
        },
        {
            "location": "/References/Classes/#mount",
            "text": "Represents a mount exported and mounted to a remote host.  from   dlpx.virtualization.platform   import   Mount  mount   =   Mount ( environment ,   path )",
            "title": "Mount"
        },
        {
            "location": "/References/Classes/#fields_4",
            "text": "Field  Type  Description      remote_environment  RemoteEnvironment  Environment for the connection. Internal virtualization platform object.    mount_path  String  The path on the remote host that has the mounted data set.    shared_path  String  Optional.  The path of the subdirectory of the data set to mount to the remote host.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#ownershipspecification",
            "text": "Represents how to set the onwership for a data set. This only applies to Unix Hosts.  from   dlpx.virtualization.platform   import   OwnershipSpecification  ownership_specification   =   OwnershipSpecification ( uid ,   gid )",
            "title": "OwnershipSpecification"
        },
        {
            "location": "/References/Classes/#fields_5",
            "text": "Field  Type  Description      uid  Integer  The user id to set the ownership of the data set to.    gid  Integer  The group id to set the ownership of the data set to.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#mountspecification",
            "text": "Represents properties for the mount associated with an exported data set.  from   dlpx.virtualization.platform   import   MountSpecification  mount_specification   =   MountSpecification ([ mount ],   ownership_specification )",
            "title": "MountSpecification"
        },
        {
            "location": "/References/Classes/#fields_6",
            "text": "Field  Type  Description      mounts  list[ Mount ]  The list of mounts to export the data sets to.    ownership_specification  OwnershipSpecification  Optional.  Control the ownership attributes for the data set. It defaults to the environment user of the remote environment if it is not specified.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#snapshotparametersdefinition",
            "text": "User provided parameters for the snapshot operation. It includes a boolean property named  resync  that can be used to indicate to the plugin whether or not to initiate a full ingestion of the dSource. The parameters are only be set during a manual snapshot. When using a sync policy  resync  defaults to  false .  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   if   snapshot_parameter . resync : \n       print ( snapshot_parameter . resync )    This class will be generated during build and is located with the autogenerated classes. As it is passed into the operation, importing it is not neccessary.",
            "title": "SnapshotParametersDefinition"
        },
        {
            "location": "/References/Classes/#fields_7",
            "text": "Field  Type  Description      resync  Boolean  Determines if this snapshot should ingest the dSource from scratch.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remoteenvironment",
            "text": "Represents a remote environment.   Warning  Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.",
            "title": "RemoteEnvironment"
        },
        {
            "location": "/References/Classes/#fields_8",
            "text": "Field  Type  Description      name  String  Name of the environment.    host  RemoteHost  Host that belongs to the environment. Internal virtualization platform object.    reference  String  Unique identifier for the environment.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remotehost",
            "text": "Represents a remote host, can we Unix or Windows.   Warning  Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.",
            "title": "RemoteHost"
        },
        {
            "location": "/References/Classes/#fields_9",
            "text": "Field  Type  Description      name  String  Host address.    binary_path  String  Path to Delphix provided binaries on the host, which are present in the toolkit pushed to the remote host like  dlpx_db_exec ,  dlpx_pfexec , etc. This property is only available for Unix hosts.    reference  String  Unique identifier for the host.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remoteuser",
            "text": "Represents a user on a remote host.   Warning  Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.",
            "title": "RemoteUser"
        },
        {
            "location": "/References/Classes/#fields_10",
            "text": "Field  Type  Description      name  String  User name.    reference  String  Unique identifier for the user.",
            "title": "Fields"
        },
        {
            "location": "/References/Plugin_Versioning/",
            "text": "Plugin Versioning\n\u00b6\n\n\nLike any other piece of software, a plugin will change over time. New features will be added, bugs will be fixed, and so on.\n\n\nTo keep track of this, plugins must specify a version string. There are rules about what sorts of plugin changes go along with changes to the version string. Before we get into the rules, let's talk about a problem that we want to avoid.\n\n\nProblems With Data Format Mismatches\n\u00b6\n\n\nPlugins supply \nschemas\n to define their own datatypes. Data that conforms to these schemas is saved by the Delphix Engine. Later, the Delphix Engine may read back that saved data, and provide it to plugin code.\n\n\nImagine this sequence of events:\n\n\n\n\nA plugin is initially released. In its snapshot schema, it defines two properties, \ndate\n and \ntime\n, that together specify when the snapshot was taken.\n\n\nA user installs the initial release of the plugin on their Delphix Engine.\n\n\nThe user takes a snapshot of a \ndSource\n. Along with this snapshot is stored the \ndate\n and \ntime\n.\n\n\nA new version of the same plugin is released. In this new version, the snapshot schema now only defines a single property called \ntimestamp\n, which specified both the date and the time together in a single property.\n\n\nThe user installs the new plugin version.\n\n\nThe user attempts to \nprovision\n a new \nVDB\n from the snapshot they took in step 3.\n\n\n\n\nNow, when provision-related plugin code is called (for example the \nconfigure\n operation), it is going to be handed the snapshot data that was stored in step 2.\n\n\nThe problem here is that we'll have a data format mismatch. The previously-saved snapshot data will have separate \ndate\n and \ntime\n fields, but the new plugin code will be expecting instead a single field called \ntimestamp\n.\n\n\nData Upgrading\n\u00b6\n\n\nComing Soon!\n\n\nVersioning Rules\n\u00b6\n\n\nEach plugin declares a version string in the format \n<major>.<minor>.<patch>\n. The \nmajor\n and \nminor\n parts must always be integers, but \npatch\n can be any alphanumeric string.\n\n\nThere are two scenarios where one version of a plugin can be installed on an engine that already has another version of the same plugin installed.\n\n\nPatch-only Changes\n\u00b6\n\n\nIf only the \npatch\n part of the version is changing, there are a relaxed set of rules:\n\n\n\n\nSchemas \nmay not\n change.\n\n\nThere is no defined ordering for patches. So long as \nmajor\n and \nminor\n do not change, any patch level can replace any other patch level.\n\n\n\n\nMajor/Minor Changes\n\u00b6\n\n\nIf either \nmajor\n or \nminor\n (or both) is changing, then the following rules are applied:\n\n\n\n\nThe major/minor pair \nmay not\n decrease. If you have version \n1.2.x\n already installed, then for example you can install \n1.3.y\n or \n2.0.y\n. But, you are not allowed to \"downgrade\" to version \n1.1.z\n.\n\n\nSchemas \nmay\n change.\n\n\nThe plugin \nmust\n provide upgrade operations so that old-format data can be converted as necessary.",
            "title": "Plugin Versioning"
        },
        {
            "location": "/References/Plugin_Versioning/#plugin-versioning",
            "text": "Like any other piece of software, a plugin will change over time. New features will be added, bugs will be fixed, and so on.  To keep track of this, plugins must specify a version string. There are rules about what sorts of plugin changes go along with changes to the version string. Before we get into the rules, let's talk about a problem that we want to avoid.",
            "title": "Plugin Versioning"
        },
        {
            "location": "/References/Plugin_Versioning/#problems-with-data-format-mismatches",
            "text": "Plugins supply  schemas  to define their own datatypes. Data that conforms to these schemas is saved by the Delphix Engine. Later, the Delphix Engine may read back that saved data, and provide it to plugin code.  Imagine this sequence of events:   A plugin is initially released. In its snapshot schema, it defines two properties,  date  and  time , that together specify when the snapshot was taken.  A user installs the initial release of the plugin on their Delphix Engine.  The user takes a snapshot of a  dSource . Along with this snapshot is stored the  date  and  time .  A new version of the same plugin is released. In this new version, the snapshot schema now only defines a single property called  timestamp , which specified both the date and the time together in a single property.  The user installs the new plugin version.  The user attempts to  provision  a new  VDB  from the snapshot they took in step 3.   Now, when provision-related plugin code is called (for example the  configure  operation), it is going to be handed the snapshot data that was stored in step 2.  The problem here is that we'll have a data format mismatch. The previously-saved snapshot data will have separate  date  and  time  fields, but the new plugin code will be expecting instead a single field called  timestamp .",
            "title": "Problems With Data Format Mismatches"
        },
        {
            "location": "/References/Plugin_Versioning/#data-upgrading",
            "text": "Coming Soon!",
            "title": "Data Upgrading"
        },
        {
            "location": "/References/Plugin_Versioning/#versioning-rules",
            "text": "Each plugin declares a version string in the format  <major>.<minor>.<patch> . The  major  and  minor  parts must always be integers, but  patch  can be any alphanumeric string.  There are two scenarios where one version of a plugin can be installed on an engine that already has another version of the same plugin installed.",
            "title": "Versioning Rules"
        },
        {
            "location": "/References/Plugin_Versioning/#patch-only-changes",
            "text": "If only the  patch  part of the version is changing, there are a relaxed set of rules:   Schemas  may not  change.  There is no defined ordering for patches. So long as  major  and  minor  do not change, any patch level can replace any other patch level.",
            "title": "Patch-only Changes"
        },
        {
            "location": "/References/Plugin_Versioning/#majorminor-changes",
            "text": "If either  major  or  minor  (or both) is changing, then the following rules are applied:   The major/minor pair  may not  decrease. If you have version  1.2.x  already installed, then for example you can install  1.3.y  or  2.0.y . But, you are not allowed to \"downgrade\" to version  1.1.z .  Schemas  may  change.  The plugin  must  provide upgrade operations so that old-format data can be converted as necessary.",
            "title": "Major/Minor Changes"
        },
        {
            "location": "/References/Glossary/",
            "text": "Glossary\n\u00b6\n\n\nArtifact\n\u00b6\n\n\nA single file that is the result of a \nbuild\n. It is this artifact which is distributed to users, and which is installed onto engines.\n\n\nAutomatic Discovery\n\u00b6\n\n\nDiscovery\n which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information.\n\n\nBuilding\n\u00b6\n\n\nThe process of creating an \nartifact\n from the collection of files that make up the plugin's source code.\n\n\nDecorator\n\u00b6\n\n\nA Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation.\n\n\nDirect Linking\n\u00b6\n\n\nA strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment.\n\n\nDiscovery\n\u00b6\n\n\nThe process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets.\n\n\ndSource\n\u00b6\n\n\nSee \nLinked Dataset\n\n\nEnvironment\n\u00b6\n\n\nA remote system that the Delphix Engine can interact with. An environment can be used as a \nsource\n, \nstaging\n or \ntarget\n environment (or any combination of those).  For example, a Linux machine that the Delphix Engine can connect to is an environment.\n\n\nLinked Dataset\n\u00b6\n\n\nA dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a \ndSource\n.\n\n\nLinked Source\n\u00b6\n\n\nAn object on the Delphix Engine that holds information related to a \nlinked dataset\n.\n\n\nLinking\n\u00b6\n\n\nThe process by which the Delphix Engine connects a new \ndSource\n to a pre-existing dataset on a source environment.\n\n\nLogging\n\u00b6\n\n\nLogging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin.\n\n\nPlugin Config\n\u00b6\n\n\nA \nYAML\n file containing a list of plugin properties: What is the plugin's name? What version of the plugin is this? Etc. More details \nhere\n.\n\n\nManual Discovery\n\u00b6\n\n\nDiscovery\n which the end user does by manually entering the necessary information into the Delphix Engine.\n\n\nMount Specification\n\u00b6\n\n\nA collection of information, provided by the plugin, which give all the details about how and where \nvirtual datasets\n should be mounted onto \ntarget environments\n. This term is often shortened to \"Mount Spec\".\n\n\nPassword Properties\n\u00b6\n\n\nIn \nschemas\n, any string property can be tagged with \n\"format\": \"password\"\n. This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen.\n\n\nPlatform Libraries\n\u00b6\n\n\nA set of Python functions that are provided by the Virtualization Platform. Plugins use these library functions to request that the Virtualization Platform do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry.\n\n\nPlugin\n\u00b6\n\n\nA tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset.\n\n\nPlugin Operation\n\u00b6\n\n\nA piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function.\nFor example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database.\n\n\nProvisioning\n\u00b6\n\n\nThe process of making a virtual copy of a dataset and making it available for use on a target environment.\n\n\nRepository\n\u00b6\n\n\nInformation that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS.\n\n\nSchema\n\u00b6\n\n\nA formal description of a data type. Plugins use JSON format for their \nschemas\n.\n\n\nSnapshot\n\u00b6\n\n\nRepresents a snapshot of a dataset and its associated metadata represented by the \nSnapshotDefinition Schema\n\n\nSnaphot\n\u00b6\n\n\nA point-in-time read-only copy of a dataset.\n\n\nSnapshot Parameter\n\u00b6\n\n\nUser provided parameters for the snapshot operation. Currently the only properties the parameter has is resync.\n\n\nSource Config\n\u00b6\n\n\nA collection of information that the Delphix Engine needs to interact with a dataset (whether \nlinked\n or \nvirtual\n on an \nenvironment\n.\n\n\nSource Environment\n\u00b6\n\n\nAn \nenvironment\n containing data that is ingested by the Delphix Engine.\n\n\nStaged Linking\n\u00b6\n\n\nA strategy where a \nstaging environment\n is used to coordinate the ingestion of data into a \ndsource\n.\n\n\nStaging Environment\n\u00b6\n\n\nAn \nenvironment\n used by the Delphix Engine to coordinate ingestion from a \nsource environment\n.\n\n\nSyncing\n\u00b6\n\n\nThe process by which the Delphix Engine ingests data from a dataset on a \nsource environment\n into a \ndsource\n. Syncing always happens immediately after \nlinking\n, and typically is done periodically thereafter.\n\n\nTarget Environment\n\u00b6\n\n\nAn \nenvironment\n on which Delphix-provided virtualized datasets can be used.\n\n\nUpgrade Operation\n\u00b6\n\n\nA special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin.\n\n\nVDB\n\u00b6\n\n\nSee \nVirtual Dataset\n\n\nVersion\n\u00b6\n\n\nA string identifier that is unique for every public release of a plugin.\n\n\nVirtual Dataset\n\u00b6\n\n\nA dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a \ntarget environment\n. A virtual dataset is often called a \"VDB\".\n\n\nVirtual Source\n\u00b6\n\n\nAn object on the Delphix Engine that holds information related to a \nvirtual dataset\n.\n\n\nYAML\n\u00b6\n\n\nYAML is a simple language often used for configuration files. Plugins define their \nplugin config\n using YAML.",
            "title": "Glossary"
        },
        {
            "location": "/References/Glossary/#glossary",
            "text": "",
            "title": "Glossary"
        },
        {
            "location": "/References/Glossary/#artifact",
            "text": "A single file that is the result of a  build . It is this artifact which is distributed to users, and which is installed onto engines.",
            "title": "Artifact"
        },
        {
            "location": "/References/Glossary/#automatic-discovery",
            "text": "Discovery  which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information.",
            "title": "Automatic Discovery"
        },
        {
            "location": "/References/Glossary/#building",
            "text": "The process of creating an  artifact  from the collection of files that make up the plugin's source code.",
            "title": "Building"
        },
        {
            "location": "/References/Glossary/#decorator",
            "text": "A Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation.",
            "title": "Decorator"
        },
        {
            "location": "/References/Glossary/#direct-linking",
            "text": "A strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment.",
            "title": "Direct Linking"
        },
        {
            "location": "/References/Glossary/#discovery",
            "text": "The process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets.",
            "title": "Discovery"
        },
        {
            "location": "/References/Glossary/#dsource",
            "text": "See  Linked Dataset",
            "title": "dSource"
        },
        {
            "location": "/References/Glossary/#environment",
            "text": "A remote system that the Delphix Engine can interact with. An environment can be used as a  source ,  staging  or  target  environment (or any combination of those).  For example, a Linux machine that the Delphix Engine can connect to is an environment.",
            "title": "Environment"
        },
        {
            "location": "/References/Glossary/#linked-dataset",
            "text": "A dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a  dSource .",
            "title": "Linked Dataset"
        },
        {
            "location": "/References/Glossary/#linked-source",
            "text": "An object on the Delphix Engine that holds information related to a  linked dataset .",
            "title": "Linked Source"
        },
        {
            "location": "/References/Glossary/#linking",
            "text": "The process by which the Delphix Engine connects a new  dSource  to a pre-existing dataset on a source environment.",
            "title": "Linking"
        },
        {
            "location": "/References/Glossary/#logging",
            "text": "Logging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin.",
            "title": "Logging"
        },
        {
            "location": "/References/Glossary/#plugin-config",
            "text": "A  YAML  file containing a list of plugin properties: What is the plugin's name? What version of the plugin is this? Etc. More details  here .",
            "title": "Plugin Config"
        },
        {
            "location": "/References/Glossary/#manual-discovery",
            "text": "Discovery  which the end user does by manually entering the necessary information into the Delphix Engine.",
            "title": "Manual Discovery"
        },
        {
            "location": "/References/Glossary/#mount-specification",
            "text": "A collection of information, provided by the plugin, which give all the details about how and where  virtual datasets  should be mounted onto  target environments . This term is often shortened to \"Mount Spec\".",
            "title": "Mount Specification"
        },
        {
            "location": "/References/Glossary/#password-properties",
            "text": "In  schemas , any string property can be tagged with  \"format\": \"password\" . This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen.",
            "title": "Password Properties"
        },
        {
            "location": "/References/Glossary/#platform-libraries",
            "text": "A set of Python functions that are provided by the Virtualization Platform. Plugins use these library functions to request that the Virtualization Platform do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry.",
            "title": "Platform Libraries"
        },
        {
            "location": "/References/Glossary/#plugin",
            "text": "A tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset.",
            "title": "Plugin"
        },
        {
            "location": "/References/Glossary/#plugin-operation",
            "text": "A piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function.\nFor example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database.",
            "title": "Plugin Operation"
        },
        {
            "location": "/References/Glossary/#provisioning",
            "text": "The process of making a virtual copy of a dataset and making it available for use on a target environment.",
            "title": "Provisioning"
        },
        {
            "location": "/References/Glossary/#repository",
            "text": "Information that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS.",
            "title": "Repository"
        },
        {
            "location": "/References/Glossary/#schema",
            "text": "A formal description of a data type. Plugins use JSON format for their  schemas .",
            "title": "Schema"
        },
        {
            "location": "/References/Glossary/#snapshot",
            "text": "Represents a snapshot of a dataset and its associated metadata represented by the  SnapshotDefinition Schema",
            "title": "Snapshot"
        },
        {
            "location": "/References/Glossary/#snaphot",
            "text": "A point-in-time read-only copy of a dataset.",
            "title": "Snaphot"
        },
        {
            "location": "/References/Glossary/#snapshot-parameter",
            "text": "User provided parameters for the snapshot operation. Currently the only properties the parameter has is resync.",
            "title": "Snapshot Parameter"
        },
        {
            "location": "/References/Glossary/#source-config",
            "text": "A collection of information that the Delphix Engine needs to interact with a dataset (whether  linked  or  virtual  on an  environment .",
            "title": "Source Config"
        },
        {
            "location": "/References/Glossary/#source-environment",
            "text": "An  environment  containing data that is ingested by the Delphix Engine.",
            "title": "Source Environment"
        },
        {
            "location": "/References/Glossary/#staged-linking",
            "text": "A strategy where a  staging environment  is used to coordinate the ingestion of data into a  dsource .",
            "title": "Staged Linking"
        },
        {
            "location": "/References/Glossary/#staging-environment",
            "text": "An  environment  used by the Delphix Engine to coordinate ingestion from a  source environment .",
            "title": "Staging Environment"
        },
        {
            "location": "/References/Glossary/#syncing",
            "text": "The process by which the Delphix Engine ingests data from a dataset on a  source environment  into a  dsource . Syncing always happens immediately after  linking , and typically is done periodically thereafter.",
            "title": "Syncing"
        },
        {
            "location": "/References/Glossary/#target-environment",
            "text": "An  environment  on which Delphix-provided virtualized datasets can be used.",
            "title": "Target Environment"
        },
        {
            "location": "/References/Glossary/#upgrade-operation",
            "text": "A special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin.",
            "title": "Upgrade Operation"
        },
        {
            "location": "/References/Glossary/#vdb",
            "text": "See  Virtual Dataset",
            "title": "VDB"
        },
        {
            "location": "/References/Glossary/#version",
            "text": "A string identifier that is unique for every public release of a plugin.",
            "title": "Version"
        },
        {
            "location": "/References/Glossary/#virtual-dataset",
            "text": "A dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a  target environment . A virtual dataset is often called a \"VDB\".",
            "title": "Virtual Dataset"
        },
        {
            "location": "/References/Glossary/#virtual-source",
            "text": "An object on the Delphix Engine that holds information related to a  virtual dataset .",
            "title": "Virtual Source"
        },
        {
            "location": "/References/Glossary/#yaml",
            "text": "YAML is a simple language often used for configuration files. Plugins define their  plugin config  using YAML.",
            "title": "YAML"
        },
        {
            "location": "/Best_Practices/Code_Sharing/",
            "text": "Code Sharing\n\u00b6\n\n\nAll Python modules inside of \nsrcDir\n can be imported just as they would be if the plugin was executing locally. When a plugin operation is executed \nsrcDir\n is the current working directory so all imports need to be relative to \nsrcDir\n regardless of the path of the module doing the import.\n\n\nPlease refer to Python's \ndocumentation on modules\n to learn more about modules and imports.\n\n\nExample\n\u00b6\n\n\nAssume we have the following file structure:\n\n\npostgres\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 operations\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 discovery.py\n    \u251c\u2500\u2500 plugin_runner.py\n    \u251c\u2500\u2500 resources\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 execute_sql.sh\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 list_installs.sh\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 list_schemas.sql\n    \u2514\u2500\u2500 utils\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 execution_util.py\n\n\n\n\n\nAny module in the plugin could import \nexecution_util.py\n with \nfrom utils import execution_util\n.\n\n\n\n\nGotcha\n\n\nSince the platform uses Python 2.7, every directory needs to have an \n__init__.py\n file in it otherwise the modules and resources in the folder will not be found at runtime. For more information on \n__init__.py\n files refer to Python's \ndocumentation on packages\n.\n\n\nNote that the \nsrcDir\n in the plugin config file (\nsrc\n in this example) does \nnot\n need an \n__init__.py\n file.\n\n\n\n\nAssume \nschema.json\n contains:\n\n\n{\n    \"repositoryDefinition\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": { \"type\": \"string\" }\n        },\n        \"nameField\": \"name\",\n        \"identityFields\": [\"name\"]\n    },\n    \"sourceConfigDefinition\": {\n        \"type\": \"object\",\n        \"required\": [\"name\"],\n        \"additionalProperties\": false,\n        \"properties\": {\n            \"name\": { \"type\": \"string\" }\n        },\n        \"nameField\": \"name\",\n        \"identityFields\": [\"name\"]\n    }\n}\n\n\n\n\n\nTo keep the code cleaner, this plugin does two things:\n\n\n\n\nSplits discovery logic into its own module: \ndiscovery.py\n.\n\n\nUses two helper funtions \nexecute_sql\n and \nexecute_shell\n in \nutils/execution_util.py\n to abstract all remote execution.\n\n\n\n\nplugin_runner.py\n\u00b6\n\n\nWhen the platform needs to execute a plugin operation, it always calls into the function decorated by the \nentryPoint\n object. The rest of the control flow is determined by the plugin. In order to split logic, the decorated function must delegate into the appropriate module. Below is an example of \nplugin_runner.py\n delegating into \ndiscovery.py\n to handle repository and source config discovery:\n\n\nfrom\n \noperations\n \nimport\n \ndiscovery\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n    \nreturn\n \ndiscovery\n.\nfind_installs\n(\nsource_connection\n);\n\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n    \nreturn\n \ndiscovery\n.\nfind_schemas\n(\nsource_connection\n,\n \nrepository\n)\n\n\n\n\n\n\n\n\nNote\n\n\ndiscovery.py\n is in the \noperations\n package so it is imported with \nfrom operations import discovery\n.\n\n\n\n\ndiscovery.py\n\u00b6\n\n\nIn \ndiscovery.py\n the plugin delegates even further to split business logic away from remote execution. \nutils/execution_util.py\n deals with remote execution and error handling so \ndiscovery.py\n can focus on business logic. Note that \ndiscovery.py\n still needs to know the format of the return value from each script.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nRepositoryDefinition\n,\n \nSourceConfigDefinition\n\n\nfrom\n \nutils\n \nimport\n \nexecution_util\n\n\n\n\ndef\n \nfind_installs\n(\nsource_connection\n):\n\n    \ninstalls\n \n=\n \nexecution_util\n.\nexecute_shell\n(\nsource_connection\n,\n \n'list_installs.sh'\n)\n\n\n    \n# Assume 'installs' is a comma separated list of the names of Postgres installations.\n\n    \ninstall_names\n \n=\n \ninstalls\n.\nsplit\n(\n','\n)\n\n    \nreturn\n \n[\nRepositoryDefinition\n(\nname\n=\nname\n)\n \nfor\n \nname\n \nin\n \ninstall_names\n]\n\n\n\n\ndef\n \nfind_schemas\n(\nsource_connection\n,\n \nrepository\n):\n\n    \nschemas\n \n=\n \nexecution_util\n.\nexecute_sql\n(\nsource_connection\n,\n \nrepository\n.\nname\n,\n \n'list_schemas.sql'\n)\n\n\n    \n# Assume 'schemas' is a comma separated list of the schema names.\n\n    \nschema_names\n \n=\n \nschemas\n.\nsplit\n(\n','\n)\n\n    \nreturn\n \n[\nSourceConfigDefinition\n(\nname\n=\nname\n)\n \nfor\n \nname\n \nin\n \nschema_names\n]\n\n\n\n\n\n\n\n\nNote\n\n\nEven though \ndiscovery.py\n is in the \noperations\n package, the import for \nexecution_util\n is still relative to the \nsrcDir\n specified in the plugin config file. \nexecution_util\n is in the \nutils\n package so it is imported with \nfrom utils import execution_util\n. \n\n\n\n\nexecution_util.py\n\u00b6\n\n\nexecution_util.py\n has two methods \nexecute_sql\n and \nexecute_shell\n. \nexecute_sql\n takes the name of a SQL script in \nresources/\n and executes it with \nresources/execute_sql.sh\n. \nexecute_shell\n takes the name of a shell script in \nresources/\n and executes it.\n\n\nimport\n \npkgutil\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\n\ndef\n \nexecute_sql\n(\nsource_connection\n,\n \ninstall_name\n,\n \nscript_name\n):\n\n    \npsql_script\n \n=\n \npkgutil\n.\nget_data\n(\n\"resources\"\n,\n \n\"execute_sql.sh\"\n)\n\n    \nsql_script\n \n=\n \npkgutil\n.\nget_data\n(\n\"resources\"\n,\n \nscript_name\n)\n\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\n\n        \nsource_connection\n,\n \npsql_script\n,\n \nvariables\n=\n{\n\"SCRIPT\"\n:\n \nsql_script\n},\n \ncheck\n=\nTrue\n\n    \n)\n\n    \nreturn\n \nresult\n.\nstdout\n\n\n\n\ndef\n \nexecute_shell\n(\nsource_connection\n,\n \nscript_name\n):\n\n    \nscript\n \n=\n \npkgutil\n.\nget_data\n(\n\"resources\"\n,\n \nscript_name\n)\n\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \nscript\n,\n \ncheck\n=\nTrue\n)\n\n    \nreturn\n \nresult\n.\nstdout\n\n\n\n\n\n\n\n\nNote\n\n\nBoth \nexecute_sql\n and \nexecute_shell\n use the \ncheck\n parameter which will cause an error to be raised if the exit code is non-zero. For more information refer to the \nrun_bash\n \ndocumentation\n.",
            "title": "Code Sharing"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#code-sharing",
            "text": "All Python modules inside of  srcDir  can be imported just as they would be if the plugin was executing locally. When a plugin operation is executed  srcDir  is the current working directory so all imports need to be relative to  srcDir  regardless of the path of the module doing the import.  Please refer to Python's  documentation on modules  to learn more about modules and imports.",
            "title": "Code Sharing"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#example",
            "text": "Assume we have the following file structure:  postgres\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 operations\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 discovery.py\n    \u251c\u2500\u2500 plugin_runner.py\n    \u251c\u2500\u2500 resources\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 execute_sql.sh\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 list_installs.sh\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 list_schemas.sql\n    \u2514\u2500\u2500 utils\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 execution_util.py  Any module in the plugin could import  execution_util.py  with  from utils import execution_util .   Gotcha  Since the platform uses Python 2.7, every directory needs to have an  __init__.py  file in it otherwise the modules and resources in the folder will not be found at runtime. For more information on  __init__.py  files refer to Python's  documentation on packages .  Note that the  srcDir  in the plugin config file ( src  in this example) does  not  need an  __init__.py  file.   Assume  schema.json  contains:  {\n    \"repositoryDefinition\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": { \"type\": \"string\" }\n        },\n        \"nameField\": \"name\",\n        \"identityFields\": [\"name\"]\n    },\n    \"sourceConfigDefinition\": {\n        \"type\": \"object\",\n        \"required\": [\"name\"],\n        \"additionalProperties\": false,\n        \"properties\": {\n            \"name\": { \"type\": \"string\" }\n        },\n        \"nameField\": \"name\",\n        \"identityFields\": [\"name\"]\n    }\n}  To keep the code cleaner, this plugin does two things:   Splits discovery logic into its own module:  discovery.py .  Uses two helper funtions  execute_sql  and  execute_shell  in  utils/execution_util.py  to abstract all remote execution.",
            "title": "Example"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#plugin_runnerpy",
            "text": "When the platform needs to execute a plugin operation, it always calls into the function decorated by the  entryPoint  object. The rest of the control flow is determined by the plugin. In order to split logic, the decorated function must delegate into the appropriate module. Below is an example of  plugin_runner.py  delegating into  discovery.py  to handle repository and source config discovery:  from   operations   import   discovery  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ): \n     return   discovery . find_installs ( source_connection );  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n     return   discovery . find_schemas ( source_connection ,   repository )    Note  discovery.py  is in the  operations  package so it is imported with  from operations import discovery .",
            "title": "plugin_runner.py"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#discoverypy",
            "text": "In  discovery.py  the plugin delegates even further to split business logic away from remote execution.  utils/execution_util.py  deals with remote execution and error handling so  discovery.py  can focus on business logic. Note that  discovery.py  still needs to know the format of the return value from each script.  from   dlpx.virtualization   import   libs  from   generated.definitions   import   RepositoryDefinition ,   SourceConfigDefinition  from   utils   import   execution_util  def   find_installs ( source_connection ): \n     installs   =   execution_util . execute_shell ( source_connection ,   'list_installs.sh' ) \n\n     # Assume 'installs' is a comma separated list of the names of Postgres installations. \n     install_names   =   installs . split ( ',' ) \n     return   [ RepositoryDefinition ( name = name )   for   name   in   install_names ]  def   find_schemas ( source_connection ,   repository ): \n     schemas   =   execution_util . execute_sql ( source_connection ,   repository . name ,   'list_schemas.sql' ) \n\n     # Assume 'schemas' is a comma separated list of the schema names. \n     schema_names   =   schemas . split ( ',' ) \n     return   [ SourceConfigDefinition ( name = name )   for   name   in   schema_names ]    Note  Even though  discovery.py  is in the  operations  package, the import for  execution_util  is still relative to the  srcDir  specified in the plugin config file.  execution_util  is in the  utils  package so it is imported with  from utils import execution_util .",
            "title": "discovery.py"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#execution_utilpy",
            "text": "execution_util.py  has two methods  execute_sql  and  execute_shell .  execute_sql  takes the name of a SQL script in  resources/  and executes it with  resources/execute_sql.sh .  execute_shell  takes the name of a shell script in  resources/  and executes it.  import   pkgutil  from   dlpx.virtualization   import   libs  def   execute_sql ( source_connection ,   install_name ,   script_name ): \n     psql_script   =   pkgutil . get_data ( \"resources\" ,   \"execute_sql.sh\" ) \n     sql_script   =   pkgutil . get_data ( \"resources\" ,   script_name ) \n\n     result   =   libs . run_bash ( \n         source_connection ,   psql_script ,   variables = { \"SCRIPT\" :   sql_script },   check = True \n     ) \n     return   result . stdout  def   execute_shell ( source_connection ,   script_name ): \n     script   =   pkgutil . get_data ( \"resources\" ,   script_name ) \n\n     result   =   libs . run_bash ( source_connection ,   script ,   check = True ) \n     return   result . stdout    Note  Both  execute_sql  and  execute_shell  use the  check  parameter which will cause an error to be raised if the exit code is non-zero. For more information refer to the  run_bash   documentation .",
            "title": "execution_util.py"
        },
        {
            "location": "/Best_Practices/Managing_Scripts_For_Remote_Execution/",
            "text": "Managing Scripts for Remote Execution\n\u00b6\n\n\nTo execute a PowerShell or Bash script or Expect script on a remote host, you must provide the script as a string to \nrun_powershell\n or \nrun_bash\n or \nrun_expect\n. While you can keep these strings as literals in your Python code, best practice is to keep them as resource files in your source directory and access them with \npkgutil\n.\n\n\npkgutil\n is part of the standard Python library. The method that is applicable to resources is \npkgutil.get_data\n.\n\n\nBasic Usage\n\u00b6\n\n\nGiven the following plugin structure:\n\n\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 plugin_runner.py\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 get_date.sh\n\n\n\n\n\nAssume \nSnapshotDefinition\n is:\n\n\n\"snapshotDefinition\": {\n    \"type\" : \"object\",\n    \"additionalProperties\" : false,\n    \"properties\" : {\n        \"name\": {\"type\": \"string\"},\n        \"date\": {\"type\": \"string\"}\n    }\n}\n\n\n\n\n\nand \nsrc/resources/get_date.sh\n contains:\n\n\n1\n2\n#!/usr/bin/env bash\n\ndate\n\n\n\n\n\n\nIf \nget_date.sh\n is needed in \npost_snapshot\n, it can be retrieved and executed:\n\n\nimport\n \npkgutil\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \npost_snapshot\n(\ndirect_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n    \n# Retrieve script contents\n\n    \nscript_content\n \n=\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_date.sh'\n)\n\n\n    \n# Execute script on remote host\n\n    \nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\ndirect_source\n.\nconnection\n,\n \nscript_content\n)\n\n\n    \n# Fail operation if the timestamp couldn't be retrieved\n\n    \nif\n \nresponse\n.\nexit_code\n \n!=\n \n0\n:\n\n        \nraise\n \nRuntimeError\n(\n'Failed to get date: {}'\n.\nformat\n(\nresponse\n.\nstdout\n))\n\n\n    \nreturn\n \nSnapshotDefinition\n(\nname\n=\n'Snapshot'\n,\n \ndate\n=\nresponse\n.\nstdout\n)\n\n\n\n\n\n\n\n\nPython's Working Directory\n\n\nThis assumes that \nsrc/\n is Python's current working directory. This is the behavior of the Virtualization Platform.\n\n\n\n\n\n\nResources need to be in a Python module\n\n\npkgutil.get_data\n cannot retrieve the contents of a resource that is not in a Python package. This means that a resource that is in the first level of your source directory will not be retrievable with \npkgutil\n. Resources must be in a subdirectory of your source directory, and that subdirectory must contain an \n__init__.py\n file.\n\n\n\n\nMulti-level Packages\n\u00b6\n\n\nGiven the following plugin structure:\n\n\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 plugin_runner.py\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 database\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 execute_sql.sh\n        \u2514\u2500\u2500 platform\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 get_date.sh\n\n\n\n\n\nThe contents of \nsrc/resources/platform/get_date.sh\n can be retrieved with:\n\n\nscript_content\n \n=\n \npkgutil\n.\nget_data\n(\n'resources.platform'\n,\n \n'get_date.sh'\n)",
            "title": "Managing Scripts for Remote Execution"
        },
        {
            "location": "/Best_Practices/Managing_Scripts_For_Remote_Execution/#managing-scripts-for-remote-execution",
            "text": "To execute a PowerShell or Bash script or Expect script on a remote host, you must provide the script as a string to  run_powershell  or  run_bash  or  run_expect . While you can keep these strings as literals in your Python code, best practice is to keep them as resource files in your source directory and access them with  pkgutil .  pkgutil  is part of the standard Python library. The method that is applicable to resources is  pkgutil.get_data .",
            "title": "Managing Scripts for Remote Execution"
        },
        {
            "location": "/Best_Practices/Managing_Scripts_For_Remote_Execution/#basic-usage",
            "text": "Given the following plugin structure:  \u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 plugin_runner.py\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 get_date.sh  Assume  SnapshotDefinition  is:  \"snapshotDefinition\": {\n    \"type\" : \"object\",\n    \"additionalProperties\" : false,\n    \"properties\" : {\n        \"name\": {\"type\": \"string\"},\n        \"date\": {\"type\": \"string\"}\n    }\n}  and  src/resources/get_date.sh  contains:  1\n2 #!/usr/bin/env bash \ndate   If  get_date.sh  is needed in  post_snapshot , it can be retrieved and executed:  import   pkgutil  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.linked.post_snapshot ()  def   post_snapshot ( direct_source ,   repository ,   source_config ): \n     # Retrieve script contents \n     script_content   =   pkgutil . get_data ( 'resources' ,   'get_date.sh' ) \n\n     # Execute script on remote host \n     response   =   libs . run_bash ( direct_source . connection ,   script_content ) \n\n     # Fail operation if the timestamp couldn't be retrieved \n     if   response . exit_code   !=   0 : \n         raise   RuntimeError ( 'Failed to get date: {}' . format ( response . stdout )) \n\n     return   SnapshotDefinition ( name = 'Snapshot' ,   date = response . stdout )    Python's Working Directory  This assumes that  src/  is Python's current working directory. This is the behavior of the Virtualization Platform.    Resources need to be in a Python module  pkgutil.get_data  cannot retrieve the contents of a resource that is not in a Python package. This means that a resource that is in the first level of your source directory will not be retrievable with  pkgutil . Resources must be in a subdirectory of your source directory, and that subdirectory must contain an  __init__.py  file.",
            "title": "Basic Usage"
        },
        {
            "location": "/Best_Practices/Managing_Scripts_For_Remote_Execution/#multi-level-packages",
            "text": "Given the following plugin structure:  \u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 plugin_runner.py\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 database\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 execute_sql.sh\n        \u2514\u2500\u2500 platform\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 get_date.sh  The contents of  src/resources/platform/get_date.sh  can be retrieved with:  script_content   =   pkgutil . get_data ( 'resources.platform' ,   'get_date.sh' )",
            "title": "Multi-level Packages"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/",
            "text": "Dealing With Sensitive Data\n\u00b6\n\n\nOften, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password.\n\n\nPlugins must be careful to handle sensitive data appropriately. Three tips for handling sensitive data are:\n\n\n\n\nTell the Delphix Engine which parts of your data are sensitive.\n\n\nWhen passing sensitive data to remote plugin library functions (such as \nrun_bash\n), use environment variables.\n\n\nAvoid logging, or otherwise writing out the sensitive data.\n\n\n\n\nEach of these tips are explained below.\n\n\nMarking Your Data As Sensitive\n\u00b6\n\n\nBecause the Delphix Engine manages the storing and retrieving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its \nschemas\n, by using the special \npassword\n keyword.\n\n\nThe following example of a schema defines an object with three properties, one of which is sensitive and tagged with the \npassword\n keyword:\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"db_connectionPort\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n        \n\"db_username\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n        \n\"db_password\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n,\n \n\"format\"\n:\n \n\"password\"\n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nThis tells the Delphix Engine to take special precautions with this password property, as follows:\n\n\n\n\nThe Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass back to the plugin.\n\n\nThe Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs).\n\n\nThe Delphix Engine's UI and CLI will not display the password.\n\n\nClients of the Delphix Engine's public API will not be able to access the password.\n\n\n\n\nUsing Environment Variables For Remote Data Passing\n\u00b6\n\n\nSometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a \nstaging environment\n, and that database command will need to use a password.\n\n\nExample\n\u00b6\n\n\nLet us take a look at a very simple example where we need to shutdown a database called \"inventory\" on a target environment by using the \ndb_cmd shutdown inventory\n command. This command will ask for a password on \nstdin\n, and for our example our password is \"hunter2\".\n\n\nIf we were running this command by hand, it might look like this:\n\n\n$ db_cmd shutdown inventory\nConnecting to database instance...\nPlease enter database password:\n\n\n\n\n\nAt this point, we would type in \"hunter2\", and the command would proceed to shut down the database.\n\n\nSince a plugin cannot type in the password by hand, it will do something like this instead:\n\n\n$ \necho\n \n\"hunter2\"\n \n|\n db_cmd shutdown inventory\n\n\n\n\n\nDon't Do This\n\u00b6\n\n\nFirst, let us take a look at how \nnot\n to do this! Here is a bit of plugin python code that will run the above command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.stop\n()\n\n\ndef\n \nmy_virtual_stop\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# THIS IS INSECURE! DO NOT DO THIS!\n\n  \nfull_command\n \n=\n \n\"echo {} | db_cmd shutdown {}\"\n.\nformat\n(\npassword\n,\n \ndb_name\n)\n\n  \nlibs\n.\nrun_bash\n(\nvirtual_source\n.\nconnection\n,\n \nfull_command\n)\n\n\n\n\n\n\nThis constructs a Python string containing exactly the desired command from above. However, this is not recommended.\n\n\nThe problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Virtualization Platform. For example, suppose the Virtualization Platform cannot make a connection to the target environment. In which case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message.\n\n\nUsing Environment Variables\n\u00b6\n\n\nThe Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let us look at a different way to run the same command as above.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.stop\n()\n\n  \n# Use environment variables to pass sensitive data to remote commands\n\n  \nenvironment_vars\n \n=\n \n{\n\n    \n\"DATABASE_PASSWORD\"\n \n:\n \npassword\n\n  \n}\n\n  \nfull_command\n \n=\n \n\"echo $DATABASE_PASSWORD | db_cmd shutdown {}\"\n.\nformat\n(\ndb_name\n)\n\n  \nlibs\n.\nrun_bash\n(\nvirtual_source\n.\nconnection\n,\n \nfull_command\n,\n \nvariables\n=\nenvironment_vars\n)\n\n\n\n\n\n\n\n\nNote\n\n\nWe are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Virtualization Platform to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself.\n\n\n\n\nOnce the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected.\n\n\nUnlike with the command string, the Virtualization Platform \ndoes\n treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc.\n\n\nDon't Write Out Sensitive Data\n\u00b6\n\n\nPlugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins.\n\n\nThe Virtualization Platform provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to \nnever log sensitive data\n.\n\n\nIn addition, remember that your plugin is not treated as sensitive by the Virtualization Platform. Plugin code is distributed unencrypted, and is viewable in cleartext by Delphix Engine users. Sensitive data such as passwords should never be hard-coded in your plugin code.",
            "title": "Dealing With Sensitive Data"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dealing-with-sensitive-data",
            "text": "Often, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password.  Plugins must be careful to handle sensitive data appropriately. Three tips for handling sensitive data are:   Tell the Delphix Engine which parts of your data are sensitive.  When passing sensitive data to remote plugin library functions (such as  run_bash ), use environment variables.  Avoid logging, or otherwise writing out the sensitive data.   Each of these tips are explained below.",
            "title": "Dealing With Sensitive Data"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#marking-your-data-as-sensitive",
            "text": "Because the Delphix Engine manages the storing and retrieving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its  schemas , by using the special  password  keyword.  The following example of a schema defines an object with three properties, one of which is sensitive and tagged with the  password  keyword:  { \n     \"type\" :   \"object\" , \n     \"properties\" :   { \n         \"db_connectionPort\" :   { \"type\" :   \"string\" }, \n         \"db_username\" :   { \"type\" :   \"string\" }, \n         \"db_password\" :   { \"type\" :   \"string\" ,   \"format\" :   \"password\" } \n     }  }   This tells the Delphix Engine to take special precautions with this password property, as follows:   The Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass back to the plugin.  The Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs).  The Delphix Engine's UI and CLI will not display the password.  Clients of the Delphix Engine's public API will not be able to access the password.",
            "title": "Marking Your Data As Sensitive"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#using-environment-variables-for-remote-data-passing",
            "text": "Sometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a  staging environment , and that database command will need to use a password.",
            "title": "Using Environment Variables For Remote Data Passing"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#example",
            "text": "Let us take a look at a very simple example where we need to shutdown a database called \"inventory\" on a target environment by using the  db_cmd shutdown inventory  command. This command will ask for a password on  stdin , and for our example our password is \"hunter2\".  If we were running this command by hand, it might look like this:  $ db_cmd shutdown inventory\nConnecting to database instance...\nPlease enter database password:  At this point, we would type in \"hunter2\", and the command would proceed to shut down the database.  Since a plugin cannot type in the password by hand, it will do something like this instead:  $  echo   \"hunter2\"   |  db_cmd shutdown inventory",
            "title": "Example"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dont-do-this",
            "text": "First, let us take a look at how  not  to do this! Here is a bit of plugin python code that will run the above command.  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.stop ()  def   my_virtual_stop ( virtual_source ,   repository ,   source_config ): \n   # THIS IS INSECURE! DO NOT DO THIS! \n   full_command   =   \"echo {} | db_cmd shutdown {}\" . format ( password ,   db_name ) \n   libs . run_bash ( virtual_source . connection ,   full_command )   This constructs a Python string containing exactly the desired command from above. However, this is not recommended.  The problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Virtualization Platform. For example, suppose the Virtualization Platform cannot make a connection to the target environment. In which case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message.",
            "title": "Don't Do This"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#using-environment-variables",
            "text": "The Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let us look at a different way to run the same command as above.  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.stop () \n   # Use environment variables to pass sensitive data to remote commands \n   environment_vars   =   { \n     \"DATABASE_PASSWORD\"   :   password \n   } \n   full_command   =   \"echo $DATABASE_PASSWORD | db_cmd shutdown {}\" . format ( db_name ) \n   libs . run_bash ( virtual_source . connection ,   full_command ,   variables = environment_vars )    Note  We are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Virtualization Platform to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself.   Once the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected.  Unlike with the command string, the Virtualization Platform  does  treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc.",
            "title": "Using Environment Variables"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dont-write-out-sensitive-data",
            "text": "Plugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins.  The Virtualization Platform provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to  never log sensitive data .  In addition, remember that your plugin is not treated as sensitive by the Virtualization Platform. Plugin code is distributed unencrypted, and is viewable in cleartext by Delphix Engine users. Sensitive data such as passwords should never be hard-coded in your plugin code.",
            "title": "Don't Write Out Sensitive Data"
        },
        {
            "location": "/Best_Practices/Working_with_Powershell/",
            "text": "Error handling in Powershell\n\u00b6\n\n\n\n\nInfo\n\n\nCommands run via run_powershell are executed as a script. The exit code returned by run_powershell as part of the RunPowershellResult is determined by the exit code from the script.\n\n\n\n\nPowerShell gives you a few ways to handle errors in your scripts:\n\n\n\n\n\n\nSet $ErrorActionPreference. This only applies to PowerShell Cmdlets. For scripts or other executables such as sqlcmd, PowerShell will return with exit code 0 even if there is an error, regardless of the value of $ErrorActionPrefe       rence. The allowable values for $ErrorActionPreference are:\n\n\nContinue (default) \u2013 Continue even if there is an error.                          \n\n  SilentlyContinue \u2013 Acts like Continue with the exception that errors are not displayed         \n\n  Inquire \u2013 Prompts the user in case of error          \n\n  Stop -  Stops execution after the first error\n\n\n\n\n\n\nUse exception handling by using traps and try/catch blocks or if statements to detect errors and return with non-zero exit codes\n\n\n\n\n\n\nUse custom error handling that can be invoked after launching each command in the script to correctly detect errors. \n\n\n\n\n\n\nExamples\n\u00b6\n\n\nThe following example will show you how setting $ErrorActionPreference will return exit codes\n\n\nIn the below code, \nls nothing123\n is expected to fail.\n\n\nls nothing123\nWrite-Host \"Test\"\n\n\n\n\n\nHere is the output when the above commands runs  on a remote host and the script will return the value of \n$?\n to be True eventhough the script failed.\n\n\n```PS C:\\Users\\dtully\\test> ./test1.ps1\nls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist.\nAt C:\\Users\\dtully\\test\\test1.ps1:1 char:1\n+ ls nothing123\n+ ~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundEx\n   ception\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand\n\n\nPS C:\\Users\\dtully\\test> Write-Host $?\nTrue\n\n\nNow lets set $ErrorActionPreference=Stop.\n\n```Windows\n$ErrorActionPreference = \"Stop\"\nls nothing123\nWrite-Host \"Test\"\n\n\n\n\n\nNow when we run the command again we see the return value of \n$?\n to be False.\n\n\nPS C:\\Users\\dtully\\test> ./test1.ps1\nls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist.\nAt C:\\Users\\dtully\\test\\test1.ps1:2 char:1\n+ ls nothing123\n+ ~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundException\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand\n\nPS C:\\Users\\dtully\\test> Write-Host $?\nFalse\n\n\n\n\n\nThe following example shows how you can use the function verifySuccess to detect whether the previous command failed, and if it did print, print an error message and return with an exit code of 1.\n\n\nfunction die {\n    Write-Error \"Error: $($args[0])\"\n    exit 1\n}\n\nfunction verifySuccess {\n    if (!$?) {\n        die \"$($args[0])\"\n    }\n}\n\nWrite-Output \"I'd rather be in Hawaii\"\nverifySuccess \"WRITE_OUTPUT_FAILED\"\n\n& C:\\Program Files\\Delphix\\scripts\\myscript.ps1\nverifySuccess \"MY_SCRIPT_FAILED\"",
            "title": "Working with Powershell"
        },
        {
            "location": "/Best_Practices/Working_with_Powershell/#error-handling-in-powershell",
            "text": "Info  Commands run via run_powershell are executed as a script. The exit code returned by run_powershell as part of the RunPowershellResult is determined by the exit code from the script.   PowerShell gives you a few ways to handle errors in your scripts:    Set $ErrorActionPreference. This only applies to PowerShell Cmdlets. For scripts or other executables such as sqlcmd, PowerShell will return with exit code 0 even if there is an error, regardless of the value of $ErrorActionPrefe       rence. The allowable values for $ErrorActionPreference are:  Continue (default) \u2013 Continue even if there is an error.                           \n  SilentlyContinue \u2013 Acts like Continue with the exception that errors are not displayed          \n  Inquire \u2013 Prompts the user in case of error           \n  Stop -  Stops execution after the first error    Use exception handling by using traps and try/catch blocks or if statements to detect errors and return with non-zero exit codes    Use custom error handling that can be invoked after launching each command in the script to correctly detect errors.",
            "title": "Error handling in Powershell"
        },
        {
            "location": "/Best_Practices/Working_with_Powershell/#examples",
            "text": "The following example will show you how setting $ErrorActionPreference will return exit codes  In the below code,  ls nothing123  is expected to fail.  ls nothing123\nWrite-Host \"Test\"  Here is the output when the above commands runs  on a remote host and the script will return the value of  $?  to be True eventhough the script failed.  ```PS C:\\Users\\dtully\\test> ./test1.ps1\nls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist.\nAt C:\\Users\\dtully\\test\\test1.ps1:1 char:1\n+ ls nothing123\n+ ~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundEx\n   ception\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand  PS C:\\Users\\dtully\\test> Write-Host $?\nTrue  Now lets set $ErrorActionPreference=Stop.\n\n```Windows\n$ErrorActionPreference = \"Stop\"\nls nothing123\nWrite-Host \"Test\"  Now when we run the command again we see the return value of  $?  to be False.  PS C:\\Users\\dtully\\test> ./test1.ps1\nls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist.\nAt C:\\Users\\dtully\\test\\test1.ps1:2 char:1\n+ ls nothing123\n+ ~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundException\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand\n\nPS C:\\Users\\dtully\\test> Write-Host $?\nFalse  The following example shows how you can use the function verifySuccess to detect whether the previous command failed, and if it did print, print an error message and return with an exit code of 1.  function die {\n    Write-Error \"Error: $($args[0])\"\n    exit 1\n}\n\nfunction verifySuccess {\n    if (!$?) {\n        die \"$($args[0])\"\n    }\n}\n\nWrite-Output \"I'd rather be in Hawaii\"\nverifySuccess \"WRITE_OUTPUT_FAILED\"\n\n& C:\\Program Files\\Delphix\\scripts\\myscript.ps1\nverifySuccess \"MY_SCRIPT_FAILED\"",
            "title": "Examples"
        },
        {
            "location": "/Release_Notes/0.4.0/",
            "text": "Release - Early Preview 2 (v0.4.0)\n\u00b6\n\n\nTo install or upgrade the SDK, refer to instructions \nhere\n.\n\n\nNew & Improved\n\u00b6\n\n\n\n\nAdded a new CLI command \ndownload-logs\n to enable downloading plugin generated logs from the Delphix Engine.\n\n\n\n\nAdded an optional argument named \ncheck\n to the following \nplatform library\n functions:\n\n\n\n\nrun_bash\n\n\nrun_powershell\n\n\n\n\nWith \ncheck=true\n, the platform library function checks the \nexit_code\n and raises an exception if it is non-zero.\n\n\n\n\n\n\nModified \ninit\n to auto-generate default implementations for all required plugin operations.\n\n\n\n\nImproved \nbuild\n validation for:\n\n\nRequired \nplugin operations\n.\n\n\nIncorrect \nplugin operation\n argument names.\n\n\nPlugin Config\n \nentryPoint\n: The \nentryPoint\n is now imported during the \nbuild\n as part of the validation.\n\n\nSchemas\n: Validated to conform to the \nJSON Schema Draft-07 Specification\n.\n\n\n\n\n\n\n\n\nImproved runtime validation and error messages for:\n\n\n\n\nObjects returned from \nplugin operations\n.\n\n\nPlatform Classes\n during instantiation.\n\n\nPlatform Library\n function arguments.\n\n\n\n\n\n\n\n\nAdded support for Docker based plugins by specifying \nrootSquashEnabled: false\n in the \nplugin config\n.\n\n\n\n\nAdded Job and thread information to plugin generated log messages to increase diagnosability and observability.\n\n\n\n\nBreaking Changes\n\u00b6\n\n\n\n\n\n\nA new argument \nsnapshot_parameters\n was added to the following \nstaged\n plugin operations:\n\n\n\n\nStaged Linked Source Pre-Snapshot\n\n\nStaged Linked Source Post-Snapshot\n\n\n\n\nThis argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are \nhere\n.\n\n\nDetailed steps to detect and make changes.\n\n\n\n\n\n\nProperties of the \nStagedSource\n class were modified:\n\n\n\n\nconnection\n was renamed to \nsource_connection\n.\n\n\nstaged_connection\n was added to allow connecting to the staging environment.\n\n\n\n\nThis will enable plugins to connect to both the source and staging environments. More details about these properties are \nhere\n.\n\n\nDetailed steps to detect and make changes.\n\n\n\n\n\n\nFixed\n\u00b6\n\n\n\n\nAllow access to nested package resources via \npkgutil.get_data\n.\n\n\nFixed Out of Memory exceptions.\n\n\n\n\nFixed missing or incorrectly populated properties for the following classes:\n\n\n\n\n\n\n\n\nClass\n\n\nProperties\n\n\n\n\n\n\n\n\n\n\nVirtualSource\n\n\nmounts\n\n\n\n\n\n\nRemoteUser\n\n\nname\n\n\n\n\n\n\nRemoteEnvironment\n\n\nname\n\n\n\n\n\n\nRemoteHost\n\n\nname\n \nbinary_path\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated Job warnings during discovery to display the underlying Python exceptions if one is raised by the plugin operations.\n\n\n\n\nRecreate the plugin's log directory if a plugin is deleted and re-uploaded to the Delphix Engine.\n\n\nMark incorrectly provisioned VDBs as unusable and prevent subsequent Delphix Engine operations on such VDBs.\n\n\nBetter error messages when incorrect environment types are used for Platform Libraries.\n\n\nBetter error messages when a plugin's \nschema\n is updated and the plugin is re-uploaded to the Delphix Engine, with clear instructions on how to proceed.\n\n\nFixed \nbuild\n failures on Windows.",
            "title": "Release - Early Preview 2 (v0.4.0)"
        },
        {
            "location": "/Release_Notes/0.4.0/#release-early-preview-2-v040",
            "text": "To install or upgrade the SDK, refer to instructions  here .",
            "title": "Release - Early Preview 2 (v0.4.0)"
        },
        {
            "location": "/Release_Notes/0.4.0/#new-improved",
            "text": "Added a new CLI command  download-logs  to enable downloading plugin generated logs from the Delphix Engine.   Added an optional argument named  check  to the following  platform library  functions:   run_bash  run_powershell   With  check=true , the platform library function checks the  exit_code  and raises an exception if it is non-zero.    Modified  init  to auto-generate default implementations for all required plugin operations.   Improved  build  validation for:  Required  plugin operations .  Incorrect  plugin operation  argument names.  Plugin Config   entryPoint : The  entryPoint  is now imported during the  build  as part of the validation.  Schemas : Validated to conform to the  JSON Schema Draft-07 Specification .     Improved runtime validation and error messages for:   Objects returned from  plugin operations .  Platform Classes  during instantiation.  Platform Library  function arguments.     Added support for Docker based plugins by specifying  rootSquashEnabled: false  in the  plugin config .   Added Job and thread information to plugin generated log messages to increase diagnosability and observability.",
            "title": "New &amp; Improved"
        },
        {
            "location": "/Release_Notes/0.4.0/#breaking-changes",
            "text": "A new argument  snapshot_parameters  was added to the following  staged  plugin operations:   Staged Linked Source Pre-Snapshot  Staged Linked Source Post-Snapshot   This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are  here .  Detailed steps to detect and make changes.    Properties of the  StagedSource  class were modified:   connection  was renamed to  source_connection .  staged_connection  was added to allow connecting to the staging environment.   This will enable plugins to connect to both the source and staging environments. More details about these properties are  here .  Detailed steps to detect and make changes.",
            "title": "Breaking Changes"
        },
        {
            "location": "/Release_Notes/0.4.0/#fixed",
            "text": "Allow access to nested package resources via  pkgutil.get_data .  Fixed Out of Memory exceptions.   Fixed missing or incorrectly populated properties for the following classes:     Class  Properties      VirtualSource  mounts    RemoteUser  name    RemoteEnvironment  name    RemoteHost  name   binary_path       Updated Job warnings during discovery to display the underlying Python exceptions if one is raised by the plugin operations.   Recreate the plugin's log directory if a plugin is deleted and re-uploaded to the Delphix Engine.  Mark incorrectly provisioned VDBs as unusable and prevent subsequent Delphix Engine operations on such VDBs.  Better error messages when incorrect environment types are used for Platform Libraries.  Better error messages when a plugin's  schema  is updated and the plugin is re-uploaded to the Delphix Engine, with clear instructions on how to proceed.  Fixed  build  failures on Windows.",
            "title": "Fixed"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/",
            "text": "Breaking Changes - Early Preview 2 (v.0.4.0)\n\u00b6\n\n\nNew Argument \nsnapshot_parameters\n\u00b6\n\n\nA new argument \nsnapshot_parameters\n was added to the following \nstaged\n plugin operations:\n\n\n\n\nStaged Linked Source Pre-Snapshot\n\n\nStaged Linked Source Post-Snapshot\n\n\n\n\nThis argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are \nhere\n.\n\n\nWhat is affected\n\u00b6\n\n\nThis argument applies only to \nstaged\n plugins. The plugin's source code will have to be updated for the following staged plugin operations:\n\n\n\n\nStaged Linked Source Pre-Snapshot\n: This plugin operation is optional and will need to be updated if the plugin implements it.\n\n\nStaged Linked Source Post-Snapshot\n: This plugin operation is required and will need to be updated.\n\n\n\n\nHow does it fail\n\u00b6\n\n\nbuild\n will fail with the following error message if the new argument is not added to the affected staged plugin operations:\n\n\n$ dvp build\nError: Number of arguments \ndo\n not match in method staged_post_snapshot. Expected: \n[\n'staged_source'\n, \n'repository'\n, \n'source_config'\n, \n'snapshot_parameters'\n]\n, Found: \n[\n'repository'\n, \n'source_config'\n, \n'staged_source'\n]\n.\nError: Number of arguments \ndo\n not match in method staged_pre_snapshot. Expected: \n[\n'staged_source'\n, \n'repository'\n, \n'source_config'\n, \n'snapshot_parameters'\n]\n, Found: \n[\n'repository'\n, \n'source_config'\n, \n'staged_source'\n]\n.\n\n\n0\n Warning\n(\ns\n)\n. \n2\n Error\n(\ns\n)\n.\n\nBUILD FAILED.\n\n\n\n\n\nHow to fix it\n\u00b6\n\n\nUpdate the affected staged plugin operations to include the new argument \nsnapshot_parameters\n.\n\n\n\n\nPrevious releases\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot_prior\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# This was the function signature prior to 0.4.0\n\n  \npass\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot_prior\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# This was the function signature prior to 0.4.0\n\n  \nreturn\n \nSnapshotDefinition\n()\n\n\n\n\n\n\n\n\n0.4.0\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot_040\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \n# Updated function signature in 0.4.0\n\n  \npass\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot_040\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \n# Updated function signature in 0.4.0\n\n  \nreturn\n \nSnapshotDefinition\n()\n\n\n\n\n\n\nStagedSource Properties Modified\n\u00b6\n\n\nProperties of the \nStagedSource\n class were modified:\n\n\n\n\nconnection\n was renamed to \nsource_connection\n.\n\n\nstaged_connection\n was added to allow connecting to the staging environment.\n\n\n\n\nThis will enable plugins to connect to both the source and staging environments. More details about these properties are \nhere\n.\n\n\nWhat is affected\n\u00b6\n\n\nThis change applies only to \nstaged\n plugins.\n\n\nRequired Changes\n\u00b6\n\n\nThe plugin's source code will have to be updated for any staged plugin operations that accesses the \nconnection\n propery of a \nStagedSource\n object.\n\n\nOptional Changes\n\u00b6\n\n\nThe plugin can choose to use the new \nstaged_connection\n property to connect to the staging environment of a dSource.\n\n\nHow does it fail\n\u00b6\n\n\nAny Delphix Engine operation that calls a plugin operation that has not been fixed would fail with the following stack trace as part of the output of the user exception:\n\n\nAttributeError\n:\n \n'StagedSource'\n \nobject\n \nhas\n \nno\n \nattribute\n \n'connection'\n\n\n\n\n\n\nHow to fix it\n\u00b6\n\n\nUpdate any staged plugin operations that access the renamed property.\n\n\n\n\nPrevious releases\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot_prior\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# Property name was 'connection' was the name of the property for staged_source prior to 0.4.0\n\n  \nlibs\n.\nrun_bash\n(\nstaged_source\n.\nconnection\n,\n \n'date'\n)\n\n\n\n\n\n\n\n\n0.4.0\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot_prior\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# Property name was updated to 'source_connection' in 0.4.0\n\n  \nlibs\n.\nrun_bash\n(\nstaged_source\n.\nsource_connection\n,\n \n'date'\n)",
            "title": "Breaking Changes - Early Preview 2 (v.0.4.0)"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#breaking-changes-early-preview-2-v040",
            "text": "",
            "title": "Breaking Changes - Early Preview 2 (v.0.4.0)"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#new-argument-snapshot_parameters",
            "text": "A new argument  snapshot_parameters  was added to the following  staged  plugin operations:   Staged Linked Source Pre-Snapshot  Staged Linked Source Post-Snapshot   This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are  here .",
            "title": "New Argument snapshot_parameters"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#what-is-affected",
            "text": "This argument applies only to  staged  plugins. The plugin's source code will have to be updated for the following staged plugin operations:   Staged Linked Source Pre-Snapshot : This plugin operation is optional and will need to be updated if the plugin implements it.  Staged Linked Source Post-Snapshot : This plugin operation is required and will need to be updated.",
            "title": "What is affected"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#how-does-it-fail",
            "text": "build  will fail with the following error message if the new argument is not added to the affected staged plugin operations:  $ dvp build\nError: Number of arguments  do  not match in method staged_post_snapshot. Expected:  [ 'staged_source' ,  'repository' ,  'source_config' ,  'snapshot_parameters' ] , Found:  [ 'repository' ,  'source_config' ,  'staged_source' ] .\nError: Number of arguments  do  not match in method staged_pre_snapshot. Expected:  [ 'staged_source' ,  'repository' ,  'source_config' ,  'snapshot_parameters' ] , Found:  [ 'repository' ,  'source_config' ,  'staged_source' ] . 0  Warning ( s ) .  2  Error ( s ) .\n\nBUILD FAILED.",
            "title": "How does it fail"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#how-to-fix-it",
            "text": "Update the affected staged plugin operations to include the new argument  snapshot_parameters .   Previous releases   from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot_prior ( staged_source ,   repository ,   source_config ): \n   # This was the function signature prior to 0.4.0 \n   pass  @plugin.linked.post_snapshot ()  def   linked_post_snapshot_prior ( staged_source ,   repository ,   source_config ): \n   # This was the function signature prior to 0.4.0 \n   return   SnapshotDefinition ()    0.4.0   from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot_040 ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   # Updated function signature in 0.4.0 \n   pass  @plugin.linked.post_snapshot ()  def   linked_post_snapshot_040 ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   # Updated function signature in 0.4.0 \n   return   SnapshotDefinition ()",
            "title": "How to fix it"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#stagedsource-properties-modified",
            "text": "Properties of the  StagedSource  class were modified:   connection  was renamed to  source_connection .  staged_connection  was added to allow connecting to the staging environment.   This will enable plugins to connect to both the source and staging environments. More details about these properties are  here .",
            "title": "StagedSource Properties Modified"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#what-is-affected_1",
            "text": "This change applies only to  staged  plugins.",
            "title": "What is affected"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#required-changes",
            "text": "The plugin's source code will have to be updated for any staged plugin operations that accesses the  connection  propery of a  StagedSource  object.",
            "title": "Required Changes"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#optional-changes",
            "text": "The plugin can choose to use the new  staged_connection  property to connect to the staging environment of a dSource.",
            "title": "Optional Changes"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#how-does-it-fail_1",
            "text": "Any Delphix Engine operation that calls a plugin operation that has not been fixed would fail with the following stack trace as part of the output of the user exception:  AttributeError :   'StagedSource'   object   has   no   attribute   'connection'",
            "title": "How does it fail"
        },
        {
            "location": "/Release_Notes/0.4.0_Breaking_Changes/#how-to-fix-it_1",
            "text": "Update any staged plugin operations that access the renamed property.   Previous releases   from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization   import   libs  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot_prior ( staged_source ,   repository ,   source_config ): \n   # Property name was 'connection' was the name of the property for staged_source prior to 0.4.0 \n   libs . run_bash ( staged_source . connection ,   'date' )    0.4.0   from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization   import   libs  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot_prior ( staged_source ,   repository ,   source_config ): \n   # Property name was updated to 'source_connection' in 0.4.0 \n   libs . run_bash ( staged_source . source_connection ,   'date' )",
            "title": "How to fix it"
        }
    ]
}