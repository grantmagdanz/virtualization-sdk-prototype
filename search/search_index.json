{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 Hey there! With this Delphix Virtualization SDK documentation we hope you will find all you need to know in order to develop your own plugins! Overview \u00b6 If you already know about plugins, and are looking for something specific, use the links to the left to find what you are looking for or search. If this is your first time here, and you are wondering what developing a Delphix plugin will do for you\u2014read on! What Does a Delphix Plugin do? \u00b6 The Delphix Engine is an appliance that lets you quickly and cheaply make virtual copies of large datasets. The engine has built-in support for interfacing with certain types of datasets, such as Oracle and SQL Server. When you develop a plugin, you enable end users to use your dataset type as if they were using a built-in dataset type, whether it\u2019s MongoDB, Cassandra, or something else. Your plugin will extend the Delphix Engine\u2019s capabilities by teaching it how to run essential virtual data operations on your datasets: How to stop and start them Where to store their data How to make virtual copies These plugin operations are the building blocks of the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality to the datasets you connect to such as: Provisioning Refreshing Rewinding Replication Syncing Where to Start \u00b6 Read through the first few sections of this documentation, and we will walk you through how to get setup for development, then how to develop, build, and deploy your first plugin. Getting Started will show you how to setup the SDK. When you finish this section, you will have a full plugin development environment, and you will be ready to start building. Building Your First Plugin will walk you step-by-step through the process of developing a very simple plugin. With it, you will learn the concepts and techniques that you will need to develop fully-fledged plugins. That does not mean this first plugin is useless\u2014you will be able to virtualize simple datasets with it. Once you complete these sections, use the rest of the documentation whenever you would like. In addition to a full reference section , we include an example of a full-featured plugin that does complicated tasks ( Coming Soon! ).","title":"Welcome!"},{"location":"#welcome","text":"Hey there! With this Delphix Virtualization SDK documentation we hope you will find all you need to know in order to develop your own plugins!","title":"Welcome!"},{"location":"#overview","text":"If you already know about plugins, and are looking for something specific, use the links to the left to find what you are looking for or search. If this is your first time here, and you are wondering what developing a Delphix plugin will do for you\u2014read on!","title":"Overview"},{"location":"#what-does-a-delphix-plugin-do","text":"The Delphix Engine is an appliance that lets you quickly and cheaply make virtual copies of large datasets. The engine has built-in support for interfacing with certain types of datasets, such as Oracle and SQL Server. When you develop a plugin, you enable end users to use your dataset type as if they were using a built-in dataset type, whether it\u2019s MongoDB, Cassandra, or something else. Your plugin will extend the Delphix Engine\u2019s capabilities by teaching it how to run essential virtual data operations on your datasets: How to stop and start them Where to store their data How to make virtual copies These plugin operations are the building blocks of the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality to the datasets you connect to such as: Provisioning Refreshing Rewinding Replication Syncing","title":"What Does a Delphix Plugin do?"},{"location":"#where-to-start","text":"Read through the first few sections of this documentation, and we will walk you through how to get setup for development, then how to develop, build, and deploy your first plugin. Getting Started will show you how to setup the SDK. When you finish this section, you will have a full plugin development environment, and you will be ready to start building. Building Your First Plugin will walk you step-by-step through the process of developing a very simple plugin. With it, you will learn the concepts and techniques that you will need to develop fully-fledged plugins. That does not mean this first plugin is useless\u2014you will be able to virtualize simple datasets with it. Once you complete these sections, use the rest of the documentation whenever you would like. In addition to a full reference section , we include an example of a full-featured plugin that does complicated tasks ( Coming Soon! ).","title":"Where to Start"},{"location":"Getting_Started/","text":"Getting Started \u00b6 The Virtualization SDK is a Python package on PyPI . Install it in your local development environment so that you can build and upload a plugin. The SDK consists of three parts: The dlpx.virtulization.platform module The dlpx.virtualization.libs module A CLI The platform and libs modules expose objects and methods needed to develop a plugin. The CLI is used to build and upload a plugin. Requirements \u00b6 macOS 10.14+, Ubuntu 16.04+, or Windows 10 Python 2.7 (Python 3 is not supported) Java 7+ Installation \u00b6 To install the latest version of the SDK run: $ pip install dvp Use a Virtual Environment We highly recommended that you develop plugins inside of a virtual environment. To learn more about virtual environments, refer to Virtualenv's documentation . The virtual environment needs to use Python 2.7. This is configured when creating the virtualenv: $ virtualenv -p /path/to/python2.7/binary ENV To install a specific version of the SDK run: $ pip install dvp==<version> To upgrade an existing installation of the SDK run: $ pip install dvp --upgrade API Build Version The version of the SDK defines the version of the Virtualization Platform API your plugin will be built against. Basic Usage \u00b6 Our CLI reference describes commands, provides examples, and a help section. To build your plugin: $ dvp build -c <plugin_config> -a <artifact_file> This will generate an upload artifact at <artifact_file> . That file can then be uploaded with: $ dvp upload -e <delphix_engine_address> -u <delphix_admin_user> -a <artifact_file> You will be prompt for the Delphix Engine user's password. Troubleshooting \u00b6 Installation fails with incorrect version spec \u00b6 Error 'install_requires' must be a string or list of strings containing valid project version requirement specifiers; Expected version spec in enum34;python_version < '3.4' at ;python_version < '3.4' This is likely caused by an out of date setuptools version (minimum version 38.0.0 ) which is often due to not installing the SDK into a virtual environment. To fix this, first setup a virtual environment and attempt to install the SDK there. If you are already using a virtual environment you can update setuptools with: $ pip install setuptools --upgrade","title":"Getting Started"},{"location":"Getting_Started/#getting-started","text":"The Virtualization SDK is a Python package on PyPI . Install it in your local development environment so that you can build and upload a plugin. The SDK consists of three parts: The dlpx.virtulization.platform module The dlpx.virtualization.libs module A CLI The platform and libs modules expose objects and methods needed to develop a plugin. The CLI is used to build and upload a plugin.","title":"Getting Started"},{"location":"Getting_Started/#requirements","text":"macOS 10.14+, Ubuntu 16.04+, or Windows 10 Python 2.7 (Python 3 is not supported) Java 7+","title":"Requirements"},{"location":"Getting_Started/#installation","text":"To install the latest version of the SDK run: $ pip install dvp Use a Virtual Environment We highly recommended that you develop plugins inside of a virtual environment. To learn more about virtual environments, refer to Virtualenv's documentation . The virtual environment needs to use Python 2.7. This is configured when creating the virtualenv: $ virtualenv -p /path/to/python2.7/binary ENV To install a specific version of the SDK run: $ pip install dvp==<version> To upgrade an existing installation of the SDK run: $ pip install dvp --upgrade API Build Version The version of the SDK defines the version of the Virtualization Platform API your plugin will be built against.","title":"Installation"},{"location":"Getting_Started/#basic-usage","text":"Our CLI reference describes commands, provides examples, and a help section. To build your plugin: $ dvp build -c <plugin_config> -a <artifact_file> This will generate an upload artifact at <artifact_file> . That file can then be uploaded with: $ dvp upload -e <delphix_engine_address> -u <delphix_admin_user> -a <artifact_file> You will be prompt for the Delphix Engine user's password.","title":"Basic Usage"},{"location":"Getting_Started/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Getting_Started/#installation-fails-with-incorrect-version-spec","text":"Error 'install_requires' must be a string or list of strings containing valid project version requirement specifiers; Expected version spec in enum34;python_version < '3.4' at ;python_version < '3.4' This is likely caused by an out of date setuptools version (minimum version 38.0.0 ) which is often due to not installing the SDK into a virtual environment. To fix this, first setup a virtual environment and attempt to install the SDK there. If you are already using a virtual environment you can update setuptools with: $ pip install setuptools --upgrade","title":"Installation fails with incorrect version spec"},{"location":"Best_Practices/Code_Sharing/","text":"Code Sharing \u00b6 All Python modules inside of srcDir can be imported just as they would be if the plugin was executing locally. When a plugin operation is executed srcDir is the current working directory so all imports need to be relative to srcDir regardless of the path of the module doing the import. Please refer to Python's documentation on modules to learn more about modules and imports. Example \u00b6 Assume we have the following file structure: postgres \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 operations \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 discovery.py \u251c\u2500\u2500 plugin_runner.py \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 execute_sql.sh \u2502 \u251c\u2500\u2500 list_installs.sh \u2502 \u2514\u2500\u2500 list_schemas.sql \u2514\u2500\u2500 utils \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 execution_util.py Any module in the plugin could import execution_util.py with from utils import execution_util . Gotcha Since the platform uses Python 2.7, every directory needs to have an __init__.py file in it otherwise the modules and resources in the folder will not be found at runtime. For more information on __init__.py files refer to Python's documentation on packages . Note that the srcDir in the plugin config file ( src in this example) does not need an __init__.py file. Assume schema.json contains: { \"repositoryDefinition\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" } }, \"nameField\": \"name\", \"identityFields\": [\"name\"] }, \"sourceConfigDefinition\": { \"type\": \"object\", \"required\": [\"name\"], \"additionalProperties\": false, \"properties\": { \"name\": { \"type\": \"string\" } }, \"nameField\": \"name\", \"identityFields\": [\"name\"] } } To keep the code cleaner, this plugin does two things: Splits discovery logic into its own module: discovery.py . Uses two helper funtions execute_sql and execute_shell in utils/execution_util.py to abstract all remote execution. plugin_runner.py \u00b6 When the platform needs to execute a plugin operation, it always calls into the function decorated by the entryPoint object. The rest of the control flow is determined by the plugin. In order to split logic, the decorated function must delegate into the appropriate module. Below is an example of plugin_runner.py delegating into discovery.py to handle repository and source config discovery: from operations import discovery from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.discovery.repository () def repository_discovery ( source_connection ): return discovery . find_installs ( source_connection ); @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): return discovery . find_schemas ( source_connection , repository ) Note discovery.py is in the operations package so it is imported with from operations import discovery . discovery.py \u00b6 In discovery.py the plugin delegates even further to split business logic away from remote execution. utils/execution_util.py deals with remote execution and error handling so discovery.py can focus on business logic. Note that discovery.py still needs to know the format of the return value from each script. from dlpx.virtualization import libs from generated.definitions import RepositoryDefinition , SourceConfigDefinition from utils import execution_util def find_installs ( source_connection ): installs = execution_util . execute_shell ( source_connection , 'list_installs.sh' ) # Assume 'installs' is a comma separated list of the names of Postgres installations. install_names = installs . split ( ',' ) return [ RepositoryDefinition ( name = name ) for name in install_names ] def find_schemas ( source_connection , repository ): schemas = execution_util . execute_sql ( source_connection , repository . name , 'list_schemas.sql' ) # Assume 'schemas' is a comma separated list of the schema names. schema_names = schemas . split ( ',' ) return [ SourceConfigDefinition ( name = name ) for name in schema_names ] Note Even though discovery.py is in the operations package, the import for execution_util is still relative to the srcDir specified in the plugin config file. execution_util is in the utils package so it is imported with from utils import execution_util . execution_util.py \u00b6 execution_util.py has two methods execute_sql and execute_shell . execute_sql takes the name of a SQL script in resources/ and executes it with resources/execute_sql.sh . execute_shell takes the name of a shell script in resources/ and executes it. import pkgutil from dlpx.virtualization import libs def execute_sql ( source_connection , install_name , script_name ): psql_script = pkgutil . get_data ( \"resources\" , \"execute_sql.sh\" ) sql_script = pkgutil . get_data ( \"resources\" , script_name ) result = libs . run_bash ( source_connection , psql_script , variables = { \"SCRIPT\" : sql_script }, check = True ) return result . stdout def execute_shell ( source_connection , script_name ): script = pkgutil . get_data ( \"resources\" , script_name ) result = libs . run_bash ( source_connection , script , check = True ) return result . stdout Note Both execute_sql and execute_shell use the check parameter which will cause an error to be raised if the exit code is non-zero. For more information refer to the run_bash documentation .","title":"Code Sharing"},{"location":"Best_Practices/Code_Sharing/#code-sharing","text":"All Python modules inside of srcDir can be imported just as they would be if the plugin was executing locally. When a plugin operation is executed srcDir is the current working directory so all imports need to be relative to srcDir regardless of the path of the module doing the import. Please refer to Python's documentation on modules to learn more about modules and imports.","title":"Code Sharing"},{"location":"Best_Practices/Code_Sharing/#example","text":"Assume we have the following file structure: postgres \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 operations \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 discovery.py \u251c\u2500\u2500 plugin_runner.py \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 execute_sql.sh \u2502 \u251c\u2500\u2500 list_installs.sh \u2502 \u2514\u2500\u2500 list_schemas.sql \u2514\u2500\u2500 utils \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 execution_util.py Any module in the plugin could import execution_util.py with from utils import execution_util . Gotcha Since the platform uses Python 2.7, every directory needs to have an __init__.py file in it otherwise the modules and resources in the folder will not be found at runtime. For more information on __init__.py files refer to Python's documentation on packages . Note that the srcDir in the plugin config file ( src in this example) does not need an __init__.py file. Assume schema.json contains: { \"repositoryDefinition\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" } }, \"nameField\": \"name\", \"identityFields\": [\"name\"] }, \"sourceConfigDefinition\": { \"type\": \"object\", \"required\": [\"name\"], \"additionalProperties\": false, \"properties\": { \"name\": { \"type\": \"string\" } }, \"nameField\": \"name\", \"identityFields\": [\"name\"] } } To keep the code cleaner, this plugin does two things: Splits discovery logic into its own module: discovery.py . Uses two helper funtions execute_sql and execute_shell in utils/execution_util.py to abstract all remote execution.","title":"Example"},{"location":"Best_Practices/Code_Sharing/#plugin_runnerpy","text":"When the platform needs to execute a plugin operation, it always calls into the function decorated by the entryPoint object. The rest of the control flow is determined by the plugin. In order to split logic, the decorated function must delegate into the appropriate module. Below is an example of plugin_runner.py delegating into discovery.py to handle repository and source config discovery: from operations import discovery from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.discovery.repository () def repository_discovery ( source_connection ): return discovery . find_installs ( source_connection ); @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): return discovery . find_schemas ( source_connection , repository ) Note discovery.py is in the operations package so it is imported with from operations import discovery .","title":"plugin_runner.py"},{"location":"Best_Practices/Code_Sharing/#discoverypy","text":"In discovery.py the plugin delegates even further to split business logic away from remote execution. utils/execution_util.py deals with remote execution and error handling so discovery.py can focus on business logic. Note that discovery.py still needs to know the format of the return value from each script. from dlpx.virtualization import libs from generated.definitions import RepositoryDefinition , SourceConfigDefinition from utils import execution_util def find_installs ( source_connection ): installs = execution_util . execute_shell ( source_connection , 'list_installs.sh' ) # Assume 'installs' is a comma separated list of the names of Postgres installations. install_names = installs . split ( ',' ) return [ RepositoryDefinition ( name = name ) for name in install_names ] def find_schemas ( source_connection , repository ): schemas = execution_util . execute_sql ( source_connection , repository . name , 'list_schemas.sql' ) # Assume 'schemas' is a comma separated list of the schema names. schema_names = schemas . split ( ',' ) return [ SourceConfigDefinition ( name = name ) for name in schema_names ] Note Even though discovery.py is in the operations package, the import for execution_util is still relative to the srcDir specified in the plugin config file. execution_util is in the utils package so it is imported with from utils import execution_util .","title":"discovery.py"},{"location":"Best_Practices/Code_Sharing/#execution_utilpy","text":"execution_util.py has two methods execute_sql and execute_shell . execute_sql takes the name of a SQL script in resources/ and executes it with resources/execute_sql.sh . execute_shell takes the name of a shell script in resources/ and executes it. import pkgutil from dlpx.virtualization import libs def execute_sql ( source_connection , install_name , script_name ): psql_script = pkgutil . get_data ( \"resources\" , \"execute_sql.sh\" ) sql_script = pkgutil . get_data ( \"resources\" , script_name ) result = libs . run_bash ( source_connection , psql_script , variables = { \"SCRIPT\" : sql_script }, check = True ) return result . stdout def execute_shell ( source_connection , script_name ): script = pkgutil . get_data ( \"resources\" , script_name ) result = libs . run_bash ( source_connection , script , check = True ) return result . stdout Note Both execute_sql and execute_shell use the check parameter which will cause an error to be raised if the exit code is non-zero. For more information refer to the run_bash documentation .","title":"execution_util.py"},{"location":"Best_Practices/Managing_Scripts_For_Remote_Execution/","text":"Managing Scripts for Remote Execution \u00b6 To execute a PowerShell or Bash script or Expect script on a remote host, you must provide the script as a string to run_powershell or run_bash or run_expect . While you can keep these strings as literals in your Python code, best practice is to keep them as resource files in your source directory and access them with pkgutil . pkgutil is part of the standard Python library. The method that is applicable to resources is pkgutil.get_data . Basic Usage \u00b6 Given the following plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 plugin_runner.py \u2514\u2500\u2500 resources \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get_date.sh Assume SnapshotDefinition is: \"snapshotDefinition\": { \"type\" : \"object\", \"additionalProperties\" : false, \"properties\" : { \"name\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"} } } and src/resources/get_date.sh contains: 1 2 #!/usr/bin/env bash date If get_date.sh is needed in post_snapshot , it can be retrieved and executed: import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.linked.post_snapshot () def post_snapshot ( direct_source , repository , source_config ): # Retrieve script contents script_content = pkgutil . get_data ( 'resources' , 'get_date.sh' ) # Execute script on remote host response = libs . run_bash ( direct_source . connection , script_content ) # Fail operation if the timestamp couldn't be retrieved if response . exit_code != 0 : raise RuntimeError ( 'Failed to get date: {}' . format ( response . stdout )) return SnapshotDefinition ( name = 'Snapshot' , date = response . stdout ) Python's Working Directory This assumes that src/ is Python's current working directory. This is the behavior of the Virtualization Platform. Resources need to be in a Python module pkgutil.get_data cannot retrieve the contents of a resource that is not in a Python package. This means that a resource that is in the first level of your source directory will not be retrievable with pkgutil . Resources must be in a subdirectory of your source directory, and that subdirectory must contain an __init__.py file. Multi-level Packages \u00b6 Given the following plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 plugin_runner.py \u2514\u2500\u2500 resources \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 execute_sql.sh \u2514\u2500\u2500 platform \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get_date.sh The contents of src/resources/platform/get_date.sh can be retrieved with: script_content = pkgutil . get_data ( 'resources.platform' , 'get_date.sh' )","title":"Managing Scripts for Remote Execution"},{"location":"Best_Practices/Managing_Scripts_For_Remote_Execution/#managing-scripts-for-remote-execution","text":"To execute a PowerShell or Bash script or Expect script on a remote host, you must provide the script as a string to run_powershell or run_bash or run_expect . While you can keep these strings as literals in your Python code, best practice is to keep them as resource files in your source directory and access them with pkgutil . pkgutil is part of the standard Python library. The method that is applicable to resources is pkgutil.get_data .","title":"Managing Scripts for Remote Execution"},{"location":"Best_Practices/Managing_Scripts_For_Remote_Execution/#basic-usage","text":"Given the following plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 plugin_runner.py \u2514\u2500\u2500 resources \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get_date.sh Assume SnapshotDefinition is: \"snapshotDefinition\": { \"type\" : \"object\", \"additionalProperties\" : false, \"properties\" : { \"name\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"} } } and src/resources/get_date.sh contains: 1 2 #!/usr/bin/env bash date If get_date.sh is needed in post_snapshot , it can be retrieved and executed: import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.linked.post_snapshot () def post_snapshot ( direct_source , repository , source_config ): # Retrieve script contents script_content = pkgutil . get_data ( 'resources' , 'get_date.sh' ) # Execute script on remote host response = libs . run_bash ( direct_source . connection , script_content ) # Fail operation if the timestamp couldn't be retrieved if response . exit_code != 0 : raise RuntimeError ( 'Failed to get date: {}' . format ( response . stdout )) return SnapshotDefinition ( name = 'Snapshot' , date = response . stdout ) Python's Working Directory This assumes that src/ is Python's current working directory. This is the behavior of the Virtualization Platform. Resources need to be in a Python module pkgutil.get_data cannot retrieve the contents of a resource that is not in a Python package. This means that a resource that is in the first level of your source directory will not be retrievable with pkgutil . Resources must be in a subdirectory of your source directory, and that subdirectory must contain an __init__.py file.","title":"Basic Usage"},{"location":"Best_Practices/Managing_Scripts_For_Remote_Execution/#multi-level-packages","text":"Given the following plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 plugin_runner.py \u2514\u2500\u2500 resources \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 execute_sql.sh \u2514\u2500\u2500 platform \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get_date.sh The contents of src/resources/platform/get_date.sh can be retrieved with: script_content = pkgutil . get_data ( 'resources.platform' , 'get_date.sh' )","title":"Multi-level Packages"},{"location":"Best_Practices/Sensitive_Data/","text":"Dealing With Sensitive Data \u00b6 Often, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password. Plugins must be careful to handle sensitive data appropriately. Three tips for handling sensitive data are: Tell the Delphix Engine which parts of your data are sensitive. When passing sensitive data to remote plugin library functions (such as run_bash ), use environment variables. Avoid logging, or otherwise writing out the sensitive data. Each of these tips are explained below. Marking Your Data As Sensitive \u00b6 Because the Delphix Engine manages the storing and retrieving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its schemas , by using the special password keyword. The following example of a schema defines an object with three properties, one of which is sensitive and tagged with the password keyword: { \"type\" : \"object\" , \"properties\" : { \"db_connectionPort\" : { \"type\" : \"string\" }, \"db_username\" : { \"type\" : \"string\" }, \"db_password\" : { \"type\" : \"string\" , \"format\" : \"password\" } } } This tells the Delphix Engine to take special precautions with this password property, as follows: The Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass back to the plugin. The Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs). The Delphix Engine's UI and CLI will not display the password. Clients of the Delphix Engine's public API will not be able to access the password. Using Environment Variables For Remote Data Passing \u00b6 Sometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a staging environment , and that database command will need to use a password. Example \u00b6 Let us take a look at a very simple example where we need to shutdown a database called \"inventory\" on a target environment by using the db_cmd shutdown inventory command. This command will ask for a password on stdin , and for our example our password is \"hunter2\". If we were running this command by hand, it might look like this: $ db_cmd shutdown inventory Connecting to database instance... Please enter database password: At this point, we would type in \"hunter2\", and the command would proceed to shut down the database. Since a plugin cannot type in the password by hand, it will do something like this instead: $ echo \"hunter2\" | db_cmd shutdown inventory Don't Do This \u00b6 First, let us take a look at how not to do this! Here is a bit of plugin python code that will run the above command. from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.stop () def my_virtual_stop ( virtual_source , repository , source_config ): # THIS IS INSECURE! DO NOT DO THIS! full_command = \"echo {} | db_cmd shutdown {}\" . format ( password , db_name ) libs . run_bash ( virtual_source . connection , full_command ) This constructs a Python string containing exactly the desired command from above. However, this is not recommended. The problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Virtualization Platform. For example, suppose the Virtualization Platform cannot make a connection to the target environment. In which case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message. Using Environment Variables \u00b6 The Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let us look at a different way to run the same command as above. from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.stop () # Use environment variables to pass sensitive data to remote commands environment_vars = { \"DATABASE_PASSWORD\" : password } full_command = \"echo $DATABASE_PASSWORD | db_cmd shutdown {}\" . format ( db_name ) libs . run_bash ( virtual_source . connection , full_command , variables = environment_vars ) Note We are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Virtualization Platform to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself. Once the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected. Unlike with the command string, the Virtualization Platform does treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc. Don't Write Out Sensitive Data \u00b6 Plugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins. The Virtualization Platform provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to never log sensitive data . In addition, remember that your plugin is not treated as sensitive by the Virtualization Platform. Plugin code is distributed unencrypted, and is viewable in cleartext by Delphix Engine users. Sensitive data such as passwords should never be hard-coded in your plugin code.","title":"Dealing With Sensitive Data"},{"location":"Best_Practices/Sensitive_Data/#dealing-with-sensitive-data","text":"Often, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password. Plugins must be careful to handle sensitive data appropriately. Three tips for handling sensitive data are: Tell the Delphix Engine which parts of your data are sensitive. When passing sensitive data to remote plugin library functions (such as run_bash ), use environment variables. Avoid logging, or otherwise writing out the sensitive data. Each of these tips are explained below.","title":"Dealing With Sensitive Data"},{"location":"Best_Practices/Sensitive_Data/#marking-your-data-as-sensitive","text":"Because the Delphix Engine manages the storing and retrieving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its schemas , by using the special password keyword. The following example of a schema defines an object with three properties, one of which is sensitive and tagged with the password keyword: { \"type\" : \"object\" , \"properties\" : { \"db_connectionPort\" : { \"type\" : \"string\" }, \"db_username\" : { \"type\" : \"string\" }, \"db_password\" : { \"type\" : \"string\" , \"format\" : \"password\" } } } This tells the Delphix Engine to take special precautions with this password property, as follows: The Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass back to the plugin. The Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs). The Delphix Engine's UI and CLI will not display the password. Clients of the Delphix Engine's public API will not be able to access the password.","title":"Marking Your Data As Sensitive"},{"location":"Best_Practices/Sensitive_Data/#using-environment-variables-for-remote-data-passing","text":"Sometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a staging environment , and that database command will need to use a password.","title":"Using Environment Variables For Remote Data Passing"},{"location":"Best_Practices/Sensitive_Data/#example","text":"Let us take a look at a very simple example where we need to shutdown a database called \"inventory\" on a target environment by using the db_cmd shutdown inventory command. This command will ask for a password on stdin , and for our example our password is \"hunter2\". If we were running this command by hand, it might look like this: $ db_cmd shutdown inventory Connecting to database instance... Please enter database password: At this point, we would type in \"hunter2\", and the command would proceed to shut down the database. Since a plugin cannot type in the password by hand, it will do something like this instead: $ echo \"hunter2\" | db_cmd shutdown inventory","title":"Example"},{"location":"Best_Practices/Sensitive_Data/#dont-do-this","text":"First, let us take a look at how not to do this! Here is a bit of plugin python code that will run the above command. from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.stop () def my_virtual_stop ( virtual_source , repository , source_config ): # THIS IS INSECURE! DO NOT DO THIS! full_command = \"echo {} | db_cmd shutdown {}\" . format ( password , db_name ) libs . run_bash ( virtual_source . connection , full_command ) This constructs a Python string containing exactly the desired command from above. However, this is not recommended. The problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Virtualization Platform. For example, suppose the Virtualization Platform cannot make a connection to the target environment. In which case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message.","title":"Don't Do This"},{"location":"Best_Practices/Sensitive_Data/#using-environment-variables","text":"The Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let us look at a different way to run the same command as above. from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.stop () # Use environment variables to pass sensitive data to remote commands environment_vars = { \"DATABASE_PASSWORD\" : password } full_command = \"echo $DATABASE_PASSWORD | db_cmd shutdown {}\" . format ( db_name ) libs . run_bash ( virtual_source . connection , full_command , variables = environment_vars ) Note We are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Virtualization Platform to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself. Once the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected. Unlike with the command string, the Virtualization Platform does treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc.","title":"Using Environment Variables"},{"location":"Best_Practices/Sensitive_Data/#dont-write-out-sensitive-data","text":"Plugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins. The Virtualization Platform provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to never log sensitive data . In addition, remember that your plugin is not treated as sensitive by the Virtualization Platform. Plugin code is distributed unencrypted, and is viewable in cleartext by Delphix Engine users. Sensitive data such as passwords should never be hard-coded in your plugin code.","title":"Don't Write Out Sensitive Data"},{"location":"Best_Practices/Working_with_Powershell/","text":"Error handling in Powershell \u00b6 Info Commands run via run_powershell are executed as a script. The exit code returned by run_powershell as part of the RunPowershellResult is determined by the exit code from the script. PowerShell gives you a few ways to handle errors in your scripts: Set $ErrorActionPreference. This only applies to PowerShell Cmdlets. For scripts or other executables such as sqlcmd, PowerShell will return with exit code 0 even if there is an error, regardless of the value of $ErrorActionPrefe rence. The allowable values for $ErrorActionPreference are: Continue (default) \u2013 Continue even if there is an error. SilentlyContinue \u2013 Acts like Continue with the exception that errors are not displayed Inquire \u2013 Prompts the user in case of error Stop - Stops execution after the first error Use exception handling by using traps and try/catch blocks or if statements to detect errors and return with non-zero exit codes Use custom error handling that can be invoked after launching each command in the script to correctly detect errors. Examples \u00b6 The following example will show you how setting $ErrorActionPreference will return exit codes In the below code, ls nothing123 is expected to fail. ls nothing123 Write-Host \"Test\" Here is the output when the above commands runs on a remote host and the script will return the value of $? to be True eventhough the script failed. ```PS C:\\Users\\dtully\\test> ./test1.ps1 ls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist. At C:\\Users\\dtully\\test\\test1.ps1:1 char:1 + ls nothing123 + ~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundEx ception + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand PS C:\\Users\\dtully\\test> Write-Host $? True Now lets set $ErrorActionPreference=Stop. ```Windows $ErrorActionPreference = \"Stop\" ls nothing123 Write-Host \"Test\" Now when we run the command again we see the return value of $? to be False. PS C:\\Users\\dtully\\test> ./test1.ps1 ls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist. At C:\\Users\\dtully\\test\\test1.ps1:2 char:1 + ls nothing123 + ~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundException + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand PS C:\\Users\\dtully\\test> Write-Host $? False The following example shows how you can use the function verifySuccess to detect whether the previous command failed, and if it did print, print an error message and return with an exit code of 1. function die { Write-Error \"Error: $($args[0])\" exit 1 } function verifySuccess { if (!$?) { die \"$($args[0])\" } } Write-Output \"I'd rather be in Hawaii\" verifySuccess \"WRITE_OUTPUT_FAILED\" & C:\\Program Files\\Delphix\\scripts\\myscript.ps1 verifySuccess \"MY_SCRIPT_FAILED\"","title":"Working with Powershell"},{"location":"Best_Practices/Working_with_Powershell/#error-handling-in-powershell","text":"Info Commands run via run_powershell are executed as a script. The exit code returned by run_powershell as part of the RunPowershellResult is determined by the exit code from the script. PowerShell gives you a few ways to handle errors in your scripts: Set $ErrorActionPreference. This only applies to PowerShell Cmdlets. For scripts or other executables such as sqlcmd, PowerShell will return with exit code 0 even if there is an error, regardless of the value of $ErrorActionPrefe rence. The allowable values for $ErrorActionPreference are: Continue (default) \u2013 Continue even if there is an error. SilentlyContinue \u2013 Acts like Continue with the exception that errors are not displayed Inquire \u2013 Prompts the user in case of error Stop - Stops execution after the first error Use exception handling by using traps and try/catch blocks or if statements to detect errors and return with non-zero exit codes Use custom error handling that can be invoked after launching each command in the script to correctly detect errors.","title":"Error handling in Powershell"},{"location":"Best_Practices/Working_with_Powershell/#examples","text":"The following example will show you how setting $ErrorActionPreference will return exit codes In the below code, ls nothing123 is expected to fail. ls nothing123 Write-Host \"Test\" Here is the output when the above commands runs on a remote host and the script will return the value of $? to be True eventhough the script failed. ```PS C:\\Users\\dtully\\test> ./test1.ps1 ls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist. At C:\\Users\\dtully\\test\\test1.ps1:1 char:1 + ls nothing123 + ~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundEx ception + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand PS C:\\Users\\dtully\\test> Write-Host $? True Now lets set $ErrorActionPreference=Stop. ```Windows $ErrorActionPreference = \"Stop\" ls nothing123 Write-Host \"Test\" Now when we run the command again we see the return value of $? to be False. PS C:\\Users\\dtully\\test> ./test1.ps1 ls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist. At C:\\Users\\dtully\\test\\test1.ps1:2 char:1 + ls nothing123 + ~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundException + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand PS C:\\Users\\dtully\\test> Write-Host $? False The following example shows how you can use the function verifySuccess to detect whether the previous command failed, and if it did print, print an error message and return with an exit code of 1. function die { Write-Error \"Error: $($args[0])\" exit 1 } function verifySuccess { if (!$?) { die \"$($args[0])\" } } Write-Output \"I'd rather be in Hawaii\" verifySuccess \"WRITE_OUTPUT_FAILED\" & C:\\Program Files\\Delphix\\scripts\\myscript.ps1 verifySuccess \"MY_SCRIPT_FAILED\"","title":"Examples"},{"location":"Building_Your_First_Plugin/Data_Ingestion/","text":"Data Ingestion \u00b6 How Does Delphix Ingest Data? \u00b6 As previously discussed, the Delphix Engine uses the discovery process to learn about datasets that live on a source environment . In this section we will learn how the Delphix Engine uses a two-step process to ingest a dataset. Linking \u00b6 The first step is called linking . This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a dSource . Syncing \u00b6 Immediately after linking, the new dSource is synced for the first time. Syncing is a process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date. The details of how this is done varies significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from: direct and staging. With the direct strategy, the plugin is not in charge of the data copying. Instead the Delphix Engine directly pulls raw data from the source environment. The plugin merely provides the location of the data. This is a very simple strategy, and is also quite limiting. For our first plugin, we will be using the more flexible staging strategy. With this strategy, the Delphix Engine uses NFS for Unix environments (or iSCSI on Windows environments) to mount storage onto a staging environment . Our plugin will then be in full control of how to get data from the source environment onto this storage mount. With the staging strategy, there are two types of syncs: sync and resync. A sync is used to ingestion incremental changes while a resync is used to re-ingest all the data for the dSource. For databases, this could mean re-ingesting from a full database backup to reset the dSource. A sync and a resync execute the same plugin operations and are differentiated by a boolean flag in the snapshot_parameters argument passed into linked.pre_snapshot and linked.post_snapshot . A regular sync is the default and is executed as part of policy driven syncs. A resync is only executed during initial ingestion or if the Delphix user manually starts one. The customer can manually trigger a resync via the UI by selecting the dSource, going to more options and selecting Resynchronize dSource . Gotcha Although it is not common, it is entirely possible that the staging environment is the same as the source environment. Be careful not to assume otherwise in your plugins. Our Syncing Strategy \u00b6 For our purposes here in this intro plugin, we will use a simple strategy. We won't do anything with the resync snapshot parameter and simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We will do this by running the Unix tool rsync from our staging environment, and rely on passwordless SSH to connect to the source environment. Info This plugin is assuming that rsync is installed on the staging host, and that the staging host user is able to SSH into the source host without having to type in a password. A more full-featured plugin would test these assumptions, usually as part of discovery. In the special case mentioned above, where the staging environment is the same as the source environment, we could likely do something more efficient. However, for simplicity's sake, we won't do that here. Defining Your Linked Source Data Format \u00b6 In order to be able to successfully do the copying required, plugins might need to get some information from the end-user of your plugin. In our case, we need to tell rsync how to access the files. This means we need to know the source environment's IP address (or domain name), the username we need to connect as, and finally the location where the files live. Again, we will be using a JSON schema to define the data format. The user will be presented with a UI that lets them provide all the information our schema specifies. Open up schema.json in your editor/IDE. Locate the LinkedSourceDefinition and replace it with the following schema: \"linkedSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"sourceAddress\" , \"username\" , \"mountLocation\" ], \"properties\" : { \"sourceAddress\" : { \"type\" : \"string\" , \"prettyName\" : \"Host from which to copy\" , \"description\" : \"IP or FQDN of host from which to copy\" }, \"username\" : { \"type\" : \"string\" , \"prettyName\" : \"Username on Source Host\" , \"description\" : \"Username for making SSH connection to source host\" }, \"mountLocation\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Mount Location on Staging Host\" , \"description\" : \"Where to mount storage onto the staging host while syncing\" } } } , Info As will be explained later, this schema will be used to generate Python code. All names in the autogenerated Python code will use lower_case_with_underscores as attribute names as per Python variable naming conventions. That is, if we were to use mountLocation as the schema property name, it would be called mount_location in the generated Python code. With this schema, the user will be required to provide the source username, the source's IP address, and the staging mount location as part of the linking process. Implementing Syncing in Your Plugin \u00b6 There are three things we must do to implement syncing. First, we need to tell the Delphix Engine where to mount storage onto the staging environment. Next we need to actually do the work of copying data onto that mounted storage. Finally, we need to generate any snapshot-related data. Mount Specification \u00b6 Before syncing can begin, the Delphix Engine needs to mount some storage onto the staging host. Since different plugins can have different requirements about where exactly this mount lives, it is up to the plugin to specify this location. As mentioned above, our simple plugin will get this location from the user. Open up the plugin_runner.py file and find the linked_mount_specification function (which was generated by dvp init ). Replace it with the following code: @plugin.linked.mount_specification() def linked_mount_specification(staged_source, repository): mount_location = staged_source.parameters.mount_location mount = Mount(staged_source.staged_connection.environment, mount_location) return MountSpecification([mount]) Let's take this line-by-line to see what's going on here. @plugin.linked.mount_specification() This decorator announces that the following function is the code that handles the mount_specification operation. This is what allows the Delphix Engine to know which function to call when it's time to learn where to mount. Every operation definition will begin with a similar decorator. def linked_mount_specification(staged_source, repository): This begins a Python function definition. We chose to call it linked_mount_specification , but we could have chosen any name at all. This function accepts two arguments, one giving information about the linked source, and one giving information about the associated repository. mount_location = staged_source.parameters.mount_location The staged_source input argument contains an attribute called parameters . This in turn contains all of the properties defined by the linkedSourceDefinition schema. So, in our case, that means it will contain attributes called source_address , username , and mount_location . Note how any attribute defined in camelCase in the schema is converted to variable_with_underscores . This line simply retrieves the user-provided mount location and saves it in a local variable. mount = Mount(staged_source.staged_connection.environment, mount_location) This line constructs a new object from the Mount class . This class holds details about how Delphix Engine storage is mounted onto remote environments. Here, we create a mount object that says to mount onto the staging environment, at the location specified by the user. return MountSpecification([mount]) On the line just before this one, we created an object that describes a single mount. Now, we must return a full mount specification . In general, a mount specification is a collection of mounts. But, in our case, we just have one single mount. Therefore, we use an array with only one item it in -- namely, the one single mount object we created just above. Data Copying \u00b6 As explained here , the Delphix Engine will always run the plugin's preSnapshot operation just before taking a snapshot of the dsource. That means our preSnapshot operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy. Unlike the previous operations we've seen so far, the pre-snapshot operation will not be autogenerated by dvp init . So, we will need to add one ourselves. Open up the plugin_runner.py file. First, we'll add a new import line near the top of the file, so that we can use Delphix's platform libraries (explained below). from dlpx.virtualization import libs Next, we'll add a new function: @plugin.linked.pre_snapshot () def copy_data_from_source ( staged_source , repository , source_config , snapshot_parameters ): stage_mount_path = staged_source . mount . mount_path data_location = \"{}@{}:{}\" . format ( staged_source . parameters . username , staged_source . parameters . source_address , source_config . path ) rsync_command = \"rsync -r {} {}\" . format ( data_location , stage_mount_path ) result = libs . run_bash ( staged_source . staged_connection , rsync_command ) if result . exit_code != 0 : raise RuntimeError ( \"Could not copy files. Please ensure that passwordless SSH works for {}. \\n {}\" . format ( staged_source . parameters . source_address , result . stderr )) Let's walk through this function and see what's going on stage_mount_path = staged_source . mount . mount_path The staged_source argument contains information about the current mount location. Here we save that to a local variable for convenience. data_location = \"{}@{}:{}\" . format ( staged_source . parameters . username , staged_source . parameters . source_address , source_config . path ) This code creates a Python string that represents the location of the data that we want to ingest. This is in the form <user>@<host>:<path> . For example jdoe@sourcehost.mycompany.com:/bin . As before with mountLocation , we have defined our schemas such that these three pieces of information were provided by the user. Here we're just putting them into a format that rsync will understand. rsync_command = \"rsync -r {} {}\" . format ( data_location , stage_mount_path ) This line is the actual Bash command that we'll be running on the staging host. This will look something like rsync -r user@host:/source/path /staging/mount/path . result = libs . run_bash ( staged_source . staged_connection , rsync_command ) This is an example of a platform library function, where we ask the Virtualization Platform to do some work on our behalf. In this case, we're asking the platform to run our Bash command on the staging environment. For full details on the run_bash platform library function and others, see this reference . if result . exit_code != 0 : raise RuntimeError ( \"Could not copy files. Please ensure that passwordless SSH works for {}. \\n {}\" . format ( staged_source . parameters . source_address , result . stderr )) Finally, we check to see if our Bash command actually worked okay. If not, we raise an error message, and describe one possible problem for the user to investigate. Saving Snapshot Data \u00b6 Whenever the Delphix Engine takes a snapshot of a dSource or VDB, the plugin has the chance to save any information it likes alongside that snapshot. Later, if the snapshot is ever used to provision a new VDB, the plugin can use the previously-saved information to help get the new VDB ready for use. The format of this data is controlled by the plugin's snapshotDefinition schema. In our case, we don't have any data we need to save. So, there's not much to do here. We will not modify the blank schema that was created by dvp init . We do still need to provide python function for the engine to call, but we don't have to do much. In fact, the default implementation that was generated by dvp init will work just fine for our purposes: @plugin.linked.post_snapshot () def linked_post_snapshot ( staged_source , repository , source_config , snapshot_parameters ): return SnapshotDefinition () The only thing this code is doing is creating a new object using our (empty) snapshot definition, and returning that new empty object. How to Link and Sync in the Delphix Engine \u00b6 Let's try it out and make sure this works! Prerequisites You should already have a repository and source config set up from the previous page. You can optionally set up a new staging environment. Or, you can simply re-use your source environment for staging. Procedure Note Recall that, for simplicity's sake, this plugin requires that passwordless SSH is set up between your staging and source environments. You may want to verify this before continuing. As before, use dvp build and dvp upload to get your latest plugin changes installed onto the Delphix Engine. Go to Manage > Environments , select your source environment, and then go to the Databases tab. Find Repository for our First Plugin , and your source config underneath it. From your source config click Add dSource . This will begin the linking process. The first screen you see should ask for the properties that you recently added to your linkedSourceDefinition . Walk through the remainder of the screens and hit Submit . This will kick off the initial link and first sync. You can confirm that your new dSource was added successfully by going to Manage > Datasets . After you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data. Gotcha Manually creating a dSource sets your plugin\u2019s linked source schema in stone, and you will have to recreate the dSource in order to modify your schema. We will cover how to deal with this correctly later, in the upgrade section. For now, if you need to change your plugin's linked source schema, you will have to first delete any dSources you have manually added. Survey Please fill out this survey to give us feedback about this section.","title":"Virtualization SDK"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#data-ingestion","text":"","title":"Data Ingestion"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#how-does-delphix-ingest-data","text":"As previously discussed, the Delphix Engine uses the discovery process to learn about datasets that live on a source environment . In this section we will learn how the Delphix Engine uses a two-step process to ingest a dataset.","title":"How Does Delphix Ingest Data?"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#linking","text":"The first step is called linking . This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a dSource .","title":"Linking"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#syncing","text":"Immediately after linking, the new dSource is synced for the first time. Syncing is a process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date. The details of how this is done varies significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from: direct and staging. With the direct strategy, the plugin is not in charge of the data copying. Instead the Delphix Engine directly pulls raw data from the source environment. The plugin merely provides the location of the data. This is a very simple strategy, and is also quite limiting. For our first plugin, we will be using the more flexible staging strategy. With this strategy, the Delphix Engine uses NFS for Unix environments (or iSCSI on Windows environments) to mount storage onto a staging environment . Our plugin will then be in full control of how to get data from the source environment onto this storage mount. With the staging strategy, there are two types of syncs: sync and resync. A sync is used to ingestion incremental changes while a resync is used to re-ingest all the data for the dSource. For databases, this could mean re-ingesting from a full database backup to reset the dSource. A sync and a resync execute the same plugin operations and are differentiated by a boolean flag in the snapshot_parameters argument passed into linked.pre_snapshot and linked.post_snapshot . A regular sync is the default and is executed as part of policy driven syncs. A resync is only executed during initial ingestion or if the Delphix user manually starts one. The customer can manually trigger a resync via the UI by selecting the dSource, going to more options and selecting Resynchronize dSource . Gotcha Although it is not common, it is entirely possible that the staging environment is the same as the source environment. Be careful not to assume otherwise in your plugins.","title":"Syncing"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#our-syncing-strategy","text":"For our purposes here in this intro plugin, we will use a simple strategy. We won't do anything with the resync snapshot parameter and simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We will do this by running the Unix tool rsync from our staging environment, and rely on passwordless SSH to connect to the source environment. Info This plugin is assuming that rsync is installed on the staging host, and that the staging host user is able to SSH into the source host without having to type in a password. A more full-featured plugin would test these assumptions, usually as part of discovery. In the special case mentioned above, where the staging environment is the same as the source environment, we could likely do something more efficient. However, for simplicity's sake, we won't do that here.","title":"Our Syncing Strategy"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#defining-your-linked-source-data-format","text":"In order to be able to successfully do the copying required, plugins might need to get some information from the end-user of your plugin. In our case, we need to tell rsync how to access the files. This means we need to know the source environment's IP address (or domain name), the username we need to connect as, and finally the location where the files live. Again, we will be using a JSON schema to define the data format. The user will be presented with a UI that lets them provide all the information our schema specifies. Open up schema.json in your editor/IDE. Locate the LinkedSourceDefinition and replace it with the following schema: \"linkedSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"sourceAddress\" , \"username\" , \"mountLocation\" ], \"properties\" : { \"sourceAddress\" : { \"type\" : \"string\" , \"prettyName\" : \"Host from which to copy\" , \"description\" : \"IP or FQDN of host from which to copy\" }, \"username\" : { \"type\" : \"string\" , \"prettyName\" : \"Username on Source Host\" , \"description\" : \"Username for making SSH connection to source host\" }, \"mountLocation\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Mount Location on Staging Host\" , \"description\" : \"Where to mount storage onto the staging host while syncing\" } } } , Info As will be explained later, this schema will be used to generate Python code. All names in the autogenerated Python code will use lower_case_with_underscores as attribute names as per Python variable naming conventions. That is, if we were to use mountLocation as the schema property name, it would be called mount_location in the generated Python code. With this schema, the user will be required to provide the source username, the source's IP address, and the staging mount location as part of the linking process.","title":"Defining Your Linked Source Data Format"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#implementing-syncing-in-your-plugin","text":"There are three things we must do to implement syncing. First, we need to tell the Delphix Engine where to mount storage onto the staging environment. Next we need to actually do the work of copying data onto that mounted storage. Finally, we need to generate any snapshot-related data.","title":"Implementing Syncing in Your Plugin"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#mount-specification","text":"Before syncing can begin, the Delphix Engine needs to mount some storage onto the staging host. Since different plugins can have different requirements about where exactly this mount lives, it is up to the plugin to specify this location. As mentioned above, our simple plugin will get this location from the user. Open up the plugin_runner.py file and find the linked_mount_specification function (which was generated by dvp init ). Replace it with the following code: @plugin.linked.mount_specification() def linked_mount_specification(staged_source, repository): mount_location = staged_source.parameters.mount_location mount = Mount(staged_source.staged_connection.environment, mount_location) return MountSpecification([mount]) Let's take this line-by-line to see what's going on here. @plugin.linked.mount_specification() This decorator announces that the following function is the code that handles the mount_specification operation. This is what allows the Delphix Engine to know which function to call when it's time to learn where to mount. Every operation definition will begin with a similar decorator. def linked_mount_specification(staged_source, repository): This begins a Python function definition. We chose to call it linked_mount_specification , but we could have chosen any name at all. This function accepts two arguments, one giving information about the linked source, and one giving information about the associated repository. mount_location = staged_source.parameters.mount_location The staged_source input argument contains an attribute called parameters . This in turn contains all of the properties defined by the linkedSourceDefinition schema. So, in our case, that means it will contain attributes called source_address , username , and mount_location . Note how any attribute defined in camelCase in the schema is converted to variable_with_underscores . This line simply retrieves the user-provided mount location and saves it in a local variable. mount = Mount(staged_source.staged_connection.environment, mount_location) This line constructs a new object from the Mount class . This class holds details about how Delphix Engine storage is mounted onto remote environments. Here, we create a mount object that says to mount onto the staging environment, at the location specified by the user. return MountSpecification([mount]) On the line just before this one, we created an object that describes a single mount. Now, we must return a full mount specification . In general, a mount specification is a collection of mounts. But, in our case, we just have one single mount. Therefore, we use an array with only one item it in -- namely, the one single mount object we created just above.","title":"Mount Specification"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#data-copying","text":"As explained here , the Delphix Engine will always run the plugin's preSnapshot operation just before taking a snapshot of the dsource. That means our preSnapshot operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy. Unlike the previous operations we've seen so far, the pre-snapshot operation will not be autogenerated by dvp init . So, we will need to add one ourselves. Open up the plugin_runner.py file. First, we'll add a new import line near the top of the file, so that we can use Delphix's platform libraries (explained below). from dlpx.virtualization import libs Next, we'll add a new function: @plugin.linked.pre_snapshot () def copy_data_from_source ( staged_source , repository , source_config , snapshot_parameters ): stage_mount_path = staged_source . mount . mount_path data_location = \"{}@{}:{}\" . format ( staged_source . parameters . username , staged_source . parameters . source_address , source_config . path ) rsync_command = \"rsync -r {} {}\" . format ( data_location , stage_mount_path ) result = libs . run_bash ( staged_source . staged_connection , rsync_command ) if result . exit_code != 0 : raise RuntimeError ( \"Could not copy files. Please ensure that passwordless SSH works for {}. \\n {}\" . format ( staged_source . parameters . source_address , result . stderr )) Let's walk through this function and see what's going on stage_mount_path = staged_source . mount . mount_path The staged_source argument contains information about the current mount location. Here we save that to a local variable for convenience. data_location = \"{}@{}:{}\" . format ( staged_source . parameters . username , staged_source . parameters . source_address , source_config . path ) This code creates a Python string that represents the location of the data that we want to ingest. This is in the form <user>@<host>:<path> . For example jdoe@sourcehost.mycompany.com:/bin . As before with mountLocation , we have defined our schemas such that these three pieces of information were provided by the user. Here we're just putting them into a format that rsync will understand. rsync_command = \"rsync -r {} {}\" . format ( data_location , stage_mount_path ) This line is the actual Bash command that we'll be running on the staging host. This will look something like rsync -r user@host:/source/path /staging/mount/path . result = libs . run_bash ( staged_source . staged_connection , rsync_command ) This is an example of a platform library function, where we ask the Virtualization Platform to do some work on our behalf. In this case, we're asking the platform to run our Bash command on the staging environment. For full details on the run_bash platform library function and others, see this reference . if result . exit_code != 0 : raise RuntimeError ( \"Could not copy files. Please ensure that passwordless SSH works for {}. \\n {}\" . format ( staged_source . parameters . source_address , result . stderr )) Finally, we check to see if our Bash command actually worked okay. If not, we raise an error message, and describe one possible problem for the user to investigate.","title":"Data Copying"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#saving-snapshot-data","text":"Whenever the Delphix Engine takes a snapshot of a dSource or VDB, the plugin has the chance to save any information it likes alongside that snapshot. Later, if the snapshot is ever used to provision a new VDB, the plugin can use the previously-saved information to help get the new VDB ready for use. The format of this data is controlled by the plugin's snapshotDefinition schema. In our case, we don't have any data we need to save. So, there's not much to do here. We will not modify the blank schema that was created by dvp init . We do still need to provide python function for the engine to call, but we don't have to do much. In fact, the default implementation that was generated by dvp init will work just fine for our purposes: @plugin.linked.post_snapshot () def linked_post_snapshot ( staged_source , repository , source_config , snapshot_parameters ): return SnapshotDefinition () The only thing this code is doing is creating a new object using our (empty) snapshot definition, and returning that new empty object.","title":"Saving Snapshot Data"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#how-to-link-and-sync-in-the-delphix-engine","text":"Let's try it out and make sure this works! Prerequisites You should already have a repository and source config set up from the previous page. You can optionally set up a new staging environment. Or, you can simply re-use your source environment for staging. Procedure Note Recall that, for simplicity's sake, this plugin requires that passwordless SSH is set up between your staging and source environments. You may want to verify this before continuing. As before, use dvp build and dvp upload to get your latest plugin changes installed onto the Delphix Engine. Go to Manage > Environments , select your source environment, and then go to the Databases tab. Find Repository for our First Plugin , and your source config underneath it. From your source config click Add dSource . This will begin the linking process. The first screen you see should ask for the properties that you recently added to your linkedSourceDefinition . Walk through the remainder of the screens and hit Submit . This will kick off the initial link and first sync. You can confirm that your new dSource was added successfully by going to Manage > Datasets . After you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data. Gotcha Manually creating a dSource sets your plugin\u2019s linked source schema in stone, and you will have to recreate the dSource in order to modify your schema. We will cover how to deal with this correctly later, in the upgrade section. For now, if you need to change your plugin's linked source schema, you will have to first delete any dSources you have manually added. Survey Please fill out this survey to give us feedback about this section.","title":"How to Link and Sync in the Delphix Engine"},{"location":"Building_Your_First_Plugin/Discovery/","text":"Discovery \u00b6 What is Discovery? \u00b6 In order to ingest data from a source environment, the Delphix Engine first needs to learn information about the data: Where does it live? How can it be accessed? What is it called? Discovery is the process by which the Delphix Engine learns about remote data. Discovery can be either: automatic \u2014 where the plugin finds the remote data on its own manual \u2014 where the user tells us about the remote data For our first plugin, we will be using a mix of these two techniques. Source Configs and Repositories \u00b6 What are Source Configs and Repositories? \u00b6 A source config is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?). For our first plugin, it is simply a directory tree on the filesystem of the remote environment. A repository represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you are working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we do not have any special dependencies, except for the simple existence of the unix system on which the directory lives. We will be using automatic discovery for our repositories, and manual discovery for our source configs. This is the default configuration that is created by dvp init , so there is nothing further we need to do here. Defining Your Data Formats \u00b6 Because each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they need to collect and store. Delphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers? For our first plugin, we do not need a lot of information. We use no special information about our repositories (except some way for the user to identify them). For source configs, all we need to know is the path to the directory from which we will be ingesting data. The plugin needs to describe all of this to the Delphix Engine, and it does so using schemas . Recall that when we ran dvp init , a file full of bare-bones schemas was created. As we build up our first toolkit, we will be augmenting these schemas to serve our needs. Repository Schema \u00b6 Open up the schema.json file in your editor/IDE and locate repositoryDefinition , it should look like this: { \"repositoryDefinition\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"name\" ] } } Since we do not have any special dependencies, we can just leave it as-is. For detailed information about exactly how repository schemas work, see the reference page . In brief, what we are doing here is saying that each of our repositories will have a single property called name , which will be used both as a unique identifier and as the user-visible name of the repository. Source Config Schema \u00b6 For source configs, the bare-bones schema is not going to be good enough. Recall that for us, a source config represents a directory tree on a remote environment. Locate the sourceConfigDefinition inside the schema.json file and modify the definition so it looks like this: \"sourceConfigDefinition\" : { \"type\" : \"object\" , \"required\" : [ \"name\" , \"path\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Dataset Name\" , \"description\" : \"User-visible name for this dataset\" }, \"path\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Path\" , \"description\" : \"Full path to data location on the remote environment\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"path\" ] } , Now, we have two properties, a property name serving as the user-visible name of the source config and path which tells us where the data lives on the remote host. Note we are using path as the unique identifier. Because we are using manual discovery, the end user is going to be responsible for filling in values for name and path . So, we have added some things to our schema that we did not need for repositories. The prettyName and description entries will be used by the UI to tell the user what these fields mean. Because we set additionalProperties to false , this will prevent users from supplying properties other than name and path . Finally, we have specified that the path property must be a well-formatted Unix path. This allows the UI to enforce that the format is correct before the user is allowed to proceed. (Note this only enforces the format, and does not actually check to see if the path really exists on some remote environment!) Refer to the reference page for Schemas for more details about these entries, and for other things that you can do in these schemas. Implementing Discovery in Your Plugin \u00b6 About Python Code \u00b6 As described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize. Right now, we are concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered. A Look at the Generated Code \u00b6 Recall that the dvp init command we ran created a file called src/plugin_runner.py . Open this file in your editor/IDE. You will see that this file already contains a bunch of Python code. Let's take a look at the first three blocks of code in this file. from dlpx.virtualization.platform import Mount , MountSpecification , Plugin from generated.definitions import ( RepositoryDefinition , SourceConfigDefinition , SnapshotDefinition , ) These import lines make certain functionality available to our Python code. Some of this functionality will be used just below, as we implement discovery. Others will be used later on, as we implement ingestion and provisioning. Later, you'll add more import s to unlock more functionality. plugin = Plugin () This line creates a Python object which allows us to define our plugin types. We have the ability to do this because of the import Plugin statement above. This object is stored in a variable we have elected to call plugin . We are free to call this variable anything we want, so long as we also change the entryPoint line in the plugin_config.yml file. For this example, we will just leave it as plugin . # # Below is an example of the repository discovery operation. # # NOTE: The decorators are defined on the 'plugin' object created above. # # Mark the function below as the operation that does repository discovery. @plugin.discovery.repository () def repository_discovery ( source_connection ): # # This is an object generated from the repositoryDefinition schema. # In order to use it locally you must run the 'build -g' command provided # by the SDK tools from the plugin's root directory. # return [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )] This is our first plugin operation . In this case, it's defining what will happen when the Delphix Engine wants to discover repositories on an environment. Let's take a look at this code line-by-line @plugin.discovery.repository () def repository_discovery ( source_connection ): This begins the definition of a function called repository_discovery . We are using a Python decorator which signals to the Delphix Engine that this is the function which should be called when it is time to do repository discovery. The actual name of the function doesn't matter here. Note that we are using our plugin variable here as part of the decorator. The Delphix Engine will pass us information about the source environment in an argument called source_connection . Warning The name of this input argument matters. That is, you'll always need to have an argument called source_connection here. Each plugin operation has its own set of required argument names. For details on which arguments apply to which operations, see the reference section . return [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )] This creates and returns a Python object that corresponds to the format defined by our repository schema. Because out repository has exactly one string property called name , therefore this Python object has one property called name . Notice that the code generator has filled in the value of name with a random string. This results in a plugin operation that works, but which will not be very helpful for the user. We'll change this later. The rest of the file contains more plugin operations, and we'll be modifying them later. Repository Discovery \u00b6 Now, we need to modify the provided repository discovery operation. This operation will examine a remote environment, find any repositories, and return information about them to the Delphix Engine. As a reminder, our only external dependency on the remote environment is simply the existence of a filesystem. Since every Unix host has a filesystem, that means we will have exactly one repository per remote environment. Therefore, our repository discovery operation can be very simple. In fact, as we saw above, the default-generated repository_discovery function does almost exactly what we want -- it returns one single repository for any Unix host that it is asked to work with. The only problem with it is that it uses unhelpful name. That's really easy to change! Replace or modify repository_discovery so it looks like this: @plugin.discovery.repository () def repository_discovery ( source_connection ): repository = RepositoryDefinition ( 'Repository for our First Plugin' ) return [ repository ] Tip Be careful to always use consistent indentation in Python code! Source Config Discovery \u00b6 For source configs, we will rely solely on manual discovery. Therefore, the user will tell us which directories they want to ingest from. We still have to define a source config discovery operation -- it just won't need to do much. The job of this operation is to return only source configs associated with the given repository . This function will be called once per repository. In our case, that means it will only be called once. Because we want to supply no automatically-discovered source configs, this function should simply returns an empty list. In fact, dvp init has already generated a function for us that does exactly this. @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): return [] If we wanted to do automatic discovery of source configs, we'd modify this function. But, for our purposes now, the existing code is fine and we don't need to change anything. How to Run Discovery in the Delphix Engine \u00b6 Let us make sure discovery works! Run the dvp build commands, as before. This will build the plugin, with all of the new changes, and create an artifact. Run dvp upload -e <engine> -u <user> , as before. This will get all the new changes onto the Delphix Engine. Once the new plugin is uploaded, add a remote unix environment to your engine. To do this, go to Manage > Environments , chose Add Environment from the menu, answer the questions, and Submit . (If you already have an environment set up, you can just refresh it instead). To keep an eye on this discovery process, you may need to open the Actions tab on the UI. If any errors happen, they will be reported here. After the automatic discovery process completes, go to the Databases tab. You will see an entry for Repository For Our First Plugin . This is the repository you created in your Python code. Notice that it says No databases found on installation . This is because we chose not to do automatic source config discovery. However, because we have allowed manual source config discovery, you can add your own entries by clicking the plus sign ( Add Database ). Complete the information in the Add Database dialog and click Add. This should all look familiar. It is precisely what we defined in our source config schema. As expected, there are two entries, one for our name property, and one for path . For example, in the above screenshot, we are specifying that we want to sync the /bin directory from the remote host, and we want to call it Binaries . You can pick any directory and name that you want. Once you have added one or more source configs, you will be able to sync. This is covered on the next page. Warning Once you have automatically or manually created source configs, you will not be allowed to modify your plugin's source config schema. We will cover how to deal with this later in the upgrade section. For now, if you need to change your plugin's source config schema: You will have to delete any source configs you have manually added. Delete the plugin and its corresponding objects (dSources, Virtual Sources, etc) if the source configs were manually discovered. Survey Please fill out this survey to give us feedback about this section.","title":"Virtualization SDK"},{"location":"Building_Your_First_Plugin/Discovery/#discovery","text":"","title":"Discovery"},{"location":"Building_Your_First_Plugin/Discovery/#what-is-discovery","text":"In order to ingest data from a source environment, the Delphix Engine first needs to learn information about the data: Where does it live? How can it be accessed? What is it called? Discovery is the process by which the Delphix Engine learns about remote data. Discovery can be either: automatic \u2014 where the plugin finds the remote data on its own manual \u2014 where the user tells us about the remote data For our first plugin, we will be using a mix of these two techniques.","title":"What is Discovery?"},{"location":"Building_Your_First_Plugin/Discovery/#source-configs-and-repositories","text":"","title":"Source Configs and Repositories"},{"location":"Building_Your_First_Plugin/Discovery/#what-are-source-configs-and-repositories","text":"A source config is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?). For our first plugin, it is simply a directory tree on the filesystem of the remote environment. A repository represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you are working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we do not have any special dependencies, except for the simple existence of the unix system on which the directory lives. We will be using automatic discovery for our repositories, and manual discovery for our source configs. This is the default configuration that is created by dvp init , so there is nothing further we need to do here.","title":"What are Source Configs and Repositories?"},{"location":"Building_Your_First_Plugin/Discovery/#defining-your-data-formats","text":"Because each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they need to collect and store. Delphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers? For our first plugin, we do not need a lot of information. We use no special information about our repositories (except some way for the user to identify them). For source configs, all we need to know is the path to the directory from which we will be ingesting data. The plugin needs to describe all of this to the Delphix Engine, and it does so using schemas . Recall that when we ran dvp init , a file full of bare-bones schemas was created. As we build up our first toolkit, we will be augmenting these schemas to serve our needs.","title":"Defining Your Data Formats"},{"location":"Building_Your_First_Plugin/Discovery/#repository-schema","text":"Open up the schema.json file in your editor/IDE and locate repositoryDefinition , it should look like this: { \"repositoryDefinition\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"name\" ] } } Since we do not have any special dependencies, we can just leave it as-is. For detailed information about exactly how repository schemas work, see the reference page . In brief, what we are doing here is saying that each of our repositories will have a single property called name , which will be used both as a unique identifier and as the user-visible name of the repository.","title":"Repository Schema"},{"location":"Building_Your_First_Plugin/Discovery/#source-config-schema","text":"For source configs, the bare-bones schema is not going to be good enough. Recall that for us, a source config represents a directory tree on a remote environment. Locate the sourceConfigDefinition inside the schema.json file and modify the definition so it looks like this: \"sourceConfigDefinition\" : { \"type\" : \"object\" , \"required\" : [ \"name\" , \"path\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Dataset Name\" , \"description\" : \"User-visible name for this dataset\" }, \"path\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Path\" , \"description\" : \"Full path to data location on the remote environment\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"path\" ] } , Now, we have two properties, a property name serving as the user-visible name of the source config and path which tells us where the data lives on the remote host. Note we are using path as the unique identifier. Because we are using manual discovery, the end user is going to be responsible for filling in values for name and path . So, we have added some things to our schema that we did not need for repositories. The prettyName and description entries will be used by the UI to tell the user what these fields mean. Because we set additionalProperties to false , this will prevent users from supplying properties other than name and path . Finally, we have specified that the path property must be a well-formatted Unix path. This allows the UI to enforce that the format is correct before the user is allowed to proceed. (Note this only enforces the format, and does not actually check to see if the path really exists on some remote environment!) Refer to the reference page for Schemas for more details about these entries, and for other things that you can do in these schemas.","title":"Source Config Schema"},{"location":"Building_Your_First_Plugin/Discovery/#implementing-discovery-in-your-plugin","text":"","title":"Implementing Discovery in Your Plugin"},{"location":"Building_Your_First_Plugin/Discovery/#about-python-code","text":"As described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize. Right now, we are concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered.","title":"About Python Code"},{"location":"Building_Your_First_Plugin/Discovery/#a-look-at-the-generated-code","text":"Recall that the dvp init command we ran created a file called src/plugin_runner.py . Open this file in your editor/IDE. You will see that this file already contains a bunch of Python code. Let's take a look at the first three blocks of code in this file. from dlpx.virtualization.platform import Mount , MountSpecification , Plugin from generated.definitions import ( RepositoryDefinition , SourceConfigDefinition , SnapshotDefinition , ) These import lines make certain functionality available to our Python code. Some of this functionality will be used just below, as we implement discovery. Others will be used later on, as we implement ingestion and provisioning. Later, you'll add more import s to unlock more functionality. plugin = Plugin () This line creates a Python object which allows us to define our plugin types. We have the ability to do this because of the import Plugin statement above. This object is stored in a variable we have elected to call plugin . We are free to call this variable anything we want, so long as we also change the entryPoint line in the plugin_config.yml file. For this example, we will just leave it as plugin . # # Below is an example of the repository discovery operation. # # NOTE: The decorators are defined on the 'plugin' object created above. # # Mark the function below as the operation that does repository discovery. @plugin.discovery.repository () def repository_discovery ( source_connection ): # # This is an object generated from the repositoryDefinition schema. # In order to use it locally you must run the 'build -g' command provided # by the SDK tools from the plugin's root directory. # return [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )] This is our first plugin operation . In this case, it's defining what will happen when the Delphix Engine wants to discover repositories on an environment. Let's take a look at this code line-by-line @plugin.discovery.repository () def repository_discovery ( source_connection ): This begins the definition of a function called repository_discovery . We are using a Python decorator which signals to the Delphix Engine that this is the function which should be called when it is time to do repository discovery. The actual name of the function doesn't matter here. Note that we are using our plugin variable here as part of the decorator. The Delphix Engine will pass us information about the source environment in an argument called source_connection . Warning The name of this input argument matters. That is, you'll always need to have an argument called source_connection here. Each plugin operation has its own set of required argument names. For details on which arguments apply to which operations, see the reference section . return [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )] This creates and returns a Python object that corresponds to the format defined by our repository schema. Because out repository has exactly one string property called name , therefore this Python object has one property called name . Notice that the code generator has filled in the value of name with a random string. This results in a plugin operation that works, but which will not be very helpful for the user. We'll change this later. The rest of the file contains more plugin operations, and we'll be modifying them later.","title":"A Look at the Generated Code"},{"location":"Building_Your_First_Plugin/Discovery/#repository-discovery","text":"Now, we need to modify the provided repository discovery operation. This operation will examine a remote environment, find any repositories, and return information about them to the Delphix Engine. As a reminder, our only external dependency on the remote environment is simply the existence of a filesystem. Since every Unix host has a filesystem, that means we will have exactly one repository per remote environment. Therefore, our repository discovery operation can be very simple. In fact, as we saw above, the default-generated repository_discovery function does almost exactly what we want -- it returns one single repository for any Unix host that it is asked to work with. The only problem with it is that it uses unhelpful name. That's really easy to change! Replace or modify repository_discovery so it looks like this: @plugin.discovery.repository () def repository_discovery ( source_connection ): repository = RepositoryDefinition ( 'Repository for our First Plugin' ) return [ repository ] Tip Be careful to always use consistent indentation in Python code!","title":"Repository Discovery"},{"location":"Building_Your_First_Plugin/Discovery/#source-config-discovery","text":"For source configs, we will rely solely on manual discovery. Therefore, the user will tell us which directories they want to ingest from. We still have to define a source config discovery operation -- it just won't need to do much. The job of this operation is to return only source configs associated with the given repository . This function will be called once per repository. In our case, that means it will only be called once. Because we want to supply no automatically-discovered source configs, this function should simply returns an empty list. In fact, dvp init has already generated a function for us that does exactly this. @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): return [] If we wanted to do automatic discovery of source configs, we'd modify this function. But, for our purposes now, the existing code is fine and we don't need to change anything.","title":"Source Config Discovery"},{"location":"Building_Your_First_Plugin/Discovery/#how-to-run-discovery-in-the-delphix-engine","text":"Let us make sure discovery works! Run the dvp build commands, as before. This will build the plugin, with all of the new changes, and create an artifact. Run dvp upload -e <engine> -u <user> , as before. This will get all the new changes onto the Delphix Engine. Once the new plugin is uploaded, add a remote unix environment to your engine. To do this, go to Manage > Environments , chose Add Environment from the menu, answer the questions, and Submit . (If you already have an environment set up, you can just refresh it instead). To keep an eye on this discovery process, you may need to open the Actions tab on the UI. If any errors happen, they will be reported here. After the automatic discovery process completes, go to the Databases tab. You will see an entry for Repository For Our First Plugin . This is the repository you created in your Python code. Notice that it says No databases found on installation . This is because we chose not to do automatic source config discovery. However, because we have allowed manual source config discovery, you can add your own entries by clicking the plus sign ( Add Database ). Complete the information in the Add Database dialog and click Add. This should all look familiar. It is precisely what we defined in our source config schema. As expected, there are two entries, one for our name property, and one for path . For example, in the above screenshot, we are specifying that we want to sync the /bin directory from the remote host, and we want to call it Binaries . You can pick any directory and name that you want. Once you have added one or more source configs, you will be able to sync. This is covered on the next page. Warning Once you have automatically or manually created source configs, you will not be allowed to modify your plugin's source config schema. We will cover how to deal with this later in the upgrade section. For now, if you need to change your plugin's source config schema: You will have to delete any source configs you have manually added. Delete the plugin and its corresponding objects (dSources, Virtual Sources, etc) if the source configs were manually discovered. Survey Please fill out this survey to give us feedback about this section.","title":"How to Run Discovery in the Delphix Engine"},{"location":"Building_Your_First_Plugin/Initial_Setup/","text":"Initial Setup \u00b6 Before we begin to start writing plugin code, we will need to do some setup work. We will be using the dvp tool, which is described in the Getting Started section. The quoted examples in this section assume you're working on a Unix-like system. Sanity check \u00b6 First a reminder that it's highly recommended that you develop your plugin in a virtual environment . Next, make sure you have a Delphix Engine ready to use, as described in the Prerequisites section on the previous page. Finally, let's quickly make sure that dvp is working! Type dvp -h and you should see something like the following: (venv)$ dvp -h Usage: dvp [OPTIONS] COMMAND [ARGS]... The tools of the Delphix Virtualization SDK that help develop, build, and upload a plugin. Options: --version Show the version and exit. -v, --verbose Enable verbose mode. Can be repeated up to three times for increased verbosity. -q, --quiet Enable quiet mode. Can be repeated up to three times for increased suppression. -h, --help Show this message and exit. Commands: build Build the plugin code and generate upload artifact file... download-logs Download plugin logs from a target Delphix Engine to a... init Create a plugin in the root directory. upload Upload the generated upload artifact (the plugin JSON file)... If this looks good, you are ready to begin! If, instead, you see something like the following, go back to Getting Started and make sure you setup everything correctly before continuing. (venv)$ dvp -bash: dvp: command not found Creating a Bare Plugin \u00b6 To start, we will create a new directory where our new plugin code will live. (venv)$ mkdir first_plugin (venv)$ cd first_plugin Now that we are in our new plugin directory, we can use the dvp tool to create a plugin for us. This plugin will be a mere skeleton -- it will not do anything useful until we modify it in the subsequent pages. (venv) first_plugin$ dvp init -n first_plugin -s STAGED The -n argument here means \"plugin name.\" We are using the name first_plugin . The -s argument tells which syncing strategy we want to use. You can type dvp init -h for more information about the options available. After running this command, you should see that files have been created for you: (venv) first_plugin$ ls plugin_config.yml schema.json src These files are described below: File Description plugin_config.yml The plugin config file, which provides a list of plugin properties schema.json Contains schemas which provide custom datatype definitions src/plugin_runner.py A Python file which will eventually contain code that handles plugin operations Open these files in your editor/IDE and take a look at them. At this point they will not have a lot of content, but we will add to them as we go through the next few pages. Building The New Plugin \u00b6 The new files we created above have to get built to produce a single artifact . This is done with the dvp tool. (venv) first_plugin$ dvp build After the build, you should see that the build process has created a new file called artifact.json . (venv) first_plugin$ ls artifact.json plugin_config.yml schema.json src Uploading The New Plugin \u00b6 Now using the dvp tool we can upload the artifact onto our Delphix Engine. (venv) first_plugin$ dvp upload -e engine.company.com -u admin The -e argument specifies the engine on which to install the plugin, and the -u argument gives the Delphix Engine user. You will be prompted for a password. Once the upload is finished, you can verify the installation from the Manage > Toolkits screen in the Delphix Engine UI. Survey Please fill out this survey to give us feedback about this section.","title":"Initial Setup"},{"location":"Building_Your_First_Plugin/Initial_Setup/#initial-setup","text":"Before we begin to start writing plugin code, we will need to do some setup work. We will be using the dvp tool, which is described in the Getting Started section. The quoted examples in this section assume you're working on a Unix-like system.","title":"Initial Setup"},{"location":"Building_Your_First_Plugin/Initial_Setup/#sanity-check","text":"First a reminder that it's highly recommended that you develop your plugin in a virtual environment . Next, make sure you have a Delphix Engine ready to use, as described in the Prerequisites section on the previous page. Finally, let's quickly make sure that dvp is working! Type dvp -h and you should see something like the following: (venv)$ dvp -h Usage: dvp [OPTIONS] COMMAND [ARGS]... The tools of the Delphix Virtualization SDK that help develop, build, and upload a plugin. Options: --version Show the version and exit. -v, --verbose Enable verbose mode. Can be repeated up to three times for increased verbosity. -q, --quiet Enable quiet mode. Can be repeated up to three times for increased suppression. -h, --help Show this message and exit. Commands: build Build the plugin code and generate upload artifact file... download-logs Download plugin logs from a target Delphix Engine to a... init Create a plugin in the root directory. upload Upload the generated upload artifact (the plugin JSON file)... If this looks good, you are ready to begin! If, instead, you see something like the following, go back to Getting Started and make sure you setup everything correctly before continuing. (venv)$ dvp -bash: dvp: command not found","title":"Sanity check"},{"location":"Building_Your_First_Plugin/Initial_Setup/#creating-a-bare-plugin","text":"To start, we will create a new directory where our new plugin code will live. (venv)$ mkdir first_plugin (venv)$ cd first_plugin Now that we are in our new plugin directory, we can use the dvp tool to create a plugin for us. This plugin will be a mere skeleton -- it will not do anything useful until we modify it in the subsequent pages. (venv) first_plugin$ dvp init -n first_plugin -s STAGED The -n argument here means \"plugin name.\" We are using the name first_plugin . The -s argument tells which syncing strategy we want to use. You can type dvp init -h for more information about the options available. After running this command, you should see that files have been created for you: (venv) first_plugin$ ls plugin_config.yml schema.json src These files are described below: File Description plugin_config.yml The plugin config file, which provides a list of plugin properties schema.json Contains schemas which provide custom datatype definitions src/plugin_runner.py A Python file which will eventually contain code that handles plugin operations Open these files in your editor/IDE and take a look at them. At this point they will not have a lot of content, but we will add to them as we go through the next few pages.","title":"Creating a Bare Plugin"},{"location":"Building_Your_First_Plugin/Initial_Setup/#building-the-new-plugin","text":"The new files we created above have to get built to produce a single artifact . This is done with the dvp tool. (venv) first_plugin$ dvp build After the build, you should see that the build process has created a new file called artifact.json . (venv) first_plugin$ ls artifact.json plugin_config.yml schema.json src","title":"Building The New Plugin"},{"location":"Building_Your_First_Plugin/Initial_Setup/#uploading-the-new-plugin","text":"Now using the dvp tool we can upload the artifact onto our Delphix Engine. (venv) first_plugin$ dvp upload -e engine.company.com -u admin The -e argument specifies the engine on which to install the plugin, and the -u argument gives the Delphix Engine user. You will be prompted for a password. Once the upload is finished, you can verify the installation from the Manage > Toolkits screen in the Delphix Engine UI. Survey Please fill out this survey to give us feedback about this section.","title":"Uploading The New Plugin"},{"location":"Building_Your_First_Plugin/Overview/","text":"Overview \u00b6 In the following few pages, we will walk through an example of making a simple, working plugin. Our plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin will not care about the contents and will treat it as a directory tree full of files. Data Flow in the Delphix Engine \u00b6 Here we will briefly overview how data moves through the Delphix Engine. Ingestion \u00b6 It all begins with Delphix ingesting data\u2014copying some data from what we call a source environment onto the Delphix Engine. Plugins can use either of two basic strategies to do this copying: direct linking , where the Delphix Engine pulls data directly from the source environment. staged linking , where the plugin is responsible for pulling data from the source environment. Our plugin will use the staged linking strategy. With staged linking, Delphix exposes and mounts storage to a staging environment . This would be an NFS share for Unix environments and iSCSI disks for Windows environments. You can use either the source environment or a different environment for staging. We will write our plugin to handle both approaches. Once Delphix mounts the storage share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the storage share, which is backed by Delphix Engine storage. When this initial copy is complete, Delphix will take a snapshot of the backing storage. ( Diagram coming soon! ) This same basic operation will be repeated when Delphix mounts an NFS share: The plugin copies data onto it, then Delphix snapshots the result. Provisioning \u00b6 Provisioning is when you take a Delphix Engine snapshot and create a virtual dataset from it. First the snapshot is cloned onto the Delphix Engine, then this newly-cloned data is mounted as a virtual dataset onto a target environment . While this new virtual dataset gets updated by its end users, the original snapshot is persistent. You can use it in a few ways: Provision other virtual datasets from it Rewind the virtual dataset back to the state it represents Create a physical database from it in what we call V2P: Virtual to Physical ( Diagram coming soon! ) Parts of a Plugin \u00b6 A plugin consists of three main parts. We will cover them briefly here, and then fill in more details later in the tutorial. Plugin Config \u00b6 Plugin config is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is being used? What type(s) of environments does the plugin work with? What features does the plugin offer?... Plugin Operations \u00b6 The plugin will need to provide operations. These are Python functions, each of which implements one small piece of functionality. This is how the plugin customizes Delphix behavior to work with the kind of dataset you\u2019re building the plugin for. One operation will handle setting up a newly-configured virtual dataset. Another will handle copying data from a source environment, and so on. Later we\u2019ll provide examples for our first plugin. See Plugin Operations for full details on the operations that are available, which are required, and what each one is required to do. Schemas \u00b6 As part of normal operations, plugins need to generate and access certain pieces of information in order to do their job. For example, plugins that work with Postgres might need to know which port number to connect to, or which credentials to use. Defining your plugin\u2019s schemas will enable it to give the Delphix Engine the details it needs to run the operations we\u2019ve built into it. Different datasets can have very different needs. The schemas you provide for your plugin will tell Delphix how to operate with your dataset. Prerequisites \u00b6 To complete the tutorial that follows, make sure you check off the things on this list: Download the SDK and get it working A running Delphix Engine, version x.y.z or above. Add at least one Unix host\u2014but preferably three\u2014to the Delphix Engine as remote environments Have a tool at hand for editing text files\u2014mostly Python and JSON. A simple text editor would work fine, or you can use a full-fledged IDE. Survey Please fill out this survey to give us feedback about this section.","title":"Virtualization SDK"},{"location":"Building_Your_First_Plugin/Overview/#overview","text":"In the following few pages, we will walk through an example of making a simple, working plugin. Our plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin will not care about the contents and will treat it as a directory tree full of files.","title":"Overview"},{"location":"Building_Your_First_Plugin/Overview/#data-flow-in-the-delphix-engine","text":"Here we will briefly overview how data moves through the Delphix Engine.","title":"Data Flow in the Delphix Engine"},{"location":"Building_Your_First_Plugin/Overview/#ingestion","text":"It all begins with Delphix ingesting data\u2014copying some data from what we call a source environment onto the Delphix Engine. Plugins can use either of two basic strategies to do this copying: direct linking , where the Delphix Engine pulls data directly from the source environment. staged linking , where the plugin is responsible for pulling data from the source environment. Our plugin will use the staged linking strategy. With staged linking, Delphix exposes and mounts storage to a staging environment . This would be an NFS share for Unix environments and iSCSI disks for Windows environments. You can use either the source environment or a different environment for staging. We will write our plugin to handle both approaches. Once Delphix mounts the storage share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the storage share, which is backed by Delphix Engine storage. When this initial copy is complete, Delphix will take a snapshot of the backing storage. ( Diagram coming soon! ) This same basic operation will be repeated when Delphix mounts an NFS share: The plugin copies data onto it, then Delphix snapshots the result.","title":"Ingestion"},{"location":"Building_Your_First_Plugin/Overview/#provisioning","text":"Provisioning is when you take a Delphix Engine snapshot and create a virtual dataset from it. First the snapshot is cloned onto the Delphix Engine, then this newly-cloned data is mounted as a virtual dataset onto a target environment . While this new virtual dataset gets updated by its end users, the original snapshot is persistent. You can use it in a few ways: Provision other virtual datasets from it Rewind the virtual dataset back to the state it represents Create a physical database from it in what we call V2P: Virtual to Physical ( Diagram coming soon! )","title":"Provisioning"},{"location":"Building_Your_First_Plugin/Overview/#parts-of-a-plugin","text":"A plugin consists of three main parts. We will cover them briefly here, and then fill in more details later in the tutorial.","title":"Parts of a Plugin"},{"location":"Building_Your_First_Plugin/Overview/#plugin-config","text":"Plugin config is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is being used? What type(s) of environments does the plugin work with? What features does the plugin offer?...","title":"Plugin Config"},{"location":"Building_Your_First_Plugin/Overview/#plugin-operations","text":"The plugin will need to provide operations. These are Python functions, each of which implements one small piece of functionality. This is how the plugin customizes Delphix behavior to work with the kind of dataset you\u2019re building the plugin for. One operation will handle setting up a newly-configured virtual dataset. Another will handle copying data from a source environment, and so on. Later we\u2019ll provide examples for our first plugin. See Plugin Operations for full details on the operations that are available, which are required, and what each one is required to do.","title":"Plugin Operations"},{"location":"Building_Your_First_Plugin/Overview/#schemas","text":"As part of normal operations, plugins need to generate and access certain pieces of information in order to do their job. For example, plugins that work with Postgres might need to know which port number to connect to, or which credentials to use. Defining your plugin\u2019s schemas will enable it to give the Delphix Engine the details it needs to run the operations we\u2019ve built into it. Different datasets can have very different needs. The schemas you provide for your plugin will tell Delphix how to operate with your dataset.","title":"Schemas"},{"location":"Building_Your_First_Plugin/Overview/#prerequisites","text":"To complete the tutorial that follows, make sure you check off the things on this list: Download the SDK and get it working A running Delphix Engine, version x.y.z or above. Add at least one Unix host\u2014but preferably three\u2014to the Delphix Engine as remote environments Have a tool at hand for editing text files\u2014mostly Python and JSON. A simple text editor would work fine, or you can use a full-fledged IDE. Survey Please fill out this survey to give us feedback about this section.","title":"Prerequisites"},{"location":"Building_Your_First_Plugin/Provisioning/","text":"Provisioning \u00b6 What is Provisioning? \u00b6 Once Delphix has a snapshot of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new virtual dataset . This new virtual dataset will be made available for use on a target environment . This process is called provisioning . Our Provisioning Strategy \u00b6 For many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment. In our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling where that data is mounted. Defining our Provision-Related Data Formats \u00b6 We have already seen four custom data formats: for repositories, source configs, snapshots and linked sources. The final one is used for virtual sources . Recall that, for our plugin, a VDB is just a directory full of files. There is no special procedure needed to enable it, no DBMS to coordinate with, etc. All we need to do is make the files available on the target environment. So, the only question for the user is \"Where should these files live?\" Open up schema.json , locate the virtualSourceDefintion section, and change it to look like this: \"virtualSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"mountLocation\" ], \"properties\" : { \"mountLocation\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Mount Location on Target Host\" , \"description\" : \"Where to mount VDB onto the target host\" } } } , This should look familiar from the source config schema that we did earlier. We only have one property, and it represents the mount location on the target environment. Implementing Provisioning \u00b6 There are numerous ways for a plugin to customize the provisioning process. For our example plugin, we just need to do a few things: Tell Delphix where to mount the virtual dataset. Create a sourceConfig to represent each newly-provisioned virtual dataset. Modify an existing sourceConfig , if necessary, when the virtual dataset is refreshed or rewound. Construct snapshot-related data any time a snapshot is taken of the virtual dataset. Controlling Mounting \u00b6 As we saw previously with linked sources, we need to tell Delphix where to mount the dataset. Open up plugin_runner.py and find the plugin.virtual.mount_specification decorator. Change that function so that it looks like this: @plugin.virtual.mount_specification () def vdb_mount_spec ( virtual_source , repository ): mount_location = virtual_source . parameters . mount_location mount = Mount ( virtual_source . connection . environment , mount_location ) return MountSpecification ([ mount ]) As we did with linked sources, we just look up what the user told us, and then package that up and return it to Delphix. Creating a Source Config for a new VDB \u00b6 Just like we saw earlier with linked datasets , each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config at provision time As a reminder, here is what our schema looks like for source configs: \"sourceConfigDefinition\" : { \"type\" : \"object\" , \"required\" : [ \"name\" , \"path\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Dataset Name\" , \"description\" : \"User-visible name for this dataset\" }, \"path\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Path\" , \"description\" : \"Full path to data location on the remote environment\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"path\" ] } , Thus, for each newly-cloned virtual dataset, we create a new source config object with a name and a path. This is done by the configure plugin operation. In addition to generating a new source config, the configure operation is also tasked with getting the newly-cloned dataset ready for use on the target environment. What this means exactly will vary from plugin to plugin. For our simple plugin, the dataset does not require any setup work, and so we only have to worry about the source config. Find the plugin.virtual.configure decorator and change the function to look like this: @plugin.virtual.configure () def configure_new_vdb ( virtual_source , snapshot , repository ): mount_location = virtual_source . parameters . mount_location name = \"VDB mounted at {}\" . format ( mount_location ) return SourceConfigDefinition ( path = mount_location , name = name ) Modifying a Source Config after Rewind or Refresh \u00b6 Just as a new VDB might need to be configured, a refreshed or rewound VDB might need to be \"reconfigured\" to handle the new post-refresh (or post-rewind) state of the VDB. So, just as there is a configure operation, there is also a reconfigure operation. The main difference between the two is that configure must create a source config, but reconfigure needs to modify a pre-existing source config. In our simple plugin, there is no special work to do at reconfigure time, and there is no reason to modify anything about the source config. We just need to write a reconfigure operation that returns the existing source config without making any changes. Find the plugin.virtual.reconfigure decorator and modify the function as follows. @plugin.virtual.reconfigure () def reconfigure_existing_vdb ( virtual_source , repository , source_config , snapshot ): return source_config Saving Snapshot Data \u00b6 As with our linked sources, we don't actually have anything we need to save when VDB snapshots are taken. And, again, dvp init has created a post-snapshot operation that will work just fine for us without modification: @plugin.virtual.post_snapshot () def virtual_post_snapshot ( virtual_source , repository , source_config ): return SnapshotDefinition () How To Provision in the Delphix Engine \u00b6 Finally, let us try it out to make sure provisioning works! Again, use dvp build and dvp upload to get your new changes onto your Delphix Engine. Click Manage > Datasets . Select the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the Provision vFiles icon. This will open the Provision VDB wizard. Complete the steps and select Submit . During VDB provisioning one of the things you will have to do is to provide the data required by your virtual source schema. In our case, that means you will be asked to provide a value for mountLocation . You will also be asked to choose a target environment on which the new VDB will live. After the wizard finishes, you will see a job appear in the Actions tab on the right-hand side of the screen. When that job completes, your new VDB should be ready. To ensure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the mountLocation . What you should see is a copy of the directory that you linked to with your dSource. Survey Please fill out this survey to give us feedback about this section.","title":"Virtualization SDK"},{"location":"Building_Your_First_Plugin/Provisioning/#provisioning","text":"","title":"Provisioning"},{"location":"Building_Your_First_Plugin/Provisioning/#what-is-provisioning","text":"Once Delphix has a snapshot of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new virtual dataset . This new virtual dataset will be made available for use on a target environment . This process is called provisioning .","title":"What is Provisioning?"},{"location":"Building_Your_First_Plugin/Provisioning/#our-provisioning-strategy","text":"For many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment. In our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling where that data is mounted.","title":"Our Provisioning Strategy"},{"location":"Building_Your_First_Plugin/Provisioning/#defining-our-provision-related-data-formats","text":"We have already seen four custom data formats: for repositories, source configs, snapshots and linked sources. The final one is used for virtual sources . Recall that, for our plugin, a VDB is just a directory full of files. There is no special procedure needed to enable it, no DBMS to coordinate with, etc. All we need to do is make the files available on the target environment. So, the only question for the user is \"Where should these files live?\" Open up schema.json , locate the virtualSourceDefintion section, and change it to look like this: \"virtualSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"mountLocation\" ], \"properties\" : { \"mountLocation\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Mount Location on Target Host\" , \"description\" : \"Where to mount VDB onto the target host\" } } } , This should look familiar from the source config schema that we did earlier. We only have one property, and it represents the mount location on the target environment.","title":"Defining our Provision-Related Data Formats"},{"location":"Building_Your_First_Plugin/Provisioning/#implementing-provisioning","text":"There are numerous ways for a plugin to customize the provisioning process. For our example plugin, we just need to do a few things: Tell Delphix where to mount the virtual dataset. Create a sourceConfig to represent each newly-provisioned virtual dataset. Modify an existing sourceConfig , if necessary, when the virtual dataset is refreshed or rewound. Construct snapshot-related data any time a snapshot is taken of the virtual dataset.","title":"Implementing Provisioning"},{"location":"Building_Your_First_Plugin/Provisioning/#controlling-mounting","text":"As we saw previously with linked sources, we need to tell Delphix where to mount the dataset. Open up plugin_runner.py and find the plugin.virtual.mount_specification decorator. Change that function so that it looks like this: @plugin.virtual.mount_specification () def vdb_mount_spec ( virtual_source , repository ): mount_location = virtual_source . parameters . mount_location mount = Mount ( virtual_source . connection . environment , mount_location ) return MountSpecification ([ mount ]) As we did with linked sources, we just look up what the user told us, and then package that up and return it to Delphix.","title":"Controlling Mounting"},{"location":"Building_Your_First_Plugin/Provisioning/#creating-a-source-config-for-a-new-vdb","text":"Just like we saw earlier with linked datasets , each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config at provision time As a reminder, here is what our schema looks like for source configs: \"sourceConfigDefinition\" : { \"type\" : \"object\" , \"required\" : [ \"name\" , \"path\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Dataset Name\" , \"description\" : \"User-visible name for this dataset\" }, \"path\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Path\" , \"description\" : \"Full path to data location on the remote environment\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"path\" ] } , Thus, for each newly-cloned virtual dataset, we create a new source config object with a name and a path. This is done by the configure plugin operation. In addition to generating a new source config, the configure operation is also tasked with getting the newly-cloned dataset ready for use on the target environment. What this means exactly will vary from plugin to plugin. For our simple plugin, the dataset does not require any setup work, and so we only have to worry about the source config. Find the plugin.virtual.configure decorator and change the function to look like this: @plugin.virtual.configure () def configure_new_vdb ( virtual_source , snapshot , repository ): mount_location = virtual_source . parameters . mount_location name = \"VDB mounted at {}\" . format ( mount_location ) return SourceConfigDefinition ( path = mount_location , name = name )","title":"Creating a Source Config for a new VDB"},{"location":"Building_Your_First_Plugin/Provisioning/#modifying-a-source-config-after-rewind-or-refresh","text":"Just as a new VDB might need to be configured, a refreshed or rewound VDB might need to be \"reconfigured\" to handle the new post-refresh (or post-rewind) state of the VDB. So, just as there is a configure operation, there is also a reconfigure operation. The main difference between the two is that configure must create a source config, but reconfigure needs to modify a pre-existing source config. In our simple plugin, there is no special work to do at reconfigure time, and there is no reason to modify anything about the source config. We just need to write a reconfigure operation that returns the existing source config without making any changes. Find the plugin.virtual.reconfigure decorator and modify the function as follows. @plugin.virtual.reconfigure () def reconfigure_existing_vdb ( virtual_source , repository , source_config , snapshot ): return source_config","title":"Modifying a Source Config after Rewind or Refresh"},{"location":"Building_Your_First_Plugin/Provisioning/#saving-snapshot-data","text":"As with our linked sources, we don't actually have anything we need to save when VDB snapshots are taken. And, again, dvp init has created a post-snapshot operation that will work just fine for us without modification: @plugin.virtual.post_snapshot () def virtual_post_snapshot ( virtual_source , repository , source_config ): return SnapshotDefinition ()","title":"Saving Snapshot Data"},{"location":"Building_Your_First_Plugin/Provisioning/#how-to-provision-in-the-delphix-engine","text":"Finally, let us try it out to make sure provisioning works! Again, use dvp build and dvp upload to get your new changes onto your Delphix Engine. Click Manage > Datasets . Select the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the Provision vFiles icon. This will open the Provision VDB wizard. Complete the steps and select Submit . During VDB provisioning one of the things you will have to do is to provide the data required by your virtual source schema. In our case, that means you will be asked to provide a value for mountLocation . You will also be asked to choose a target environment on which the new VDB will live. After the wizard finishes, you will see a job appear in the Actions tab on the right-hand side of the screen. When that job completes, your new VDB should be ready. To ensure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the mountLocation . What you should see is a copy of the directory that you linked to with your dSource. Survey Please fill out this survey to give us feedback about this section.","title":"How To Provision in the Delphix Engine"},{"location":"References/CLI/","text":"CLI \u00b6 The CLI is installed with the SDK. To install the SDK, refer to the Getting Started section. Help \u00b6 Every command has a -h flag including the CLI itself. This will print the help menu. Examples \u00b6 Get the CLI's help menu. $ dvp -h Usage: dvp [OPTIONS] COMMAND [ARGS]... The tools of the Delphix Virtualization SDK that help develop, build, and upload a plugin. Options: --version Show the version and exit. -v, --verbose Enable verbose mode. Can be repeated up to three times for increased verbosity. -q, --quiet Enable quiet mode. Can be repeated up to three times for increased suppression. -h, --help Show this message and exit. Commands: build Build the plugin code and generate upload artifact file using the... init Create a plugin in the root directory. upload Upload the generated upload artifact (the plugin JSON file) that... Get the build command's help menu. $ dvp build -h Usage: dvp build [OPTIONS] Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file. Options: -c, --plugin-config FILE Set the path to plugin config file.This file contains the configuration required to build the plugin. [default: plugin_config.yml] -a, --upload-artifact FILE Set the upload artifact.The upload artifact file generated by build process will be writtento this file and later used by upload command. [default: artifact.json] -g, --generate-only Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact. [default: False] -h, --help Show this message and exit. Verbosity \u00b6 To change the verbosity level of the CLI you can specify up to three -v (to increase) or -q (to decrease) the amount that is printed to the console. This is an option on the CLI itself and can be used with any command. Option Output -qqq None -qq Critical -q Error -v Info -vv Debug -vvv All Examples \u00b6 Print everything to the console. $ dvp -vvv build Print nothing to the console. $ dvp -qqq upload -e engine.example.com -u admin Commands \u00b6 init \u00b6 Description \u00b6 Create a plugin in the root directory. The plugin will be valid but have no functionality. Options \u00b6 Option Description Required Default -r, --root-dir DIRECTORY Set the plugin root directory. N os.cwd() -n, --plugin-name TEXT Set the name of the plugin that will be used to identify it. N id -s, --ingestion-strategy [DIRECT|STAGED] Set the ingestion strategy of the plugin. A \"direct\" plugin ingests without a staging server while a \"staged\" plugin requires a staging server. N DIRECT Examples \u00b6 Create a plugin in the current working directory with the DIRECT ingestion strategy. Here the name of the plugin will be equal to the id that is generated. $ dvp init Create a plugin in the current working directory with the DIRECT ingestion strategy and use postgres as the display name. $ dvp init -n postgres Create a plugin called mongodb in a custom location with the STAGED ingestion strategy. $ dvp init -n mongodb -s STAGED build \u00b6 Description \u00b6 Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file. Options \u00b6 Option Description Required Default -c, --plugin-config FILE Set the path to plugin config file.This file contains the configuration required to build the plugin. N plugin_config.yml -a, --upload-artifact FILE Set the upload artifact.The upload artifact file generated by build process will be written to this file and later used by upload command. N artifact.json -g, --generate-only Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact. N False Examples \u00b6 Do a full build of the plugin and write the upload artifact to ./artifact.json . This assumes current working directory contains a plugin config file named plugin_config.yml . $ dvp build Do a partial build and just generate the Python classes from the schema definitions. This assumes current working directory contains ad plugin config file named plugin_config.yml . $ dvp build -g Do a full build of a plugin and write the artifact file to a custom location. $ dvp build -c config.yml -a build/artifact.json upload \u00b6 Description \u00b6 Upload the generated upload artifact (the plugin JSON file) that was built to a target Delphix Engine. Note that the upload artifact should be the file created after running the build command and will fail if it's not readable or valid. Options \u00b6 Option Description Required Default -e, --delphix-engine TEXT Upload plugin to the provided engine. This should be either the hostname or IP address. Y None -u, --user TEXT Authenticate to the Delphix Engine with the provided user. Y None -a, --upload-artifact FILE Path to the upload artifact that was generated through build. N artifact.json --password TEXT Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt. N None Examples \u00b6 Upload artifact build/artifact.json to delphix-engine.domain using the user admin . Since the password option is ommitted, a secure password prompt is used instead. $ dvp upload -a build/artifact -e engine.example.com -u admin Password: download-logs \u00b6 Description \u00b6 Download plugin logs from a Delphix Engine to a local directory. Options \u00b6 Option Description Required Default -e, --delphix-engine TEXT Download plugin logs from the provided Delphix engine. This should be either the hostname or IP address. Y None -c, --plugin-config FILE Set the path to plugin config file. This file contains the plugin name to download logs for. N plugin_config.yml -u, --user TEXT Authenticate to the Delphix Engine with the provided user. Y None -d, --directory DIRECTORY Specify the directory of where to download the plugin logs. N os.cwd() --password TEXT Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt. N None Examples \u00b6 Download plugin logs from engine.example.com using the user admin . Since the password option is ommitted, a secure password prompt is used instead. $ dvp download-logs -e engine.example.com -u admin Password:","title":"CLI"},{"location":"References/CLI/#cli","text":"The CLI is installed with the SDK. To install the SDK, refer to the Getting Started section.","title":"CLI"},{"location":"References/CLI/#help","text":"Every command has a -h flag including the CLI itself. This will print the help menu.","title":"Help"},{"location":"References/CLI/#examples","text":"Get the CLI's help menu. $ dvp -h Usage: dvp [OPTIONS] COMMAND [ARGS]... The tools of the Delphix Virtualization SDK that help develop, build, and upload a plugin. Options: --version Show the version and exit. -v, --verbose Enable verbose mode. Can be repeated up to three times for increased verbosity. -q, --quiet Enable quiet mode. Can be repeated up to three times for increased suppression. -h, --help Show this message and exit. Commands: build Build the plugin code and generate upload artifact file using the... init Create a plugin in the root directory. upload Upload the generated upload artifact (the plugin JSON file) that... Get the build command's help menu. $ dvp build -h Usage: dvp build [OPTIONS] Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file. Options: -c, --plugin-config FILE Set the path to plugin config file.This file contains the configuration required to build the plugin. [default: plugin_config.yml] -a, --upload-artifact FILE Set the upload artifact.The upload artifact file generated by build process will be writtento this file and later used by upload command. [default: artifact.json] -g, --generate-only Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact. [default: False] -h, --help Show this message and exit.","title":"Examples"},{"location":"References/CLI/#verbosity","text":"To change the verbosity level of the CLI you can specify up to three -v (to increase) or -q (to decrease) the amount that is printed to the console. This is an option on the CLI itself and can be used with any command. Option Output -qqq None -qq Critical -q Error -v Info -vv Debug -vvv All","title":"Verbosity"},{"location":"References/CLI/#examples_1","text":"Print everything to the console. $ dvp -vvv build Print nothing to the console. $ dvp -qqq upload -e engine.example.com -u admin","title":"Examples"},{"location":"References/CLI/#commands","text":"","title":"Commands"},{"location":"References/CLI/#init","text":"","title":"init"},{"location":"References/CLI/#description","text":"Create a plugin in the root directory. The plugin will be valid but have no functionality.","title":"Description"},{"location":"References/CLI/#options","text":"Option Description Required Default -r, --root-dir DIRECTORY Set the plugin root directory. N os.cwd() -n, --plugin-name TEXT Set the name of the plugin that will be used to identify it. N id -s, --ingestion-strategy [DIRECT|STAGED] Set the ingestion strategy of the plugin. A \"direct\" plugin ingests without a staging server while a \"staged\" plugin requires a staging server. N DIRECT","title":"Options"},{"location":"References/CLI/#examples_2","text":"Create a plugin in the current working directory with the DIRECT ingestion strategy. Here the name of the plugin will be equal to the id that is generated. $ dvp init Create a plugin in the current working directory with the DIRECT ingestion strategy and use postgres as the display name. $ dvp init -n postgres Create a plugin called mongodb in a custom location with the STAGED ingestion strategy. $ dvp init -n mongodb -s STAGED","title":"Examples"},{"location":"References/CLI/#build","text":"","title":"build"},{"location":"References/CLI/#description_1","text":"Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file.","title":"Description"},{"location":"References/CLI/#options_1","text":"Option Description Required Default -c, --plugin-config FILE Set the path to plugin config file.This file contains the configuration required to build the plugin. N plugin_config.yml -a, --upload-artifact FILE Set the upload artifact.The upload artifact file generated by build process will be written to this file and later used by upload command. N artifact.json -g, --generate-only Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact. N False","title":"Options"},{"location":"References/CLI/#examples_3","text":"Do a full build of the plugin and write the upload artifact to ./artifact.json . This assumes current working directory contains a plugin config file named plugin_config.yml . $ dvp build Do a partial build and just generate the Python classes from the schema definitions. This assumes current working directory contains ad plugin config file named plugin_config.yml . $ dvp build -g Do a full build of a plugin and write the artifact file to a custom location. $ dvp build -c config.yml -a build/artifact.json","title":"Examples"},{"location":"References/CLI/#upload","text":"","title":"upload"},{"location":"References/CLI/#description_2","text":"Upload the generated upload artifact (the plugin JSON file) that was built to a target Delphix Engine. Note that the upload artifact should be the file created after running the build command and will fail if it's not readable or valid.","title":"Description"},{"location":"References/CLI/#options_2","text":"Option Description Required Default -e, --delphix-engine TEXT Upload plugin to the provided engine. This should be either the hostname or IP address. Y None -u, --user TEXT Authenticate to the Delphix Engine with the provided user. Y None -a, --upload-artifact FILE Path to the upload artifact that was generated through build. N artifact.json --password TEXT Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt. N None","title":"Options"},{"location":"References/CLI/#examples_4","text":"Upload artifact build/artifact.json to delphix-engine.domain using the user admin . Since the password option is ommitted, a secure password prompt is used instead. $ dvp upload -a build/artifact -e engine.example.com -u admin Password:","title":"Examples"},{"location":"References/CLI/#download-logs","text":"","title":"download-logs"},{"location":"References/CLI/#description_3","text":"Download plugin logs from a Delphix Engine to a local directory.","title":"Description"},{"location":"References/CLI/#options_3","text":"Option Description Required Default -e, --delphix-engine TEXT Download plugin logs from the provided Delphix engine. This should be either the hostname or IP address. Y None -c, --plugin-config FILE Set the path to plugin config file. This file contains the plugin name to download logs for. N plugin_config.yml -u, --user TEXT Authenticate to the Delphix Engine with the provided user. Y None -d, --directory DIRECTORY Specify the directory of where to download the plugin logs. N os.cwd() --password TEXT Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt. N None","title":"Options"},{"location":"References/CLI/#examples_5","text":"Download plugin logs from engine.example.com using the user admin . Since the password option is ommitted, a secure password prompt is used instead. $ dvp download-logs -e engine.example.com -u admin Password:","title":"Examples"},{"location":"References/Classes/","text":"Classes \u00b6 DirectSource \u00b6 Represents a Linked Source object and its properties when using a Direct Linking strategy. from dlpx.virtualization.platform import DirectSource direct_source = DirectSource ( guid , connection , parameters ) Fields \u00b6 Field Type Description guid String Unique Identifier for the source. connection RemoteConnection Connection for the source environment. parameters LinkedSourceDefinition User input as per the LinkedSource Schema . StagedSource \u00b6 Represents a Linked Source object and its properties when using a Staged Linking strategy. from dlpx.virtualization.platform import StagedSource staged_source = StagedSource ( guid , source_connection , parameters , mount , staged_connection ) Fields \u00b6 Field Type Description guid String Unique Identifier for the source. source_connection RemoteConnection Connection for the source environment. parameters LinkedSourceDefinition User input as per the LinkedSource Schema . mount Mount Mount point associated with the source. staged_connection RemoteConnection Connection for the staging environment. VirtualSource \u00b6 Represents a Virtual Source object and its properties. from dlpx.virtualization.platform import VirtualSource virtual_source = VirtualSource ( guid , connection , parameters , mounts ) Fields \u00b6 Field Type Description guid String Unique Identifier for the source. connection RemoteConnection Connection for the source environment. parameters VirtualSourceDefinition User input as per the VirtualSource Schema . mounts list[ Mount ] Mount points associated with the source. RemoteConnection \u00b6 Represents a connection to a source. from dlpx.virtualization.platform import RemoteConnection connection = RemoteConnection ( environment , user ) Fields \u00b6 Field Type Description environment RemoteEnvironment Environment for the connection. Internal virtualization platform object. user RemoteUser User for the connection. Internal virtualization platform object. Status \u00b6 An enum used to represent the state of a linked or virtual source and whether is functioning as expected. from dlpx.virtualization.platform import Status status = Status . ACTIVE Values \u00b6 Value Description ACTIVE Source is healthy and functioning as expected. INACTIVE Source is not functioning as expected. Mount \u00b6 Represents a mount exported and mounted to a remote host. from dlpx.virtualization.platform import Mount mount = Mount ( environment , path ) Fields \u00b6 Field Type Description remote_environment RemoteEnvironment Environment for the connection. Internal virtualization platform object. mount_path String The path on the remote host that has the mounted data set. shared_path String Optional. The path of the subdirectory of the data set to mount to the remote host. OwnershipSpecification \u00b6 Represents how to set the onwership for a data set. This only applies to Unix Hosts. from dlpx.virtualization.platform import OwnershipSpecification ownership_specification = OwnershipSpecification ( uid , gid ) Fields \u00b6 Field Type Description uid Integer The user id to set the ownership of the data set to. gid Integer The group id to set the ownership of the data set to. MountSpecification \u00b6 Represents properties for the mount associated with an exported data set. from dlpx.virtualization.platform import MountSpecification mount_specification = MountSpecification ([ mount ], ownership_specification ) Fields \u00b6 Field Type Description mounts list[ Mount ] The list of mounts to export the data sets to. ownership_specification OwnershipSpecification Optional. Control the ownership attributes for the data set. It defaults to the environment user of the remote environment if it is not specified. SnapshotParametersDefinition \u00b6 User provided parameters for the snapshot operation. It includes a boolean property named resync that can be used to indicate to the plugin whether or not to initiate a full ingestion of the dSource. The parameters are only be set during a manual snapshot. When using a sync policy resync defaults to false . from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot ( staged_source , repository , source_config , snapshot_parameters ): if snapshot_parameter . resync : print ( snapshot_parameter . resync ) This class will be generated during build and is located with the autogenerated classes. As it is passed into the operation, importing it is not neccessary. Fields \u00b6 Field Type Description resync Boolean Determines if this snapshot should ingest the dSource from scratch. RemoteEnvironment \u00b6 Represents a remote environment. Warning Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified. Fields \u00b6 Field Type Description name String Name of the environment. host RemoteHost Host that belongs to the environment. Internal virtualization platform object. reference String Unique identifier for the environment. RemoteHost \u00b6 Represents a remote host, can we Unix or Windows. Warning Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified. Fields \u00b6 Field Type Description name String Host address. binary_path String Path to Delphix provided binaries on the host, which are present in the toolkit pushed to the remote host like dlpx_db_exec , dlpx_pfexec , etc. This property is only available for Unix hosts. reference String Unique identifier for the host. RemoteUser \u00b6 Represents a user on a remote host. Warning Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified. Fields \u00b6 Field Type Description name String User name. reference String Unique identifier for the user.","title":"Virtualization SDK"},{"location":"References/Classes/#classes","text":"","title":"Classes"},{"location":"References/Classes/#directsource","text":"Represents a Linked Source object and its properties when using a Direct Linking strategy. from dlpx.virtualization.platform import DirectSource direct_source = DirectSource ( guid , connection , parameters )","title":"DirectSource"},{"location":"References/Classes/#fields","text":"Field Type Description guid String Unique Identifier for the source. connection RemoteConnection Connection for the source environment. parameters LinkedSourceDefinition User input as per the LinkedSource Schema .","title":"Fields"},{"location":"References/Classes/#stagedsource","text":"Represents a Linked Source object and its properties when using a Staged Linking strategy. from dlpx.virtualization.platform import StagedSource staged_source = StagedSource ( guid , source_connection , parameters , mount , staged_connection )","title":"StagedSource"},{"location":"References/Classes/#fields_1","text":"Field Type Description guid String Unique Identifier for the source. source_connection RemoteConnection Connection for the source environment. parameters LinkedSourceDefinition User input as per the LinkedSource Schema . mount Mount Mount point associated with the source. staged_connection RemoteConnection Connection for the staging environment.","title":"Fields"},{"location":"References/Classes/#virtualsource","text":"Represents a Virtual Source object and its properties. from dlpx.virtualization.platform import VirtualSource virtual_source = VirtualSource ( guid , connection , parameters , mounts )","title":"VirtualSource"},{"location":"References/Classes/#fields_2","text":"Field Type Description guid String Unique Identifier for the source. connection RemoteConnection Connection for the source environment. parameters VirtualSourceDefinition User input as per the VirtualSource Schema . mounts list[ Mount ] Mount points associated with the source.","title":"Fields"},{"location":"References/Classes/#remoteconnection","text":"Represents a connection to a source. from dlpx.virtualization.platform import RemoteConnection connection = RemoteConnection ( environment , user )","title":"RemoteConnection"},{"location":"References/Classes/#fields_3","text":"Field Type Description environment RemoteEnvironment Environment for the connection. Internal virtualization platform object. user RemoteUser User for the connection. Internal virtualization platform object.","title":"Fields"},{"location":"References/Classes/#status","text":"An enum used to represent the state of a linked or virtual source and whether is functioning as expected. from dlpx.virtualization.platform import Status status = Status . ACTIVE","title":"Status"},{"location":"References/Classes/#values","text":"Value Description ACTIVE Source is healthy and functioning as expected. INACTIVE Source is not functioning as expected.","title":"Values"},{"location":"References/Classes/#mount","text":"Represents a mount exported and mounted to a remote host. from dlpx.virtualization.platform import Mount mount = Mount ( environment , path )","title":"Mount"},{"location":"References/Classes/#fields_4","text":"Field Type Description remote_environment RemoteEnvironment Environment for the connection. Internal virtualization platform object. mount_path String The path on the remote host that has the mounted data set. shared_path String Optional. The path of the subdirectory of the data set to mount to the remote host.","title":"Fields"},{"location":"References/Classes/#ownershipspecification","text":"Represents how to set the onwership for a data set. This only applies to Unix Hosts. from dlpx.virtualization.platform import OwnershipSpecification ownership_specification = OwnershipSpecification ( uid , gid )","title":"OwnershipSpecification"},{"location":"References/Classes/#fields_5","text":"Field Type Description uid Integer The user id to set the ownership of the data set to. gid Integer The group id to set the ownership of the data set to.","title":"Fields"},{"location":"References/Classes/#mountspecification","text":"Represents properties for the mount associated with an exported data set. from dlpx.virtualization.platform import MountSpecification mount_specification = MountSpecification ([ mount ], ownership_specification )","title":"MountSpecification"},{"location":"References/Classes/#fields_6","text":"Field Type Description mounts list[ Mount ] The list of mounts to export the data sets to. ownership_specification OwnershipSpecification Optional. Control the ownership attributes for the data set. It defaults to the environment user of the remote environment if it is not specified.","title":"Fields"},{"location":"References/Classes/#snapshotparametersdefinition","text":"User provided parameters for the snapshot operation. It includes a boolean property named resync that can be used to indicate to the plugin whether or not to initiate a full ingestion of the dSource. The parameters are only be set during a manual snapshot. When using a sync policy resync defaults to false . from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot ( staged_source , repository , source_config , snapshot_parameters ): if snapshot_parameter . resync : print ( snapshot_parameter . resync ) This class will be generated during build and is located with the autogenerated classes. As it is passed into the operation, importing it is not neccessary.","title":"SnapshotParametersDefinition"},{"location":"References/Classes/#fields_7","text":"Field Type Description resync Boolean Determines if this snapshot should ingest the dSource from scratch.","title":"Fields"},{"location":"References/Classes/#remoteenvironment","text":"Represents a remote environment. Warning Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.","title":"RemoteEnvironment"},{"location":"References/Classes/#fields_8","text":"Field Type Description name String Name of the environment. host RemoteHost Host that belongs to the environment. Internal virtualization platform object. reference String Unique identifier for the environment.","title":"Fields"},{"location":"References/Classes/#remotehost","text":"Represents a remote host, can we Unix or Windows. Warning Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.","title":"RemoteHost"},{"location":"References/Classes/#fields_9","text":"Field Type Description name String Host address. binary_path String Path to Delphix provided binaries on the host, which are present in the toolkit pushed to the remote host like dlpx_db_exec , dlpx_pfexec , etc. This property is only available for Unix hosts. reference String Unique identifier for the host.","title":"Fields"},{"location":"References/Classes/#remoteuser","text":"Represents a user on a remote host. Warning Objects of this class are instantiated by the platform. They should always be treated as read-only by the plugin, and their properties should only be accessed, but never modified.","title":"RemoteUser"},{"location":"References/Classes/#fields_10","text":"Field Type Description name String User name. reference String Unique identifier for the user.","title":"Fields"},{"location":"References/Decorators/","text":"Decorators \u00b6 The Virtualization SDK exposes decorators to be able to annotate functions that correspond to each Plugin Operation . In the example below, it first instantiates a Plugin() object, that can then be used to tag plugin operations. from dlpx.virtualization.platform import Plugin # Initialize a plugin object plugin = Plugin () # Use the decorator to annotate the function that corresponds to the \"Virtual Source Start\" Plugin Operation @plugin.virtual_source.start () def my_start ( virtual_source , repository , source_config ): print \"running start\" Info Decorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses () appended at the end. Assuming the name of the object, is plugin as above, the table below lists the corresponding decorators for each plugin operation. Plugin Operation Decorator Repository Discovey @plugin.discovery.repository() Source Config Discovey @plugin.discovery.source_config() Direct Linked Source Pre-Snapshot @plugin.linked.pre_snapshot() Direct Linked Source Post-Snapshot @plugin.linked.post_snapshot() Staged Linked Source Pre-Snapshot @plugin.linked.pre_snapshot() Staged Linked Source Post-Snapshot @plugin.linked.post_snapshot() Staged Linked Source Start-Staging @plugin.linked.start_staging() Staged Linked Source Stop-Staging @plugin.linked.stop_staging() Staged Linked Source Status @plugin.linked.status() Staged Linked Source Worker @plugin.linked.worker() Staged Linked Source Mount Specification @plugin.linked.mount_specification() Virtual Source Configure @plugin.virtual.configure() Virtual Source Unconfigure @plugin.virtual.unconfigure() Virtual Source Reconfigure @plugin.virtual.reconfigure() Virtual Source Start @plugin.virtual.start() Virtual Source Stop @plugin.virtual.stop() VirtualSource Pre-Snapshot @plugin.virtual.pre_snapshot() Virtual Source Post-Snapshot @plugin.virtual.post_snapshot() Virtual Source Mount Specification @plugin.virtual.mount_specification() Virtual Source Status @plugin.virtual.status() Warning A plugin should only implement the direct operations or the staged operations based on the plugin type","title":"Virtualization SDK"},{"location":"References/Decorators/#decorators","text":"The Virtualization SDK exposes decorators to be able to annotate functions that correspond to each Plugin Operation . In the example below, it first instantiates a Plugin() object, that can then be used to tag plugin operations. from dlpx.virtualization.platform import Plugin # Initialize a plugin object plugin = Plugin () # Use the decorator to annotate the function that corresponds to the \"Virtual Source Start\" Plugin Operation @plugin.virtual_source.start () def my_start ( virtual_source , repository , source_config ): print \"running start\" Info Decorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses () appended at the end. Assuming the name of the object, is plugin as above, the table below lists the corresponding decorators for each plugin operation. Plugin Operation Decorator Repository Discovey @plugin.discovery.repository() Source Config Discovey @plugin.discovery.source_config() Direct Linked Source Pre-Snapshot @plugin.linked.pre_snapshot() Direct Linked Source Post-Snapshot @plugin.linked.post_snapshot() Staged Linked Source Pre-Snapshot @plugin.linked.pre_snapshot() Staged Linked Source Post-Snapshot @plugin.linked.post_snapshot() Staged Linked Source Start-Staging @plugin.linked.start_staging() Staged Linked Source Stop-Staging @plugin.linked.stop_staging() Staged Linked Source Status @plugin.linked.status() Staged Linked Source Worker @plugin.linked.worker() Staged Linked Source Mount Specification @plugin.linked.mount_specification() Virtual Source Configure @plugin.virtual.configure() Virtual Source Unconfigure @plugin.virtual.unconfigure() Virtual Source Reconfigure @plugin.virtual.reconfigure() Virtual Source Start @plugin.virtual.start() Virtual Source Stop @plugin.virtual.stop() VirtualSource Pre-Snapshot @plugin.virtual.pre_snapshot() Virtual Source Post-Snapshot @plugin.virtual.post_snapshot() Virtual Source Mount Specification @plugin.virtual.mount_specification() Virtual Source Status @plugin.virtual.status() Warning A plugin should only implement the direct operations or the staged operations based on the plugin type","title":"Decorators"},{"location":"References/Glossary/","text":"Glossary \u00b6 Artifact \u00b6 A single file that is the result of a build . It is this artifact which is distributed to users, and which is installed onto engines. Automatic Discovery \u00b6 Discovery which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information. Building \u00b6 The process of creating an artifact from the collection of files that make up the plugin's source code. Decorator \u00b6 A Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation. Direct Linking \u00b6 A strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment. Discovery \u00b6 The process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets. dSource \u00b6 See Linked Dataset Environment \u00b6 A remote system that the Delphix Engine can interact with. An environment can be used as a source , staging or target environment (or any combination of those). For example, a Linux machine that the Delphix Engine can connect to is an environment. Linked Dataset \u00b6 A dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a dSource . Linked Source \u00b6 An object on the Delphix Engine that holds information related to a linked dataset . Linking \u00b6 The process by which the Delphix Engine connects a new dSource to a pre-existing dataset on a source environment. Logging \u00b6 Logging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin. Plugin Config \u00b6 A YAML file containing a list of plugin properties: What is the plugin's name? What version of the plugin is this? Etc. More details here . Manual Discovery \u00b6 Discovery which the end user does by manually entering the necessary information into the Delphix Engine. Mount Specification \u00b6 A collection of information, provided by the plugin, which give all the details about how and where virtual datasets should be mounted onto target environments . This term is often shortened to \"Mount Spec\". Password Properties \u00b6 In schemas , any string property can be tagged with \"format\": \"password\" . This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen. Platform Libraries \u00b6 A set of Python functions that are provided by the Virtualization Platform. Plugins use these library functions to request that the Virtualization Platform do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry. Plugin \u00b6 A tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset. Plugin Operation \u00b6 A piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function. For example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database. Provisioning \u00b6 The process of making a virtual copy of a dataset and making it available for use on a target environment. Repository \u00b6 Information that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS. Schema \u00b6 A formal description of a data type. Plugins use JSON format for their schemas . Snapshot \u00b6 Represents a snapshot of a dataset and its associated metadata represented by the SnapshotDefinition Schema Snaphot \u00b6 A point-in-time read-only copy of a dataset. Snapshot Parameter \u00b6 User provided parameters for the snapshot operation. Currently the only properties the parameter has is resync. Source Config \u00b6 A collection of information that the Delphix Engine needs to interact with a dataset (whether linked or virtual on an environment . Source Environment \u00b6 An environment containing data that is ingested by the Delphix Engine. Staged Linking \u00b6 A strategy where a staging environment is used to coordinate the ingestion of data into a dsource . Staging Environment \u00b6 An environment used by the Delphix Engine to coordinate ingestion from a source environment . Syncing \u00b6 The process by which the Delphix Engine ingests data from a dataset on a source environment into a dsource . Syncing always happens immediately after linking , and typically is done periodically thereafter. Target Environment \u00b6 An environment on which Delphix-provided virtualized datasets can be used. Upgrade Operation \u00b6 A special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin. VDB \u00b6 See Virtual Dataset Version \u00b6 A string identifier that is unique for every public release of a plugin. Virtual Dataset \u00b6 A dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a target environment . A virtual dataset is often called a \"VDB\". Virtual Source \u00b6 An object on the Delphix Engine that holds information related to a virtual dataset . YAML \u00b6 YAML is a simple language often used for configuration files. Plugins define their plugin config using YAML.","title":"Glossary"},{"location":"References/Glossary/#glossary","text":"","title":"Glossary"},{"location":"References/Glossary/#artifact","text":"A single file that is the result of a build . It is this artifact which is distributed to users, and which is installed onto engines.","title":"Artifact"},{"location":"References/Glossary/#automatic-discovery","text":"Discovery which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information.","title":"Automatic Discovery"},{"location":"References/Glossary/#building","text":"The process of creating an artifact from the collection of files that make up the plugin's source code.","title":"Building"},{"location":"References/Glossary/#decorator","text":"A Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation.","title":"Decorator"},{"location":"References/Glossary/#direct-linking","text":"A strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment.","title":"Direct Linking"},{"location":"References/Glossary/#discovery","text":"The process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets.","title":"Discovery"},{"location":"References/Glossary/#dsource","text":"See Linked Dataset","title":"dSource"},{"location":"References/Glossary/#environment","text":"A remote system that the Delphix Engine can interact with. An environment can be used as a source , staging or target environment (or any combination of those). For example, a Linux machine that the Delphix Engine can connect to is an environment.","title":"Environment"},{"location":"References/Glossary/#linked-dataset","text":"A dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a dSource .","title":"Linked Dataset"},{"location":"References/Glossary/#linked-source","text":"An object on the Delphix Engine that holds information related to a linked dataset .","title":"Linked Source"},{"location":"References/Glossary/#linking","text":"The process by which the Delphix Engine connects a new dSource to a pre-existing dataset on a source environment.","title":"Linking"},{"location":"References/Glossary/#logging","text":"Logging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin.","title":"Logging"},{"location":"References/Glossary/#plugin-config","text":"A YAML file containing a list of plugin properties: What is the plugin's name? What version of the plugin is this? Etc. More details here .","title":"Plugin Config"},{"location":"References/Glossary/#manual-discovery","text":"Discovery which the end user does by manually entering the necessary information into the Delphix Engine.","title":"Manual Discovery"},{"location":"References/Glossary/#mount-specification","text":"A collection of information, provided by the plugin, which give all the details about how and where virtual datasets should be mounted onto target environments . This term is often shortened to \"Mount Spec\".","title":"Mount Specification"},{"location":"References/Glossary/#password-properties","text":"In schemas , any string property can be tagged with \"format\": \"password\" . This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen.","title":"Password Properties"},{"location":"References/Glossary/#platform-libraries","text":"A set of Python functions that are provided by the Virtualization Platform. Plugins use these library functions to request that the Virtualization Platform do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry.","title":"Platform Libraries"},{"location":"References/Glossary/#plugin","text":"A tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset.","title":"Plugin"},{"location":"References/Glossary/#plugin-operation","text":"A piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function. For example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database.","title":"Plugin Operation"},{"location":"References/Glossary/#provisioning","text":"The process of making a virtual copy of a dataset and making it available for use on a target environment.","title":"Provisioning"},{"location":"References/Glossary/#repository","text":"Information that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS.","title":"Repository"},{"location":"References/Glossary/#schema","text":"A formal description of a data type. Plugins use JSON format for their schemas .","title":"Schema"},{"location":"References/Glossary/#snapshot","text":"Represents a snapshot of a dataset and its associated metadata represented by the SnapshotDefinition Schema","title":"Snapshot"},{"location":"References/Glossary/#snaphot","text":"A point-in-time read-only copy of a dataset.","title":"Snaphot"},{"location":"References/Glossary/#snapshot-parameter","text":"User provided parameters for the snapshot operation. Currently the only properties the parameter has is resync.","title":"Snapshot Parameter"},{"location":"References/Glossary/#source-config","text":"A collection of information that the Delphix Engine needs to interact with a dataset (whether linked or virtual on an environment .","title":"Source Config"},{"location":"References/Glossary/#source-environment","text":"An environment containing data that is ingested by the Delphix Engine.","title":"Source Environment"},{"location":"References/Glossary/#staged-linking","text":"A strategy where a staging environment is used to coordinate the ingestion of data into a dsource .","title":"Staged Linking"},{"location":"References/Glossary/#staging-environment","text":"An environment used by the Delphix Engine to coordinate ingestion from a source environment .","title":"Staging Environment"},{"location":"References/Glossary/#syncing","text":"The process by which the Delphix Engine ingests data from a dataset on a source environment into a dsource . Syncing always happens immediately after linking , and typically is done periodically thereafter.","title":"Syncing"},{"location":"References/Glossary/#target-environment","text":"An environment on which Delphix-provided virtualized datasets can be used.","title":"Target Environment"},{"location":"References/Glossary/#upgrade-operation","text":"A special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin.","title":"Upgrade Operation"},{"location":"References/Glossary/#vdb","text":"See Virtual Dataset","title":"VDB"},{"location":"References/Glossary/#version","text":"A string identifier that is unique for every public release of a plugin.","title":"Version"},{"location":"References/Glossary/#virtual-dataset","text":"A dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a target environment . A virtual dataset is often called a \"VDB\".","title":"Virtual Dataset"},{"location":"References/Glossary/#virtual-source","text":"An object on the Delphix Engine that holds information related to a virtual dataset .","title":"Virtual Source"},{"location":"References/Glossary/#yaml","text":"YAML is a simple language often used for configuration files. Plugins define their plugin config using YAML.","title":"YAML"},{"location":"References/Logging/","text":"Logging \u00b6 What is logging? \u00b6 The Virtualization Platform keeps plugin-specific log files. A plugin can, at any point in any of its plugin operations , write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin. Overview \u00b6 The Virtualization Platform integrates with Python's built-in logging framework . A special Handler is exposed by the platform at dlpx.virtualization.libs.PlatformHandler . This handler needs to be added to the Python logger your plugin creates. Logging statements made through Python's logging framework will then be routed to the platform. Basic Setup \u00b6 Below is the absolute minimum needed to setup logging for the platform. Please refer to Python's logging documentation and the example below to better understand how it can be customized. import logging from dlpx.virtualization.libs import PlatformHandler # Get the root logger. logger = logging . getLogger () logger . addHandler ( PlatformHandler ()) # The root logger's default level is logging.WARNING. # Without the line below, logging statements of levels # lower than logging.WARNING will be suppressed. logger . setLevel ( logging . DEBUG ) Logging Setup Python's logging framework is global. Setup only needs to happen once, but where it happens is important. Any logging statements that occur before the PlatformHandler is added will not be logged by the platform. It is highly recommended that the logging setup is done in the plugin's entry point module before any operations are ran. Add the PlatformHandler to the root logger Loggers in Python have a hierarchy and all loggers are children of a special logger called the \"root logger\". Logging hierarchy is not always intuitive and depends on how modules are structured. To avoid this complexity, add the PlatformHandler to the root logger. The root logger can be retrieved with logging.getLogger() . Usage \u00b6 Once the PlatformHandler has been added to the logger, logging is done with Python's Logger object. Below is a simple example including the basic setup code used above: import logging from dlpx.virtualization.libs import PlatformHandler logger = logging . getLogger () logger . addHandler ( PlatformHandler ()) # The root logger's default level is logging.WARNING. # Without the line below, logging statements of levels # lower than logging.WARNING will be suppressed. logger . setLevel ( logging . DEBUG ) logger . debug ( 'debug' ) logger . info ( 'info' ) logger . error ( 'error' ) Example \u00b6 Imagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why. Info Refer to Managing Scripts for Remote Execution for how remote scripts can be stored and retrieved. Suppose your plugin has a source config discovery operation that looks like this (code is abbreviated to be easier to follow): import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.discovery.repository () def repository_discovery ( source_connection ): return [ RepositoryDefinition ( 'Logging Example' )] @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): version_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_version.sh' )) users_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_users.sh' )) db_results = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_databases.sh' )) status_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_database_statuses.sh' )) # Return an empty list for simplicity. In reality # something would be done with the results above. return [] Now, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this: import logging import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin from generated.definitions import RepositoryDefinition # This should probably be defined in its own module outside # of the plugin's entry point file. It is here for simplicity. def _setup_logger (): # This will log the time, level, filename, line number, and log message. log_message_format = '[ %(asctime)s ] [ %(levelname)s ] [ %(filename)s : %(lineno)d ] %(message)s ' log_message_date_format = '%Y-%m- %d %H:%M:%S' # Create a custom formatter. This will help with diagnosability. formatter = logging . Formatter ( log_message_format , datefmt = log_message_date_format ) platform_handler = libs . PlatformHandler () platform_handler . setFormatter ( formatter ) logger = logging . getLogger () logger . addHandler ( platform_handler ) # By default the root logger's level is logging.WARNING. logger . setLevel ( logging . DEBUG ) # Setup the logger. _setup_logger () # logging.getLogger(__name__) is the convention way to get a logger in Python. # It returns a new logger per module and will be a child of the root logger. # Since we setup the root logger, nothing else needs to be done to set this # one up. logger = logging . getLogger ( __name__ ) plugin = Plugin () @plugin.discovery.repository () def repository_discovery ( source_connection ): return [ RepositoryDefinition ( 'Logging Example' )] @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): logger . debug ( 'About to get DB version' ) version_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_version.sh' )) logger . debug ( 'About to get DB users' ) users_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_users.sh' )) logger . debug ( 'About to get databases' ) db_results = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_databases.sh' )) logger . debug ( 'About to get DB statuses' ) status_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_database_statuses.sh' )) logger . debug ( 'Done collecting data' ) # Return an empty list for simplicity. In reality # something would be done with the results above. return [] When you look at the log file, perhaps you'll see something like this: [Worker-360|JOB-315|ENVIRONMENT_DISCOVER(UNIX_HOST_ENVIRONMENT-5)] [2019-04-30 12:10:42] [DEBUG] [python_runner.py:44] About to get DB version [Worker-360|JOB-316|DB_SYNC(APPDATA_CONTAINER-21)] [2019-04-30 12:19:35] [DEBUG] [python_runner.py:49] About to get DB users [Worker-325|JOB-280|ENVIRONMENT_REFRESH(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:51] About to get databases [Worker-326|JOB-281|SOURCES_DISABLE(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:53] About to get DB statuses You can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes! We now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem. How to retrieve logs \u00b6 Download a support bundle by going to Help > Support Logs and select Download . The logs will be in a the support bundle under log/mgmt_log/plugin_log/<plugin name> . Logging Levels \u00b6 Python has a number of preset logging levels and allows for custom ones as well. Since logging on the Virtualization Platform uses the logging framework, log statements of all levels are supported. However, the Virtualization Platform will map all logging levels into three files: debug.log , info.log , and error.log in the following way: Python Logging Level Logging File DEBUG debug.log INFO info.log WARN error.log WARNING error.log ERROR error.log CRITICAL error.log As is the case with the logging framework, logging statements are hierarchical: logging statements made at the logging.DEBUG level will be written only to debug.log while logging statements made at the logging.ERROR level will be written to debug.log , info.log , and error.log . Sensitive data \u00b6 Remember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on sensitive data","title":"Logging"},{"location":"References/Logging/#logging","text":"","title":"Logging"},{"location":"References/Logging/#what-is-logging","text":"The Virtualization Platform keeps plugin-specific log files. A plugin can, at any point in any of its plugin operations , write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin.","title":"What is logging?"},{"location":"References/Logging/#overview","text":"The Virtualization Platform integrates with Python's built-in logging framework . A special Handler is exposed by the platform at dlpx.virtualization.libs.PlatformHandler . This handler needs to be added to the Python logger your plugin creates. Logging statements made through Python's logging framework will then be routed to the platform.","title":"Overview"},{"location":"References/Logging/#basic-setup","text":"Below is the absolute minimum needed to setup logging for the platform. Please refer to Python's logging documentation and the example below to better understand how it can be customized. import logging from dlpx.virtualization.libs import PlatformHandler # Get the root logger. logger = logging . getLogger () logger . addHandler ( PlatformHandler ()) # The root logger's default level is logging.WARNING. # Without the line below, logging statements of levels # lower than logging.WARNING will be suppressed. logger . setLevel ( logging . DEBUG ) Logging Setup Python's logging framework is global. Setup only needs to happen once, but where it happens is important. Any logging statements that occur before the PlatformHandler is added will not be logged by the platform. It is highly recommended that the logging setup is done in the plugin's entry point module before any operations are ran. Add the PlatformHandler to the root logger Loggers in Python have a hierarchy and all loggers are children of a special logger called the \"root logger\". Logging hierarchy is not always intuitive and depends on how modules are structured. To avoid this complexity, add the PlatformHandler to the root logger. The root logger can be retrieved with logging.getLogger() .","title":"Basic Setup"},{"location":"References/Logging/#usage","text":"Once the PlatformHandler has been added to the logger, logging is done with Python's Logger object. Below is a simple example including the basic setup code used above: import logging from dlpx.virtualization.libs import PlatformHandler logger = logging . getLogger () logger . addHandler ( PlatformHandler ()) # The root logger's default level is logging.WARNING. # Without the line below, logging statements of levels # lower than logging.WARNING will be suppressed. logger . setLevel ( logging . DEBUG ) logger . debug ( 'debug' ) logger . info ( 'info' ) logger . error ( 'error' )","title":"Usage"},{"location":"References/Logging/#example","text":"Imagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why. Info Refer to Managing Scripts for Remote Execution for how remote scripts can be stored and retrieved. Suppose your plugin has a source config discovery operation that looks like this (code is abbreviated to be easier to follow): import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.discovery.repository () def repository_discovery ( source_connection ): return [ RepositoryDefinition ( 'Logging Example' )] @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): version_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_version.sh' )) users_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_users.sh' )) db_results = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_databases.sh' )) status_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_database_statuses.sh' )) # Return an empty list for simplicity. In reality # something would be done with the results above. return [] Now, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this: import logging import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin from generated.definitions import RepositoryDefinition # This should probably be defined in its own module outside # of the plugin's entry point file. It is here for simplicity. def _setup_logger (): # This will log the time, level, filename, line number, and log message. log_message_format = '[ %(asctime)s ] [ %(levelname)s ] [ %(filename)s : %(lineno)d ] %(message)s ' log_message_date_format = '%Y-%m- %d %H:%M:%S' # Create a custom formatter. This will help with diagnosability. formatter = logging . Formatter ( log_message_format , datefmt = log_message_date_format ) platform_handler = libs . PlatformHandler () platform_handler . setFormatter ( formatter ) logger = logging . getLogger () logger . addHandler ( platform_handler ) # By default the root logger's level is logging.WARNING. logger . setLevel ( logging . DEBUG ) # Setup the logger. _setup_logger () # logging.getLogger(__name__) is the convention way to get a logger in Python. # It returns a new logger per module and will be a child of the root logger. # Since we setup the root logger, nothing else needs to be done to set this # one up. logger = logging . getLogger ( __name__ ) plugin = Plugin () @plugin.discovery.repository () def repository_discovery ( source_connection ): return [ RepositoryDefinition ( 'Logging Example' )] @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): logger . debug ( 'About to get DB version' ) version_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_version.sh' )) logger . debug ( 'About to get DB users' ) users_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_users.sh' )) logger . debug ( 'About to get databases' ) db_results = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_databases.sh' )) logger . debug ( 'About to get DB statuses' ) status_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_database_statuses.sh' )) logger . debug ( 'Done collecting data' ) # Return an empty list for simplicity. In reality # something would be done with the results above. return [] When you look at the log file, perhaps you'll see something like this: [Worker-360|JOB-315|ENVIRONMENT_DISCOVER(UNIX_HOST_ENVIRONMENT-5)] [2019-04-30 12:10:42] [DEBUG] [python_runner.py:44] About to get DB version [Worker-360|JOB-316|DB_SYNC(APPDATA_CONTAINER-21)] [2019-04-30 12:19:35] [DEBUG] [python_runner.py:49] About to get DB users [Worker-325|JOB-280|ENVIRONMENT_REFRESH(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:51] About to get databases [Worker-326|JOB-281|SOURCES_DISABLE(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:53] About to get DB statuses You can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes! We now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem.","title":"Example"},{"location":"References/Logging/#how-to-retrieve-logs","text":"Download a support bundle by going to Help > Support Logs and select Download . The logs will be in a the support bundle under log/mgmt_log/plugin_log/<plugin name> .","title":"How to retrieve logs"},{"location":"References/Logging/#logging-levels","text":"Python has a number of preset logging levels and allows for custom ones as well. Since logging on the Virtualization Platform uses the logging framework, log statements of all levels are supported. However, the Virtualization Platform will map all logging levels into three files: debug.log , info.log , and error.log in the following way: Python Logging Level Logging File DEBUG debug.log INFO info.log WARN error.log WARNING error.log ERROR error.log CRITICAL error.log As is the case with the logging framework, logging statements are hierarchical: logging statements made at the logging.DEBUG level will be written only to debug.log while logging statements made at the logging.ERROR level will be written to debug.log , info.log , and error.log .","title":"Logging Levels"},{"location":"References/Logging/#sensitive-data","text":"Remember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on sensitive data","title":"Sensitive data"},{"location":"References/Platform_Libraries/","text":"Platform Libraries \u00b6 Set of functions that plugins can use these for executing remote commands, etc. run_bash \u00b6 Executes a bash command on a remote Unix host. Signature \u00b6 def run_bash(remote_connection, command, variables=None, use_login_shell=False, check=False) Arguments \u00b6 Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Command to run on the host. variables dict[String, String] Optional . Environement variables to set when running the command. use_login_shell boolean Optional . Whether to use a login shell. check boolean Optional . Whether or not to raise an exception if the exit_code in the RunBashResponse is non-zero. Returns \u00b6 An object of RunBashResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command. Examples \u00b6 Calling bash with an inline command. from dlpx.virtualization import libs command = \"echo 'Hi' >> /tmp/debug.log\" vars = { \"var\" : \"val\" } response = libs . run_bash ( connection , command , vars ) print response . exit_code print response . stdout print response . stderr Using parameters to construct a bash command. from dlpx.virtualization import libs name = virtual_source . parameters . username port = virtual_source . parameters . port command = \"mysqldump -u {} -p {}\" . format ( name , port ) response = libs . run_bash ( connection , command ) Running a bash script that is saved in a directory. import pkgutil from dlpx.virtualization import libs script_content = pkgutil . get_data ( 'resources' , 'get_date.sh' ) # Execute script on remote host response = libs . run_bash ( direct_source . connection , script_content ) For more information please go to Managing Scripts for Remote Execution section. run_expect \u00b6 Executes a tcl command or script on a remote Unix host. Signature \u00b6 def run_expect(remote_connection, command, variables=None) Arguments \u00b6 Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Expect(Tcl) command to run. variables dict[String, String] Optional . Environement variables to set when running the command. Returns \u00b6 An object of RunExpectResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command. Example \u00b6 Calling expect with an inline command. from dlpx.virtualization import libs command = \"puts 'Hi'\" vars = { \"var\" : \"val\" } repsonse = libs . run_expect ( connection , command , vars ) print response . exit_code print response . stdout print response . stderr run_powershell \u00b6 Executes a powershell command on a remote Windows host. Signature \u00b6 def run_powershell(remote_connection, command, variables=None, check=False) Arguments \u00b6 Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Command to run to the remote host. variables dict[String, String] Optional . Environement variables to set when running the command. check boolean Optional . Whether or not to raise an exception if the exit_code in the RunPowershellResponse is non-zero. Returns \u00b6 An object of RunPowershellResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command. Example \u00b6 Calling powershell with an inline command. from dlpx.virtualization import libs command = \"Write-Output 'Hi'\" vars = { \"var\" : \"val\" } response = libs . run_powershell ( connection , command , vars ) print response . exit_code print response . stdout print response . stderr run_sync \u00b6 Copies files from the remote source host directly into the dSource, without involving a staging host. Signature \u00b6 def run_sync(remote_connection, source_directory, rsync_user=None, exclude_paths=None, sym_links_to_follow=None) Arguments \u00b6 Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. source_directory String Directory of files to be synced. rsync_user String Optional User who has access to the directory to be synced. exclude_paths list[String] Optional Paths to be excluded. sym_links_to_follow list[String] Optional Symbollic links to follow if any. Returns \u00b6 None Example \u00b6 from dlpx.virtualization import libs source_directory = \"sourceDirectory\" rsync_user = \"rsyncUser\" exclude_paths = [ \"/path1\" , \"/path2\" ] sym_links_to_follow = [ \"/path3\" , \"/path4\" ] libs . run_sync ( connection , source_directory , rsync_user , exclude_paths , sym_links_to_follow )","title":"Virtualization SDK"},{"location":"References/Platform_Libraries/#platform-libraries","text":"Set of functions that plugins can use these for executing remote commands, etc.","title":"Platform Libraries"},{"location":"References/Platform_Libraries/#run_bash","text":"Executes a bash command on a remote Unix host.","title":"run_bash"},{"location":"References/Platform_Libraries/#signature","text":"def run_bash(remote_connection, command, variables=None, use_login_shell=False, check=False)","title":"Signature"},{"location":"References/Platform_Libraries/#arguments","text":"Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Command to run on the host. variables dict[String, String] Optional . Environement variables to set when running the command. use_login_shell boolean Optional . Whether to use a login shell. check boolean Optional . Whether or not to raise an exception if the exit_code in the RunBashResponse is non-zero.","title":"Arguments"},{"location":"References/Platform_Libraries/#returns","text":"An object of RunBashResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command.","title":"Returns"},{"location":"References/Platform_Libraries/#examples","text":"Calling bash with an inline command. from dlpx.virtualization import libs command = \"echo 'Hi' >> /tmp/debug.log\" vars = { \"var\" : \"val\" } response = libs . run_bash ( connection , command , vars ) print response . exit_code print response . stdout print response . stderr Using parameters to construct a bash command. from dlpx.virtualization import libs name = virtual_source . parameters . username port = virtual_source . parameters . port command = \"mysqldump -u {} -p {}\" . format ( name , port ) response = libs . run_bash ( connection , command ) Running a bash script that is saved in a directory. import pkgutil from dlpx.virtualization import libs script_content = pkgutil . get_data ( 'resources' , 'get_date.sh' ) # Execute script on remote host response = libs . run_bash ( direct_source . connection , script_content ) For more information please go to Managing Scripts for Remote Execution section.","title":"Examples"},{"location":"References/Platform_Libraries/#run_expect","text":"Executes a tcl command or script on a remote Unix host.","title":"run_expect"},{"location":"References/Platform_Libraries/#signature_1","text":"def run_expect(remote_connection, command, variables=None)","title":"Signature"},{"location":"References/Platform_Libraries/#arguments_1","text":"Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Expect(Tcl) command to run. variables dict[String, String] Optional . Environement variables to set when running the command.","title":"Arguments"},{"location":"References/Platform_Libraries/#returns_1","text":"An object of RunExpectResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command.","title":"Returns"},{"location":"References/Platform_Libraries/#example","text":"Calling expect with an inline command. from dlpx.virtualization import libs command = \"puts 'Hi'\" vars = { \"var\" : \"val\" } repsonse = libs . run_expect ( connection , command , vars ) print response . exit_code print response . stdout print response . stderr","title":"Example"},{"location":"References/Platform_Libraries/#run_powershell","text":"Executes a powershell command on a remote Windows host.","title":"run_powershell"},{"location":"References/Platform_Libraries/#signature_2","text":"def run_powershell(remote_connection, command, variables=None, check=False)","title":"Signature"},{"location":"References/Platform_Libraries/#arguments_2","text":"Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Command to run to the remote host. variables dict[String, String] Optional . Environement variables to set when running the command. check boolean Optional . Whether or not to raise an exception if the exit_code in the RunPowershellResponse is non-zero.","title":"Arguments"},{"location":"References/Platform_Libraries/#returns_2","text":"An object of RunPowershellResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command.","title":"Returns"},{"location":"References/Platform_Libraries/#example_1","text":"Calling powershell with an inline command. from dlpx.virtualization import libs command = \"Write-Output 'Hi'\" vars = { \"var\" : \"val\" } response = libs . run_powershell ( connection , command , vars ) print response . exit_code print response . stdout print response . stderr","title":"Example"},{"location":"References/Platform_Libraries/#run_sync","text":"Copies files from the remote source host directly into the dSource, without involving a staging host.","title":"run_sync"},{"location":"References/Platform_Libraries/#signature_3","text":"def run_sync(remote_connection, source_directory, rsync_user=None, exclude_paths=None, sym_links_to_follow=None)","title":"Signature"},{"location":"References/Platform_Libraries/#arguments_3","text":"Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. source_directory String Directory of files to be synced. rsync_user String Optional User who has access to the directory to be synced. exclude_paths list[String] Optional Paths to be excluded. sym_links_to_follow list[String] Optional Symbollic links to follow if any.","title":"Arguments"},{"location":"References/Platform_Libraries/#returns_3","text":"None","title":"Returns"},{"location":"References/Platform_Libraries/#example_2","text":"from dlpx.virtualization import libs source_directory = \"sourceDirectory\" rsync_user = \"rsyncUser\" exclude_paths = [ \"/path1\" , \"/path2\" ] sym_links_to_follow = [ \"/path3\" , \"/path4\" ] libs . run_sync ( connection , source_directory , rsync_user , exclude_paths , sym_links_to_follow )","title":"Example"},{"location":"References/Plugin_Config/","text":"Plugin Config \u00b6 The plugin config is a YAML file that marks the root of a plugin and defines metadata about the plugin and its structure. The config file is read at build time to generate the upload artifact. The name of the file does not matter and can be specified during the build, but, by default, the build looks for plugin_config.yml . Fields \u00b6 Field Name Required Type Description id Y string The unique id of the plugin. This will be autogenerated by dvp init . name N string The display name of the plugin. This will be used in the UI. If it is not specified name will be equal to id. version Y string The plugin's version in the format x.y.z . hostTypes Y list The host type that the plugin supports. Either UNIX or WINDOWS . schemaFile Y string The path to the JSON file that contains the plugin's schema definitions . This path can be absolute or relative to the directory containing the plugin config file. srcDir Y string The path to the directory that contains the source code for the plugin. During execution of a plugin operation, this directory will be the current working directory of the Python interpreter. Any modules or resources defined outside of this directory will be inaccessible at runtime. This path can be absolute or relative to the directory containing the plugin config file. entryPoint Y string A fully qualified Python symbol that points to the dlpx.virtualization.platform.Plugin object that defines the plugin. It must be in the form importable.module:object_name where importable.module is in srcDir . manualDiscovery N boolean True if the plugin supports manual discovery of source config objects. The default value is true . pluginType Y enum The ingestion strategy of the plugin. Can be either STAGED or DIRECT . language Y enum Must be PYTHON27 . defaultLocale N enum The locale to be used by the plugin if the Delphix user does not specify one. Plugin messages will be displayed in this locale by default. The default value is en-us . rootSquashEnabled N boolean This dictates whether \"root squash\" is enabled on NFS mounts for the plugin (i.e. whether the root user on remote hosts has access to the NFS mounts). Setting this to false allows processes usually run as root , like Docker daemons, access to the NFS mounts. The default value is true . This field only applies to Unix hosts. Example \u00b6 Assume the following basic plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u2514\u2500\u2500 mongo_runner.py mongo_runner.py contains: from dlpx.virtualization.platform import Plugin mongodb = Plugin () This is a valid plugin config for the plugin: id : 7cf830f2-82f3-4d5d-a63c-7bbe50c22b32 name : MongoDB version : 2.0.0 hostTypes : - UNIX entryPoint : mongo_runner:mongodb srcDir : src/ schemaFile : schema.json pluginType : DIRECT language : PYTHON27 This is a valid plugin config for the plugin with manualDiscovery set to false: id : 7cf830f2-82f3-4d5d-a63c-7bbe50c22b32 name : MongoDB version : 2.0.0 hostTypes : - UNIX entryPoint : mongo_runner:mongodb srcDir : src/ schemaFile : schema.json manualDiscovery : false pluginType : DIRECT language : PYTHON27","title":"Plugin Config"},{"location":"References/Plugin_Config/#plugin-config","text":"The plugin config is a YAML file that marks the root of a plugin and defines metadata about the plugin and its structure. The config file is read at build time to generate the upload artifact. The name of the file does not matter and can be specified during the build, but, by default, the build looks for plugin_config.yml .","title":"Plugin Config"},{"location":"References/Plugin_Config/#fields","text":"Field Name Required Type Description id Y string The unique id of the plugin. This will be autogenerated by dvp init . name N string The display name of the plugin. This will be used in the UI. If it is not specified name will be equal to id. version Y string The plugin's version in the format x.y.z . hostTypes Y list The host type that the plugin supports. Either UNIX or WINDOWS . schemaFile Y string The path to the JSON file that contains the plugin's schema definitions . This path can be absolute or relative to the directory containing the plugin config file. srcDir Y string The path to the directory that contains the source code for the plugin. During execution of a plugin operation, this directory will be the current working directory of the Python interpreter. Any modules or resources defined outside of this directory will be inaccessible at runtime. This path can be absolute or relative to the directory containing the plugin config file. entryPoint Y string A fully qualified Python symbol that points to the dlpx.virtualization.platform.Plugin object that defines the plugin. It must be in the form importable.module:object_name where importable.module is in srcDir . manualDiscovery N boolean True if the plugin supports manual discovery of source config objects. The default value is true . pluginType Y enum The ingestion strategy of the plugin. Can be either STAGED or DIRECT . language Y enum Must be PYTHON27 . defaultLocale N enum The locale to be used by the plugin if the Delphix user does not specify one. Plugin messages will be displayed in this locale by default. The default value is en-us . rootSquashEnabled N boolean This dictates whether \"root squash\" is enabled on NFS mounts for the plugin (i.e. whether the root user on remote hosts has access to the NFS mounts). Setting this to false allows processes usually run as root , like Docker daemons, access to the NFS mounts. The default value is true . This field only applies to Unix hosts.","title":"Fields"},{"location":"References/Plugin_Config/#example","text":"Assume the following basic plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u2514\u2500\u2500 mongo_runner.py mongo_runner.py contains: from dlpx.virtualization.platform import Plugin mongodb = Plugin () This is a valid plugin config for the plugin: id : 7cf830f2-82f3-4d5d-a63c-7bbe50c22b32 name : MongoDB version : 2.0.0 hostTypes : - UNIX entryPoint : mongo_runner:mongodb srcDir : src/ schemaFile : schema.json pluginType : DIRECT language : PYTHON27 This is a valid plugin config for the plugin with manualDiscovery set to false: id : 7cf830f2-82f3-4d5d-a63c-7bbe50c22b32 name : MongoDB version : 2.0.0 hostTypes : - UNIX entryPoint : mongo_runner:mongodb srcDir : src/ schemaFile : schema.json manualDiscovery : false pluginType : DIRECT language : PYTHON27","title":"Example"},{"location":"References/Plugin_Operations/","text":"Plugin Operations \u00b6 Warning If a Plugin Operations is Required and is not present, the corresponding Delphix Engine Operation will fail when invoked. The plugin can still be built and uploaded to the Delphix Engine. Warning For each operation, the argument names must match exactly. For example, the Repository Discovery operation must have a single argument named source_connection . Plugin Operation Required Decorator Delphix Engine Operations Repository Discovery Yes discovery.repository() Environment Discovery Environment Refresh Source Config Discovery Yes discovery.source_config() Environment Discovery Environment Refresh Direct Linked Source Pre-Snapshot No linked.pre_snapshot() Linked Source Sync Direct Linked Source Post-Snapshot Yes linked.post_snapshot() Linked Source Sync Staged Linked Source Pre-Snapshot No linked.pre_snapshot() Linked Source Sync Staged Linked Source Post-Snapshot Yes linked.post_snapshot() Linked Source Sync Staged Linked Source Start-Staging No linked.start_staging() Linked Source Enable Staged Linked Source Stop-Staging No linked.stop_staging() Linked Source Disable Linked Source Delete Staged Linked Source Status No linked.status() N/A Staged Linked Source Worker No linked.worker() N/A Staged Linked Source Mount Specification Yes linked.mount_specification() Linked Source Sync Linked Source Enable Virtual Source Configure Yes virtual.configure() Virtual Source Provision Virtual Source Refresh Virtual Source Unconfigure No virtual.unconfigure() Virtual Source Refresh Virtual Source Delete Virtual Source Reconfigure Yes virtual.reconfigure() Virtual Source Rollback Virtual Source Enable Virtual Source Start No virtual.start() Virtual Source Start Virtual Source Stop No virtual.stop() Virtual Source Stop Virtual Source Pre-Snapshot No virtual.pre_snapshot() Virtual Source Snapshot Virtual Source Post-Snapshot Yes virtual.post_snapshot() Virtual Source Snapshot Virtual Source Mount Specification Yes virtual.mount_specification() Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start Virtual Source Status No virtual.status() Virtual Source Enable Repository Discovery \u00b6 Discovers the set of repositories for a plugin on an environment . For a DBMS, this can correspond to the set of binaries installed on a Unix host. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Environment Refresh Environment Discovery Signature \u00b6 def repository_discovery(source_connection) Decorator \u00b6 discovery.repository() Arguments \u00b6 Argument Type Description source_connection RemoteConnection The connection associated with the remote environment to run repository discovery Returns \u00b6 A list of RepositoryDefinition objects. Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.defintions import RepositoryDefinition plugin = Plugin () @plugin.discovery.repository () def repository_discovery ( source_connection ): repository = RepositoryDefinition () repository . installPath = \"/usr/bin/install\" repository . version = \"1.2.3\" return [ repository ] The above command assumes a Repository Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"installPath\" : { \"type\" : \"string\" }, \"version\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"installPath\" ], \"nameField\" : [ \"installPath\" ] } Source Config Discovery \u00b6 Discovers the set of source configs for a plugin for a repository . For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Environment Refresh Environment Discovery Signature \u00b6 def source_config_discovery(source_connection, repository) Decorator \u00b6 discovery.source_config() Arguments \u00b6 Argument Type Description source_connection RemoteConnection The connection to the remote environment the corresponds to the repository. repository RepositoryDefinition The repository to discover source configs for. Returns \u00b6 A list of SourceConfigDefinition objects. Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): source_config = SourceConfigDefinition () source_config . name = \"my_name\" source_config . port = 10000 return [ source_config ] The above command assumes a Source Config Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"number\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] } Direct Linked Source Pre-Snapshot \u00b6 Sets up a dSource to ingest data. Only applies when using a Direct Linking strategy. Required / Optional \u00b6 Optional Delphix Engine Operations \u00b6 Linked Source Sync Signature \u00b6 def linked_pre_snapshot(direct_source, repository, source_config) Decorator \u00b6 linked.pre_snapshot() Arguments \u00b6 Argument Type Description direct_source DirectSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot ( direct_source , repository , source_config ): pass Direct Linked Source Post-Snapshot \u00b6 Captures metadata from a dSource once data has been ingested. Only applies when using a Direct Linking strategy. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Linked Source Sync Signature \u00b6 def linked_post_snapshot(direct_source, repository, source_config) Decorator \u00b6 linked.post_snapshot() Arguments \u00b6 Argument Type Description direct_source DirectSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 SnapshotDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.linked.post_snapshot () def linked_post_snapshot ( direct_source , repository , source_config ): snapshot = SnapshotDefinition () snapshot . transaction_id = 1000 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"integer\" } } } Staged Linked Source Pre-Snapshot \u00b6 Sets up a dSource to ingest data. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Linked Source Sync Signature \u00b6 def linked_pre_snapshot(staged_source, repository, source_config, snapshot_parameters) Decorator \u00b6 linked.pre_snapshot() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. snapshot_parameters SnapshotParametersDefinition The snapshot parameters. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot ( staged_source , repository , source_config , snapshot_parameters ): pass Staged Linked Source Post-Snapshot \u00b6 Captures metadata from a dSource once data has been ingested. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Linked Source Sync Signature \u00b6 def linked_post_snapshot(staged_source, repository, source_config, snapshot_parameters) Decorator \u00b6 linked.post_snapshot() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. snapshot_parameters SnapshotParametersDefinition The snapshot parameters. Returns \u00b6 SnapshotDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.linked.post_snapshot () def linked_post_snapshot ( staged_source , repository , source_config , snapshot_parameters ): snapshot = SnapshotDefinition () if snapshot_parameters . resync : snapshot . transaction_id = 1000 else : snapshot . transaction_id = 10 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"integer\" } } } Staged Linked Source Start-Staging \u00b6 Sets up a Staging Source to ingest data. Only applies when using a Staged Linking strategy. Required to implement for Delphix Engine operations: Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Linked Source Enable Signature \u00b6 def start_staging(staged_source, repository, source_config) Decorator \u00b6 linked.start_staging() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.start_staging () def start_staging ( staged_source , repository , source_config ): pass Staged Linked Source Stop-Staging \u00b6 Quiesces a Staging Source to pause ingestion. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Linked Source Disable Linked Source Delete Signature \u00b6 def stop_staging(staged_source, repository, source_config) Decorator \u00b6 linked.stop_staging() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Examples \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.stop_staging () def stop_staging ( staged_source , repository , source_config ): pass Staged Linked Source Status \u00b6 Determines the status of a Staging Source to show end users whether it is healthy or not. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Optional. If not implemented, the platform assumes that the status is Status.ACTIVE Delphix Engine Operations \u00b6 N/A Signature \u00b6 def linked_status(staged_source, repository, source_config) Decorator \u00b6 linked.status() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 Status Status.ACTIVE if the plugin operation is not implemented. Example \u00b6 from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Status plugin = Plugin () @plugin.linked.status () def linked_status ( staged_source , repository , source_config ): return Status . ACTIVE Staged Linked Source Worker \u00b6 Monitors the status of a Staging Source on a reqular interval. It can be used to fix up any errors on staging if it is not functioning as expected. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 N/A Signature \u00b6 def worker(staged_source, repository, source_config) Decorator \u00b6 linked.worker() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.worker () def worker ( staged_source , repository , source_config ): pass Staged Linked Source Mount Specification \u00b6 Returns configurations for the mounts associated for data in staged source. The ownership_specification is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Linked Source Sync Linked Source Enable Signature \u00b6 def linked_mount_specification(staged_source, repository) Decorator \u00b6 linked.mount_specification() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. Returns \u00b6 MountSpecification Example \u00b6 Info ownership_specification only applies to Unix hosts. from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Mount from dlpx.virtualization.platform import MountSpecification from dlpx.virtualization.platform import OwenershipSpecification from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.linked.mount_specification () def linked_mount_specification ( staged_source , repository ): mount = Mount ( staged_source . staged_connection . environment , \"/some/path\" ) ownership_spec = OwenershipSpecification ( repository . uid , repository . gid ) return MountSpecification ([ mount ], ownership_spec ) The above command assumes a Repository Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"uid\" : { \"type\" : \"integer\" }, \"gid\" : { \"type\" : \"integer\" } } } Virtual Source Configure \u00b6 Configures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Virtual Source Provision Virtual Source Refresh Signature \u00b6 def configure(virtual_source, snapshot, repository) Decorator \u00b6 virtual.configure() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. snapshot SnapshotDefinition The snapshot of the data set to configure. repository RepositoryDefinition The repository associated with this source. Returns \u00b6 SourceConfigDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.defintions import SourceConfigDefinition plugin = Plugin () @plugin.virtual.configure () def configure ( virtual_source , repository , snapshot ): name = \"config_name\" source_config = SourceConfigDefinition () source_config . name = name return source_config The above command assumes a SourceConfig Schema defined as: { \"type\" : \"object\" , \"required\" : [ \"name\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] } Virtual Source Unconfigure \u00b6 Quiesces the virtual source on a target environment. For database data files, shutting down and unregistering a database on a host. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Virtual Source Refresh Virtual Source Delete Signature \u00b6 def unconfigure(virtual_source, repository, source_config) Decorator \u00b6 virtual.unconfigure() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.unconfigure () def unconfigure ( virtual_source , repository , source_config ): pass Virtual Source Reconfigure \u00b6 Re-configures the data for a virtual source to point to the data in a prior snapshot for the virtual source. For database data files, this may mean recovering from a crash consistent format or backup of a new snapshot. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Virtual Source Rollback Virtual Source Enable Signature \u00b6 def reconfigure(virtual_source, repository, source_config, snapshot) Decorator \u00b6 virtual.reconfigure() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. snapshot SnapshotDefinition The snapshot of the data set to configure. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 SourceConfigDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin.virtual.reconfigure () def configure ( virtual_source , repository , source_config , snapshot ): name = \"updated_config_name\" source_config = SourceConfigDefinition () source_config . name = name return source_config The above command assumes a SourceConfig Schema defined as: { \"type\" : \"object\" , \"required\" : [ \"name\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] } Virtual Source Start \u00b6 Executed whenever the data should be placed in a \"running\" state. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Virtual Source Start Signature \u00b6 def start(virtual_source, repository, source_config) Decorator \u00b6 virtual.start() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.start () def start ( virtual_source , repository , source_config ): pass Virtual Source Stop \u00b6 Executed whenever the data needs to be shut down. Required to implement for Delphix Engine operations: Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Virtual Source Stop Signature \u00b6 def stop(virtual_source, repository, source_config) Decorator \u00b6 virtual.stop() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.stop () def stop ( virtual_source , repository , source_config ): pass Virtual Source Pre-Snapshot \u00b6 Prepares the virtual source for taking a snapshot of the data. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Virtual Source Snapshot Signature \u00b6 def virtual_pre_snapshot(virtual_source, repository, source_config) Decorator \u00b6 virtual.pre_snapshot() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.pre_snapshot () def virtual_pre_snapshot ( virtual_source , repository , source_config ): pass Virtual Source Post-Snapshot \u00b6 Captures metadata after a snapshot. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Virtual Source Snapshot Signature \u00b6 def virtual_post_snapshot(virtual_source, repository, source_config) Decorator \u00b6 virtual.post_snapshot() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 SnapshotDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.defintions import SnapshotDefinition plugin = Plugin () @plugin.virtual.post_snapshot () def virtual_post_snapshot ( virtual_source , repository , source_config ): snapshot = SnapshotDefinition () snapshot . transaction_id = 1000 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"string\" } } } Virtual Source Mount Specification \u00b6 Returns configurations for the mounts associated for data in virtual source. The ownership_specification is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start Signature \u00b6 def virtual_mount_specification(virtual_source, repository) Decorator \u00b6 virtual.mount_specification() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. Returns \u00b6 MountSpecification Example \u00b6 Info ownership_specification only applies to Unix hosts. from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Mount from dlpx.virtualization.platform import MountSpecification from dlpx.virtualization.platform import OwenershipSpecification from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.virtual.mount_specification () def virtual_mount_specification ( virtual_source , repository ): mount = Mount ( virtual_source . connection . environment , \"/some/path\" ) ownership_spec = OwenershipSpecification ( repository . uid , repository . gid ) return MountSpecification ([ mount ], ownership_spec ) The above command assumes a Repository Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"uid\" : { \"type\" : \"integer\" }, \"gid\" : { \"type\" : \"integer\" } } } Virtual Source Status \u00b6 Determines the status of a Virtual Source to show end users whether it is healthy or not. Required / Optional \u00b6 Optional. If not implemented, the platform assumes that the status is Status.ACTIVE . Delphix Engine Operations \u00b6 Virtual Source Enable Signature \u00b6 def virtual_status(virtual_source, repository, source_config) Decorator \u00b6 virtual.status() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 Status Status.ACTIVE if the plugin operation is not implemented. Example \u00b6 from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Status plugin = Plugin () @plugin.virtual.status () def virtual_status ( virtual_source , repository , source_config ): return Status . ACTIVE","title":"Virtualization SDK"},{"location":"References/Plugin_Operations/#plugin-operations","text":"Warning If a Plugin Operations is Required and is not present, the corresponding Delphix Engine Operation will fail when invoked. The plugin can still be built and uploaded to the Delphix Engine. Warning For each operation, the argument names must match exactly. For example, the Repository Discovery operation must have a single argument named source_connection . Plugin Operation Required Decorator Delphix Engine Operations Repository Discovery Yes discovery.repository() Environment Discovery Environment Refresh Source Config Discovery Yes discovery.source_config() Environment Discovery Environment Refresh Direct Linked Source Pre-Snapshot No linked.pre_snapshot() Linked Source Sync Direct Linked Source Post-Snapshot Yes linked.post_snapshot() Linked Source Sync Staged Linked Source Pre-Snapshot No linked.pre_snapshot() Linked Source Sync Staged Linked Source Post-Snapshot Yes linked.post_snapshot() Linked Source Sync Staged Linked Source Start-Staging No linked.start_staging() Linked Source Enable Staged Linked Source Stop-Staging No linked.stop_staging() Linked Source Disable Linked Source Delete Staged Linked Source Status No linked.status() N/A Staged Linked Source Worker No linked.worker() N/A Staged Linked Source Mount Specification Yes linked.mount_specification() Linked Source Sync Linked Source Enable Virtual Source Configure Yes virtual.configure() Virtual Source Provision Virtual Source Refresh Virtual Source Unconfigure No virtual.unconfigure() Virtual Source Refresh Virtual Source Delete Virtual Source Reconfigure Yes virtual.reconfigure() Virtual Source Rollback Virtual Source Enable Virtual Source Start No virtual.start() Virtual Source Start Virtual Source Stop No virtual.stop() Virtual Source Stop Virtual Source Pre-Snapshot No virtual.pre_snapshot() Virtual Source Snapshot Virtual Source Post-Snapshot Yes virtual.post_snapshot() Virtual Source Snapshot Virtual Source Mount Specification Yes virtual.mount_specification() Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start Virtual Source Status No virtual.status() Virtual Source Enable","title":"Plugin Operations"},{"location":"References/Plugin_Operations/#repository-discovery","text":"Discovers the set of repositories for a plugin on an environment . For a DBMS, this can correspond to the set of binaries installed on a Unix host.","title":"Repository Discovery"},{"location":"References/Plugin_Operations/#required-optional","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations","text":"Environment Refresh Environment Discovery","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature","text":"def repository_discovery(source_connection)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator","text":"discovery.repository()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments","text":"Argument Type Description source_connection RemoteConnection The connection associated with the remote environment to run repository discovery","title":"Arguments"},{"location":"References/Plugin_Operations/#returns","text":"A list of RepositoryDefinition objects.","title":"Returns"},{"location":"References/Plugin_Operations/#example","text":"from dlpx.virtualization.platform import Plugin from generated.defintions import RepositoryDefinition plugin = Plugin () @plugin.discovery.repository () def repository_discovery ( source_connection ): repository = RepositoryDefinition () repository . installPath = \"/usr/bin/install\" repository . version = \"1.2.3\" return [ repository ] The above command assumes a Repository Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"installPath\" : { \"type\" : \"string\" }, \"version\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"installPath\" ], \"nameField\" : [ \"installPath\" ] }","title":"Example"},{"location":"References/Plugin_Operations/#source-config-discovery","text":"Discovers the set of source configs for a plugin for a repository . For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host.","title":"Source Config Discovery"},{"location":"References/Plugin_Operations/#required-optional_1","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_1","text":"Environment Refresh Environment Discovery","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_1","text":"def source_config_discovery(source_connection, repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_1","text":"discovery.source_config()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_1","text":"Argument Type Description source_connection RemoteConnection The connection to the remote environment the corresponds to the repository. repository RepositoryDefinition The repository to discover source configs for.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_1","text":"A list of SourceConfigDefinition objects.","title":"Returns"},{"location":"References/Plugin_Operations/#example_1","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin.discovery.source_config () def source_config_discovery ( source_connection , repository ): source_config = SourceConfigDefinition () source_config . name = \"my_name\" source_config . port = 10000 return [ source_config ] The above command assumes a Source Config Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"number\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] }","title":"Example"},{"location":"References/Plugin_Operations/#direct-linked-source-pre-snapshot","text":"Sets up a dSource to ingest data. Only applies when using a Direct Linking strategy.","title":"Direct Linked Source Pre-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_2","text":"Optional","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_2","text":"Linked Source Sync","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_2","text":"def linked_pre_snapshot(direct_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_2","text":"linked.pre_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_2","text":"Argument Type Description direct_source DirectSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_2","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_2","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot ( direct_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#direct-linked-source-post-snapshot","text":"Captures metadata from a dSource once data has been ingested. Only applies when using a Direct Linking strategy.","title":"Direct Linked Source Post-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_3","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_3","text":"Linked Source Sync","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_3","text":"def linked_post_snapshot(direct_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_3","text":"linked.post_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_3","text":"Argument Type Description direct_source DirectSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_3","text":"SnapshotDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_3","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.linked.post_snapshot () def linked_post_snapshot ( direct_source , repository , source_config ): snapshot = SnapshotDefinition () snapshot . transaction_id = 1000 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"integer\" } } }","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-pre-snapshot","text":"Sets up a dSource to ingest data. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Pre-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_4","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_4","text":"Linked Source Sync","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_4","text":"def linked_pre_snapshot(staged_source, repository, source_config, snapshot_parameters)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_4","text":"linked.pre_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_4","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. snapshot_parameters SnapshotParametersDefinition The snapshot parameters.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_4","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_4","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot ( staged_source , repository , source_config , snapshot_parameters ): pass","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-post-snapshot","text":"Captures metadata from a dSource once data has been ingested. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Post-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_5","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_5","text":"Linked Source Sync","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_5","text":"def linked_post_snapshot(staged_source, repository, source_config, snapshot_parameters)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_5","text":"linked.post_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_5","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. snapshot_parameters SnapshotParametersDefinition The snapshot parameters.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_5","text":"SnapshotDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_5","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.linked.post_snapshot () def linked_post_snapshot ( staged_source , repository , source_config , snapshot_parameters ): snapshot = SnapshotDefinition () if snapshot_parameters . resync : snapshot . transaction_id = 1000 else : snapshot . transaction_id = 10 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"integer\" } } }","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-start-staging","text":"Sets up a Staging Source to ingest data. Only applies when using a Staged Linking strategy. Required to implement for Delphix Engine operations:","title":"Staged Linked Source Start-Staging"},{"location":"References/Plugin_Operations/#required-optional_6","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_6","text":"Linked Source Enable","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_6","text":"def start_staging(staged_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_6","text":"linked.start_staging()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_6","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_6","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_6","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.start_staging () def start_staging ( staged_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-stop-staging","text":"Quiesces a Staging Source to pause ingestion. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Stop-Staging"},{"location":"References/Plugin_Operations/#required-optional_7","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_7","text":"Linked Source Disable Linked Source Delete","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_7","text":"def stop_staging(staged_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_7","text":"linked.stop_staging()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_7","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_7","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#examples","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.stop_staging () def stop_staging ( staged_source , repository , source_config ): pass","title":"Examples"},{"location":"References/Plugin_Operations/#staged-linked-source-status","text":"Determines the status of a Staging Source to show end users whether it is healthy or not. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Status"},{"location":"References/Plugin_Operations/#required-optional_8","text":"Optional. If not implemented, the platform assumes that the status is Status.ACTIVE","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_8","text":"N/A","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_8","text":"def linked_status(staged_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_8","text":"linked.status()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_8","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_8","text":"Status Status.ACTIVE if the plugin operation is not implemented.","title":"Returns"},{"location":"References/Plugin_Operations/#example_7","text":"from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Status plugin = Plugin () @plugin.linked.status () def linked_status ( staged_source , repository , source_config ): return Status . ACTIVE","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-worker","text":"Monitors the status of a Staging Source on a reqular interval. It can be used to fix up any errors on staging if it is not functioning as expected. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Worker"},{"location":"References/Plugin_Operations/#required-optional_9","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_9","text":"N/A","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_9","text":"def worker(staged_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_9","text":"linked.worker()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_9","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_9","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_8","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.worker () def worker ( staged_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-mount-specification","text":"Returns configurations for the mounts associated for data in staged source. The ownership_specification is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.","title":"Staged Linked Source Mount Specification"},{"location":"References/Plugin_Operations/#required-optional_10","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_10","text":"Linked Source Sync Linked Source Enable","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_10","text":"def linked_mount_specification(staged_source, repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_10","text":"linked.mount_specification()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_10","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_10","text":"MountSpecification","title":"Returns"},{"location":"References/Plugin_Operations/#example_9","text":"Info ownership_specification only applies to Unix hosts. from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Mount from dlpx.virtualization.platform import MountSpecification from dlpx.virtualization.platform import OwenershipSpecification from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.linked.mount_specification () def linked_mount_specification ( staged_source , repository ): mount = Mount ( staged_source . staged_connection . environment , \"/some/path\" ) ownership_spec = OwenershipSpecification ( repository . uid , repository . gid ) return MountSpecification ([ mount ], ownership_spec ) The above command assumes a Repository Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"uid\" : { \"type\" : \"integer\" }, \"gid\" : { \"type\" : \"integer\" } } }","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-configure","text":"Configures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.","title":"Virtual Source Configure"},{"location":"References/Plugin_Operations/#required-optional_11","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_11","text":"Virtual Source Provision Virtual Source Refresh","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_11","text":"def configure(virtual_source, snapshot, repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_11","text":"virtual.configure()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_11","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. snapshot SnapshotDefinition The snapshot of the data set to configure. repository RepositoryDefinition The repository associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_11","text":"SourceConfigDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_10","text":"from dlpx.virtualization.platform import Plugin from generated.defintions import SourceConfigDefinition plugin = Plugin () @plugin.virtual.configure () def configure ( virtual_source , repository , snapshot ): name = \"config_name\" source_config = SourceConfigDefinition () source_config . name = name return source_config The above command assumes a SourceConfig Schema defined as: { \"type\" : \"object\" , \"required\" : [ \"name\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] }","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-unconfigure","text":"Quiesces the virtual source on a target environment. For database data files, shutting down and unregistering a database on a host.","title":"Virtual Source Unconfigure"},{"location":"References/Plugin_Operations/#required-optional_12","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_12","text":"Virtual Source Refresh Virtual Source Delete","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_12","text":"def unconfigure(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_12","text":"virtual.unconfigure()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_12","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_12","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_11","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.unconfigure () def unconfigure ( virtual_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-reconfigure","text":"Re-configures the data for a virtual source to point to the data in a prior snapshot for the virtual source. For database data files, this may mean recovering from a crash consistent format or backup of a new snapshot. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.","title":"Virtual Source Reconfigure"},{"location":"References/Plugin_Operations/#required-optional_13","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_13","text":"Virtual Source Rollback Virtual Source Enable","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_13","text":"def reconfigure(virtual_source, repository, source_config, snapshot)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_13","text":"virtual.reconfigure()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_13","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. snapshot SnapshotDefinition The snapshot of the data set to configure. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_13","text":"SourceConfigDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_12","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin.virtual.reconfigure () def configure ( virtual_source , repository , source_config , snapshot ): name = \"updated_config_name\" source_config = SourceConfigDefinition () source_config . name = name return source_config The above command assumes a SourceConfig Schema defined as: { \"type\" : \"object\" , \"required\" : [ \"name\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] }","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-start","text":"Executed whenever the data should be placed in a \"running\" state.","title":"Virtual Source Start"},{"location":"References/Plugin_Operations/#required-optional_14","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_14","text":"Virtual Source Start","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_14","text":"def start(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_14","text":"virtual.start()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_14","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_14","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_13","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.start () def start ( virtual_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-stop","text":"Executed whenever the data needs to be shut down. Required to implement for Delphix Engine operations:","title":"Virtual Source Stop"},{"location":"References/Plugin_Operations/#required-optional_15","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_15","text":"Virtual Source Stop","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_15","text":"def stop(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_15","text":"virtual.stop()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_15","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_15","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_14","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.stop () def stop ( virtual_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-pre-snapshot","text":"Prepares the virtual source for taking a snapshot of the data.","title":"Virtual Source Pre-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_16","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_16","text":"Virtual Source Snapshot","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_16","text":"def virtual_pre_snapshot(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_16","text":"virtual.pre_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_16","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_16","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_15","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.virtual.pre_snapshot () def virtual_pre_snapshot ( virtual_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-post-snapshot","text":"Captures metadata after a snapshot.","title":"Virtual Source Post-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_17","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_17","text":"Virtual Source Snapshot","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_17","text":"def virtual_post_snapshot(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_17","text":"virtual.post_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_17","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_17","text":"SnapshotDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_16","text":"from dlpx.virtualization.platform import Plugin from generated.defintions import SnapshotDefinition plugin = Plugin () @plugin.virtual.post_snapshot () def virtual_post_snapshot ( virtual_source , repository , source_config ): snapshot = SnapshotDefinition () snapshot . transaction_id = 1000 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"string\" } } }","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-mount-specification","text":"Returns configurations for the mounts associated for data in virtual source. The ownership_specification is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.","title":"Virtual Source Mount Specification"},{"location":"References/Plugin_Operations/#required-optional_18","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_18","text":"Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_18","text":"def virtual_mount_specification(virtual_source, repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_18","text":"virtual.mount_specification()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_18","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_18","text":"MountSpecification","title":"Returns"},{"location":"References/Plugin_Operations/#example_17","text":"Info ownership_specification only applies to Unix hosts. from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Mount from dlpx.virtualization.platform import MountSpecification from dlpx.virtualization.platform import OwenershipSpecification from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin.virtual.mount_specification () def virtual_mount_specification ( virtual_source , repository ): mount = Mount ( virtual_source . connection . environment , \"/some/path\" ) ownership_spec = OwenershipSpecification ( repository . uid , repository . gid ) return MountSpecification ([ mount ], ownership_spec ) The above command assumes a Repository Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"uid\" : { \"type\" : \"integer\" }, \"gid\" : { \"type\" : \"integer\" } } }","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-status","text":"Determines the status of a Virtual Source to show end users whether it is healthy or not.","title":"Virtual Source Status"},{"location":"References/Plugin_Operations/#required-optional_19","text":"Optional. If not implemented, the platform assumes that the status is Status.ACTIVE .","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_19","text":"Virtual Source Enable","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_19","text":"def virtual_status(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_19","text":"virtual.status()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_19","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_19","text":"Status Status.ACTIVE if the plugin operation is not implemented.","title":"Returns"},{"location":"References/Plugin_Operations/#example_18","text":"from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Status plugin = Plugin () @plugin.virtual.status () def virtual_status ( virtual_source , repository , source_config ): return Status . ACTIVE","title":"Example"},{"location":"References/Plugin_Versioning/","text":"Plugin Versioning \u00b6 Like any other piece of software, a plugin will change over time. New features will be added, bugs will be fixed, and so on. To keep track of this, plugins must specify a version string. There are rules about what sorts of plugin changes go along with changes to the version string. Before we get into the rules, let's talk about a problem that we want to avoid. Problems With Data Format Mismatches \u00b6 Plugins supply schemas to define their own datatypes. Data that conforms to these schemas is saved by the Delphix Engine. Later, the Delphix Engine may read back that saved data, and provide it to plugin code. Imagine this sequence of events: A plugin is initially released. In its snapshot schema, it defines two properties, date and time , that together specify when the snapshot was taken. A user installs the initial release of the plugin on their Delphix Engine. The user takes a snapshot of a dSource . Along with this snapshot is stored the date and time . A new version of the same plugin is released. In this new version, the snapshot schema now only defines a single property called timestamp , which specified both the date and the time together in a single property. The user installs the new plugin version. The user attempts to provision a new VDB from the snapshot they took in step 3. Now, when provision-related plugin code is called (for example the configure operation), it is going to be handed the snapshot data that was stored in step 2. The problem here is that we'll have a data format mismatch. The previously-saved snapshot data will have separate date and time fields, but the new plugin code will be expecting instead a single field called timestamp . Data Upgrading \u00b6 Coming Soon! Versioning Rules \u00b6 Each plugin declares a version string in the format <major>.<minor>.<patch> . The major and minor parts must always be integers, but patch can be any alphanumeric string. There are two scenarios where one version of a plugin can be installed on an engine that already has another version of the same plugin installed. Patch-only Changes \u00b6 If only the patch part of the version is changing, there are a relaxed set of rules: Schemas may not change. There is no defined ordering for patches. So long as major and minor do not change, any patch level can replace any other patch level. Major/Minor Changes \u00b6 If either major or minor (or both) is changing, then the following rules are applied: The major/minor pair may not decrease. If you have version 1.2.x already installed, then for example you can install 1.3.y or 2.0.y . But, you are not allowed to \"downgrade\" to version 1.1.z . Schemas may change. The plugin must provide upgrade operations so that old-format data can be converted as necessary.","title":"Plugin Versioning"},{"location":"References/Plugin_Versioning/#plugin-versioning","text":"Like any other piece of software, a plugin will change over time. New features will be added, bugs will be fixed, and so on. To keep track of this, plugins must specify a version string. There are rules about what sorts of plugin changes go along with changes to the version string. Before we get into the rules, let's talk about a problem that we want to avoid.","title":"Plugin Versioning"},{"location":"References/Plugin_Versioning/#problems-with-data-format-mismatches","text":"Plugins supply schemas to define their own datatypes. Data that conforms to these schemas is saved by the Delphix Engine. Later, the Delphix Engine may read back that saved data, and provide it to plugin code. Imagine this sequence of events: A plugin is initially released. In its snapshot schema, it defines two properties, date and time , that together specify when the snapshot was taken. A user installs the initial release of the plugin on their Delphix Engine. The user takes a snapshot of a dSource . Along with this snapshot is stored the date and time . A new version of the same plugin is released. In this new version, the snapshot schema now only defines a single property called timestamp , which specified both the date and the time together in a single property. The user installs the new plugin version. The user attempts to provision a new VDB from the snapshot they took in step 3. Now, when provision-related plugin code is called (for example the configure operation), it is going to be handed the snapshot data that was stored in step 2. The problem here is that we'll have a data format mismatch. The previously-saved snapshot data will have separate date and time fields, but the new plugin code will be expecting instead a single field called timestamp .","title":"Problems With Data Format Mismatches"},{"location":"References/Plugin_Versioning/#data-upgrading","text":"Coming Soon!","title":"Data Upgrading"},{"location":"References/Plugin_Versioning/#versioning-rules","text":"Each plugin declares a version string in the format <major>.<minor>.<patch> . The major and minor parts must always be integers, but patch can be any alphanumeric string. There are two scenarios where one version of a plugin can be installed on an engine that already has another version of the same plugin installed.","title":"Versioning Rules"},{"location":"References/Plugin_Versioning/#patch-only-changes","text":"If only the patch part of the version is changing, there are a relaxed set of rules: Schemas may not change. There is no defined ordering for patches. So long as major and minor do not change, any patch level can replace any other patch level.","title":"Patch-only Changes"},{"location":"References/Plugin_Versioning/#majorminor-changes","text":"If either major or minor (or both) is changing, then the following rules are applied: The major/minor pair may not decrease. If you have version 1.2.x already installed, then for example you can install 1.3.y or 2.0.y . But, you are not allowed to \"downgrade\" to version 1.1.z . Schemas may change. The plugin must provide upgrade operations so that old-format data can be converted as necessary.","title":"Major/Minor Changes"},{"location":"References/Schemas/","text":"Schemas \u00b6 About Schemas \u00b6 Any time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data: What is the set of data needed and what should they be called? What is the type of each piece of data: Strings? Integers? Booleans? Plugins use schemas to describe the format of such data. Once a schema is defined, it is used in three ways It tells the Delphix Engine how to store the data for later use. It is used to autogenerate a custom user interface, and to validate user inputs. It is used to autogenerate Python classes that can be used by plugin code to access and manipulate user input and stored data. There are five plugin-customizable data formats: Delphix Object Schema Autogenerated Class Repository RepositoryDefinition RepositoryDefinition Source Config SourceConfigDefinition SourceConfigDefinition Linked Source LinkedSourceDefinition LinkedSourceDefinition Virtual Source VirtualSourceDefinition VirtualSourceDefinition Snapshot SnapshotDefinition SnapshotDefinition JSON Schemas \u00b6 Plugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below: What is JSON? What is a JSON schema? How has Delphix augmented JSON schemas? JSON \u00b6 JSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format: JSON Description \"hello\" A string. Note the double quotes. 17 An integer true A boolean {\"name\": \"Julie\", \"age\": 37} A JSON object with two fields, name (a string), and age (an integer). Objects are denoted with curly braces. [ true, false, true] A JSON array with three booleans. Arrays are denoted with square brackets. For more details on JSON, please see https://www.json.org/. JSON Schemas \u00b6 The \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the description of the format of data (whereas \"raw\" JSON is intended for storing data). Here is an example of a JSON schema that defines a (simplified) US address: { \"type\" : \"object\" , \"required\" : [ \"name\" , \"streetNumber\" , \"street\" , \"city\" , \"state\" , \"zip5\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"streetNumber\" : { \"type\" : \"string\" }, \"street\" : { \"type\" : \"string\" }, \"unit\" : { \"type\" : \"string\" }, \"city\" : { \"type\" : \"string\" , \"pattern\" : \"^[A-Z][A-Za-z ]*$\" }, \"state\" : { \"type\" : \"string\" , \"pattern\" : \"^[A-Z]{2}$\" }, \"zip5\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]{5}\" }, \"zipPlus4\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]{4}\" } } } Note that this is perfectly valid JSON data. It's a JSON object with four fields: type (a JSON string), required (A JSON array), additionalProperties (a JSON boolean), and properties . properties , in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc. But, this isn't just a JSON object. This is a JSON schema. It uses special keywords like type required , and additionalProperties . These have specially-defined meanings in the context of JSON schemas. Here is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords. keyword description additionalProperties Determines whether the schema allows properties that are not explicitly listed in the properties specification. Must be a true or false . pattern Used with string types to specify a regular expression that the property must conform to. required A list of required properties. Properties not listed in this list are optional. string Used with type to declare that a property must be a string. type Specifies a datatype. Common values are object , array , number , integer , boolean , and string . Some points to note about the address schema above: Because of the required list, all valid addresses must have fields called name , streetNumber and so on. unit and zipPlus4 do not appear in the required list, and therefore are optional. Because of additionalProperties being false , valid addresses cannot make up their own fields like nickname or doorbellLocation . Because of the pattern , any state field in a valid address must consist of exactly two capital letters. Similarly, city must only contain letters and spaces, and zip and zipPlus4 must only contain digits. Each property has its own valid subschema that describes its own type definition. Here is a JSON object that conforms to the above schema: { \"name\" : \"Delphix\" , \"streetNumber\" : \"220\" , \"street\" : \"Congress St.\" , \"unit\" : \"200\" , \"city\" : \"Boston\" , \"state\" : \"MA\" , \"zip\" : \"02210\" } Info A common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema describes what an address looks like. The address itself is not a schema. For much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see https://json-schema.org/understanding-json-schema/ Delphix-specific Extensions to JSON Schema \u00b6 The JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords. The list below outlines each of these keywords, and provides minimal examples of how they might be used. description \u00b6 Summary Required or Optional? Optional Where? In any property subschema, at the same level as type . The description keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown. In this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget. { \"properties\" : { \"name\" : { \"type\" : \"string\" , \"description\" : \"User-readable name for the provisioned database\" } } } identityFields \u00b6 Summary Required or Optional? Required (for repository and source config schemas only) Where? At the top level of a repository or source config schema, at the same level as type and properties . The identityFields is a list of property names that, together, serve as a unique identifier for a repository or source config. When a plugin's automatic discovery code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about. For example, suppose the engine already knows about a single repository with data {\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"} (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data { \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"} . What should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name? identityFields is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if all of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data. identityFields is required for RepositoryDefinition and SourceConfigDefinition schemas, and may not be used in any other schemas. In this example, we'll tell the Delphix Engine that path is the sole unique identifier. { \"properties\" : { \"dbname\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"path\" ] } nameField \u00b6 Summary Required or Optional? Required (for repository and source config schemas only) Where? At the top level of a repository or source config schema, at the same level as type and properties . The nameField keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as properties . It is required for RepositoryDefinition and SourceConfigDefinition schemas, and may not be used in any other schemas. In this example, we will use the path property as the user-visible name. { \"properties\" : { \"path\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } }, \"nameField\" : \"path\" } So, if we have an repository object that looks like { \"path\" : \"/usr/bin\" , \"port\" : 8800 } then the user will be able to refer to this object as /usr/bin . password \u00b6 Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The password keyword can be used to specify the format of a string . (Note that format is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described here . In this example, the dbPass field on any object will be treated as a password. { \"properties\" : { \"dbPass\" : { \"type\" : \"string\" , \"format\" : \"password\" } } } prettyName \u00b6 Summary Required or Optional? Optional Where? In any property subschema, at the same level as type . The prettyName keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used. In this example, the user would see \"Name of Database\" on the UI, instead of just \"name\". { \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Name of Database\" } } } unixpath \u00b6 Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The unixpath keyword is used to specify the format of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path. { \"properties\" : { \"datapath\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" } } }","title":"Virtualization SDK"},{"location":"References/Schemas/#schemas","text":"","title":"Schemas"},{"location":"References/Schemas/#about-schemas","text":"Any time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data: What is the set of data needed and what should they be called? What is the type of each piece of data: Strings? Integers? Booleans? Plugins use schemas to describe the format of such data. Once a schema is defined, it is used in three ways It tells the Delphix Engine how to store the data for later use. It is used to autogenerate a custom user interface, and to validate user inputs. It is used to autogenerate Python classes that can be used by plugin code to access and manipulate user input and stored data. There are five plugin-customizable data formats: Delphix Object Schema Autogenerated Class Repository RepositoryDefinition RepositoryDefinition Source Config SourceConfigDefinition SourceConfigDefinition Linked Source LinkedSourceDefinition LinkedSourceDefinition Virtual Source VirtualSourceDefinition VirtualSourceDefinition Snapshot SnapshotDefinition SnapshotDefinition","title":"About Schemas"},{"location":"References/Schemas/#json-schemas","text":"Plugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below: What is JSON? What is a JSON schema? How has Delphix augmented JSON schemas?","title":"JSON Schemas"},{"location":"References/Schemas/#json","text":"JSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format: JSON Description \"hello\" A string. Note the double quotes. 17 An integer true A boolean {\"name\": \"Julie\", \"age\": 37} A JSON object with two fields, name (a string), and age (an integer). Objects are denoted with curly braces. [ true, false, true] A JSON array with three booleans. Arrays are denoted with square brackets. For more details on JSON, please see https://www.json.org/.","title":"JSON"},{"location":"References/Schemas/#json-schemas_1","text":"The \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the description of the format of data (whereas \"raw\" JSON is intended for storing data). Here is an example of a JSON schema that defines a (simplified) US address: { \"type\" : \"object\" , \"required\" : [ \"name\" , \"streetNumber\" , \"street\" , \"city\" , \"state\" , \"zip5\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"streetNumber\" : { \"type\" : \"string\" }, \"street\" : { \"type\" : \"string\" }, \"unit\" : { \"type\" : \"string\" }, \"city\" : { \"type\" : \"string\" , \"pattern\" : \"^[A-Z][A-Za-z ]*$\" }, \"state\" : { \"type\" : \"string\" , \"pattern\" : \"^[A-Z]{2}$\" }, \"zip5\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]{5}\" }, \"zipPlus4\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]{4}\" } } } Note that this is perfectly valid JSON data. It's a JSON object with four fields: type (a JSON string), required (A JSON array), additionalProperties (a JSON boolean), and properties . properties , in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc. But, this isn't just a JSON object. This is a JSON schema. It uses special keywords like type required , and additionalProperties . These have specially-defined meanings in the context of JSON schemas. Here is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords. keyword description additionalProperties Determines whether the schema allows properties that are not explicitly listed in the properties specification. Must be a true or false . pattern Used with string types to specify a regular expression that the property must conform to. required A list of required properties. Properties not listed in this list are optional. string Used with type to declare that a property must be a string. type Specifies a datatype. Common values are object , array , number , integer , boolean , and string . Some points to note about the address schema above: Because of the required list, all valid addresses must have fields called name , streetNumber and so on. unit and zipPlus4 do not appear in the required list, and therefore are optional. Because of additionalProperties being false , valid addresses cannot make up their own fields like nickname or doorbellLocation . Because of the pattern , any state field in a valid address must consist of exactly two capital letters. Similarly, city must only contain letters and spaces, and zip and zipPlus4 must only contain digits. Each property has its own valid subschema that describes its own type definition. Here is a JSON object that conforms to the above schema: { \"name\" : \"Delphix\" , \"streetNumber\" : \"220\" , \"street\" : \"Congress St.\" , \"unit\" : \"200\" , \"city\" : \"Boston\" , \"state\" : \"MA\" , \"zip\" : \"02210\" } Info A common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema describes what an address looks like. The address itself is not a schema. For much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see https://json-schema.org/understanding-json-schema/","title":"JSON Schemas"},{"location":"References/Schemas/#delphix-specific-extensions-to-json-schema","text":"The JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords. The list below outlines each of these keywords, and provides minimal examples of how they might be used.","title":"Delphix-specific Extensions to JSON Schema"},{"location":"References/Schemas/#description","text":"Summary Required or Optional? Optional Where? In any property subschema, at the same level as type . The description keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown. In this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget. { \"properties\" : { \"name\" : { \"type\" : \"string\" , \"description\" : \"User-readable name for the provisioned database\" } } }","title":"description"},{"location":"References/Schemas/#identityfields","text":"Summary Required or Optional? Required (for repository and source config schemas only) Where? At the top level of a repository or source config schema, at the same level as type and properties . The identityFields is a list of property names that, together, serve as a unique identifier for a repository or source config. When a plugin's automatic discovery code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about. For example, suppose the engine already knows about a single repository with data {\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"} (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data { \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"} . What should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name? identityFields is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if all of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data. identityFields is required for RepositoryDefinition and SourceConfigDefinition schemas, and may not be used in any other schemas. In this example, we'll tell the Delphix Engine that path is the sole unique identifier. { \"properties\" : { \"dbname\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"path\" ] }","title":"identityFields"},{"location":"References/Schemas/#namefield","text":"Summary Required or Optional? Required (for repository and source config schemas only) Where? At the top level of a repository or source config schema, at the same level as type and properties . The nameField keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as properties . It is required for RepositoryDefinition and SourceConfigDefinition schemas, and may not be used in any other schemas. In this example, we will use the path property as the user-visible name. { \"properties\" : { \"path\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } }, \"nameField\" : \"path\" } So, if we have an repository object that looks like { \"path\" : \"/usr/bin\" , \"port\" : 8800 } then the user will be able to refer to this object as /usr/bin .","title":"nameField"},{"location":"References/Schemas/#password","text":"Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The password keyword can be used to specify the format of a string . (Note that format is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described here . In this example, the dbPass field on any object will be treated as a password. { \"properties\" : { \"dbPass\" : { \"type\" : \"string\" , \"format\" : \"password\" } } }","title":"password"},{"location":"References/Schemas/#prettyname","text":"Summary Required or Optional? Optional Where? In any property subschema, at the same level as type . The prettyName keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used. In this example, the user would see \"Name of Database\" on the UI, instead of just \"name\". { \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Name of Database\" } } }","title":"prettyName"},{"location":"References/Schemas/#unixpath","text":"Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The unixpath keyword is used to specify the format of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path. { \"properties\" : { \"datapath\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" } } }","title":"unixpath"},{"location":"References/Schemas_and_Autogenerated_Classes/","text":"Schemas and Autogenerated Classes \u00b6 Plugin operations will sometimes need to work with data in these custom formats. For example, the configure operation will accept snapshot data as an input, and must produce source config data as an output. To enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes. Info Autogenerated Python code will use lower_case_with_underscores as attribute names as per Python variable naming conventions. That is, if we were to use mountLocation as the schema property name, it would be called mount_location in the generated Python code. RepositoryDefinition \u00b6 Defines properties used to identify a Repository . RepositoryDefinition Schema \u00b6 The plugin must also decide on a name field and a set of identityFields to display and uniquely identify the repository . { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" , \"path\" ], \"nameField\" : \"name\" } RepositoryDefinition Class \u00b6 Autogenerated based on the RepositoryDefinition Schema . class RepositoryDefinition : def __init__ ( self , repository_name , repository_path ): self . _inner_dict = { \"name\" : repository_name , \"path\" : repository_path } To use the class: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . name = \"name\" repository . path = \"/some/path\" SourceConfigDefinition \u00b6 Defines properties used to identify a Source Config . SourceConfigDefinition Schema \u00b6 The plugin must also decide on a name field and a set of identityFields to display and uniquely identify the source config . { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : \"name\" } SourceConfigDefinition Class \u00b6 Autogenerated based on the SourceConfigDefinition Schema . class SourceConfigDefinition : def __init__ ( self , config_name , config_path ): self . _inner_dict = { \"name\" : config_name , \"path\" : config_path } To use the class: from generated.defintions import SourceConfigDefinition source_config = SourceConfigDefinition () source_config . name = \"name\" source_config . path = \"/some/path\" LinkedSourceDefinition \u00b6 Defines properties used to identify linked sources . LinkedSourceDefinition Schema \u00b6 { \"type\" : \"object\" , \"required\" : [ \"name\" , \"port\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } } LinkedSourceDefinition Class \u00b6 Autogenerated based on the LinkedSourceDefinition Schema . class LinkedSourceDefinition : def __init__ ( self , source_name , source_port ): self . _inner_dict = { \"name\" : source_name , \"port\" : source_port } To use the class: from generated.defintions import LinkedSourceDefinition source = LinkedSourceDefinition ( \"name\" , 1000 ) name = source . name port = source . port VirtualSourceDefinition \u00b6 Defines properties used to identify virtual sources . VirtualSourceDefinition Schema \u00b6 { \"type\" : \"object\" , \"required\" : [ \"name\" , \"port\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } } VirtualSourceDefinition Class \u00b6 Autogenerated based on the VirtualSourceDefinition Schema . class VirtualSourceDefinition : def __init__ ( self , source_name , source_port ): self . _inner_dict = { \"name\" : source_name , \"port\" : source_port } To use the class: from generated.defintions import VirtualSourceDefinition source = VirtualSourceDefinition ( \"name\" , 1000 ) name = source . name port = source . port SnapshotDefinition \u00b6 Defines properties used to snapshots . SnapshotDefinition Schema \u00b6 { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"version\" : { \"type\" : \"string\" }, \"version\" : { \"type\" : \"integer\" } } SnapshotDefinition Class \u00b6 Autogenerated based on the VirtualSourceDefinition Schema . class VirtualSourceDefinition : def __init__ ( self , snapshot_version , snapshot_transaction_id ): self . _inner_dict = { \"version\" : snapshot_version , \"transaction_id\" : snapshot_transaction_id } To use the class: from generated.defintions import SnapshotDefinition snapshot = SnapshotDefinition () snapshot . version = \"1.2.3\" snapshot . transaction_id = 1000","title":"Schemas and Autogenerated Classes"},{"location":"References/Schemas_and_Autogenerated_Classes/#schemas-and-autogenerated-classes","text":"Plugin operations will sometimes need to work with data in these custom formats. For example, the configure operation will accept snapshot data as an input, and must produce source config data as an output. To enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes. Info Autogenerated Python code will use lower_case_with_underscores as attribute names as per Python variable naming conventions. That is, if we were to use mountLocation as the schema property name, it would be called mount_location in the generated Python code.","title":"Schemas and Autogenerated Classes"},{"location":"References/Schemas_and_Autogenerated_Classes/#repositorydefinition","text":"Defines properties used to identify a Repository .","title":"RepositoryDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#repositorydefinition-schema","text":"The plugin must also decide on a name field and a set of identityFields to display and uniquely identify the repository . { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" , \"path\" ], \"nameField\" : \"name\" }","title":"RepositoryDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#repositorydefinition-class","text":"Autogenerated based on the RepositoryDefinition Schema . class RepositoryDefinition : def __init__ ( self , repository_name , repository_path ): self . _inner_dict = { \"name\" : repository_name , \"path\" : repository_path } To use the class: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . name = \"name\" repository . path = \"/some/path\"","title":"RepositoryDefinition Class"},{"location":"References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition","text":"Defines properties used to identify a Source Config .","title":"SourceConfigDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-schema","text":"The plugin must also decide on a name field and a set of identityFields to display and uniquely identify the source config . { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : \"name\" }","title":"SourceConfigDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-class","text":"Autogenerated based on the SourceConfigDefinition Schema . class SourceConfigDefinition : def __init__ ( self , config_name , config_path ): self . _inner_dict = { \"name\" : config_name , \"path\" : config_path } To use the class: from generated.defintions import SourceConfigDefinition source_config = SourceConfigDefinition () source_config . name = \"name\" source_config . path = \"/some/path\"","title":"SourceConfigDefinition Class"},{"location":"References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition","text":"Defines properties used to identify linked sources .","title":"LinkedSourceDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-schema","text":"{ \"type\" : \"object\" , \"required\" : [ \"name\" , \"port\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } }","title":"LinkedSourceDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-class","text":"Autogenerated based on the LinkedSourceDefinition Schema . class LinkedSourceDefinition : def __init__ ( self , source_name , source_port ): self . _inner_dict = { \"name\" : source_name , \"port\" : source_port } To use the class: from generated.defintions import LinkedSourceDefinition source = LinkedSourceDefinition ( \"name\" , 1000 ) name = source . name port = source . port","title":"LinkedSourceDefinition Class"},{"location":"References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition","text":"Defines properties used to identify virtual sources .","title":"VirtualSourceDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-schema","text":"{ \"type\" : \"object\" , \"required\" : [ \"name\" , \"port\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } }","title":"VirtualSourceDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-class","text":"Autogenerated based on the VirtualSourceDefinition Schema . class VirtualSourceDefinition : def __init__ ( self , source_name , source_port ): self . _inner_dict = { \"name\" : source_name , \"port\" : source_port } To use the class: from generated.defintions import VirtualSourceDefinition source = VirtualSourceDefinition ( \"name\" , 1000 ) name = source . name port = source . port","title":"VirtualSourceDefinition Class"},{"location":"References/Schemas_and_Autogenerated_Classes/#snapshotdefinition","text":"Defines properties used to snapshots .","title":"SnapshotDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-schema","text":"{ \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"version\" : { \"type\" : \"string\" }, \"version\" : { \"type\" : \"integer\" } }","title":"SnapshotDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-class","text":"Autogenerated based on the VirtualSourceDefinition Schema . class VirtualSourceDefinition : def __init__ ( self , snapshot_version , snapshot_transaction_id ): self . _inner_dict = { \"version\" : snapshot_version , \"transaction_id\" : snapshot_transaction_id } To use the class: from generated.defintions import SnapshotDefinition snapshot = SnapshotDefinition () snapshot . version = \"1.2.3\" snapshot . transaction_id = 1000","title":"SnapshotDefinition Class"},{"location":"References/Workflows/","text":"Workflows \u00b6 Legend \u00b6 Environment Discovery / Refresh \u00b6 Linked Source Sync \u00b6 Linked Source Enable \u00b6 Linked Source Disable \u00b6 Linked Source Delete \u00b6 Virtual Source Provision \u00b6 Virtual Source Snapshot \u00b6 Virtual Source Refresh \u00b6 Virtual Source Rollback \u00b6 Virtual Source Delete \u00b6 Virtual Source Start \u00b6 Virtual Source Stop \u00b6 Virtual Source Enable \u00b6 Virtual Source Disable \u00b6","title":"Virtualization SDK"},{"location":"References/Workflows/#workflows","text":"","title":"Workflows"},{"location":"References/Workflows/#legend","text":"","title":"Legend"},{"location":"References/Workflows/#environment-discovery-refresh","text":"","title":"Environment Discovery / Refresh"},{"location":"References/Workflows/#linked-source-sync","text":"","title":"Linked Source Sync"},{"location":"References/Workflows/#linked-source-enable","text":"","title":"Linked Source Enable"},{"location":"References/Workflows/#linked-source-disable","text":"","title":"Linked Source Disable"},{"location":"References/Workflows/#linked-source-delete","text":"","title":"Linked Source Delete"},{"location":"References/Workflows/#virtual-source-provision","text":"","title":"Virtual Source Provision"},{"location":"References/Workflows/#virtual-source-snapshot","text":"","title":"Virtual Source Snapshot"},{"location":"References/Workflows/#virtual-source-refresh","text":"","title":"Virtual Source Refresh"},{"location":"References/Workflows/#virtual-source-rollback","text":"","title":"Virtual Source Rollback"},{"location":"References/Workflows/#virtual-source-delete","text":"","title":"Virtual Source Delete"},{"location":"References/Workflows/#virtual-source-start","text":"","title":"Virtual Source Start"},{"location":"References/Workflows/#virtual-source-stop","text":"","title":"Virtual Source Stop"},{"location":"References/Workflows/#virtual-source-enable","text":"","title":"Virtual Source Enable"},{"location":"References/Workflows/#virtual-source-disable","text":"","title":"Virtual Source Disable"},{"location":"Release_Notes/0.4.0/","text":"Release - Early Preview 2 (v0.4.0) \u00b6 To install or upgrade the SDK, refer to instructions here . New & Improved \u00b6 Added a new CLI command download-logs to enable downloading plugin generated logs from the Delphix Engine. Added an optional argument named check to the following platform library functions: run_bash run_powershell With check=true , the platform library function checks the exit_code and raises an exception if it is non-zero. Modified init to auto-generate default implementations for all required plugin operations. Improved build validation for: Required plugin operations . Incorrect plugin operation argument names. Plugin Config entryPoint : The entryPoint is now imported during the build as part of the validation. Schemas : Validated to conform to the JSON Schema Draft-07 Specification . Improved runtime validation and error messages for: Objects returned from plugin operations . Platform Classes during instantiation. Platform Library function arguments. Added support for Docker based plugins by specifying rootSquashEnabled: false in the plugin config . Added Job and thread information to plugin generated log messages to increase diagnosability and observability. Breaking Changes \u00b6 A new argument snapshot_parameters was added to the following staged plugin operations: Staged Linked Source Pre-Snapshot Staged Linked Source Post-Snapshot This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are here . Detailed steps to detect and make changes. Properties of the StagedSource class were modified: connection was renamed to source_connection . staged_connection was added to allow connecting to the staging environment. This will enable plugins to connect to both the source and staging environments. More details about these properties are here . Detailed steps to detect and make changes. Fixed \u00b6 Allow access to nested package resources via pkgutil.get_data . Fixed Out of Memory exceptions. Fixed missing or incorrectly populated properties for the following classes: Class Properties VirtualSource mounts RemoteUser name RemoteEnvironment name RemoteHost name binary_path Updated Job warnings during discovery to display the underlying Python exceptions if one is raised by the plugin operations. Recreate the plugin's log directory if a plugin is deleted and re-uploaded to the Delphix Engine. Mark incorrectly provisioned VDBs as unusable and prevent subsequent Delphix Engine operations on such VDBs. Better error messages when incorrect environment types are used for Platform Libraries. Better error messages when a plugin's schema is updated and the plugin is re-uploaded to the Delphix Engine, with clear instructions on how to proceed. Fixed build failures on Windows.","title":"Release - Early Preview 2 (v0.4.0)"},{"location":"Release_Notes/0.4.0/#release-early-preview-2-v040","text":"To install or upgrade the SDK, refer to instructions here .","title":"Release - Early Preview 2 (v0.4.0)"},{"location":"Release_Notes/0.4.0/#new-improved","text":"Added a new CLI command download-logs to enable downloading plugin generated logs from the Delphix Engine. Added an optional argument named check to the following platform library functions: run_bash run_powershell With check=true , the platform library function checks the exit_code and raises an exception if it is non-zero. Modified init to auto-generate default implementations for all required plugin operations. Improved build validation for: Required plugin operations . Incorrect plugin operation argument names. Plugin Config entryPoint : The entryPoint is now imported during the build as part of the validation. Schemas : Validated to conform to the JSON Schema Draft-07 Specification . Improved runtime validation and error messages for: Objects returned from plugin operations . Platform Classes during instantiation. Platform Library function arguments. Added support for Docker based plugins by specifying rootSquashEnabled: false in the plugin config . Added Job and thread information to plugin generated log messages to increase diagnosability and observability.","title":"New &amp; Improved"},{"location":"Release_Notes/0.4.0/#breaking-changes","text":"A new argument snapshot_parameters was added to the following staged plugin operations: Staged Linked Source Pre-Snapshot Staged Linked Source Post-Snapshot This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are here . Detailed steps to detect and make changes. Properties of the StagedSource class were modified: connection was renamed to source_connection . staged_connection was added to allow connecting to the staging environment. This will enable plugins to connect to both the source and staging environments. More details about these properties are here . Detailed steps to detect and make changes.","title":"Breaking Changes"},{"location":"Release_Notes/0.4.0/#fixed","text":"Allow access to nested package resources via pkgutil.get_data . Fixed Out of Memory exceptions. Fixed missing or incorrectly populated properties for the following classes: Class Properties VirtualSource mounts RemoteUser name RemoteEnvironment name RemoteHost name binary_path Updated Job warnings during discovery to display the underlying Python exceptions if one is raised by the plugin operations. Recreate the plugin's log directory if a plugin is deleted and re-uploaded to the Delphix Engine. Mark incorrectly provisioned VDBs as unusable and prevent subsequent Delphix Engine operations on such VDBs. Better error messages when incorrect environment types are used for Platform Libraries. Better error messages when a plugin's schema is updated and the plugin is re-uploaded to the Delphix Engine, with clear instructions on how to proceed. Fixed build failures on Windows.","title":"Fixed"},{"location":"Release_Notes/0.4.0_Breaking_Changes/","text":"Breaking Changes - Early Preview 2 (v.0.4.0) \u00b6 New Argument snapshot_parameters \u00b6 A new argument snapshot_parameters was added to the following staged plugin operations: Staged Linked Source Pre-Snapshot Staged Linked Source Post-Snapshot This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are here . What is affected \u00b6 This argument applies only to staged plugins. The plugin's source code will have to be updated for the following staged plugin operations: Staged Linked Source Pre-Snapshot : This plugin operation is optional and will need to be updated if the plugin implements it. Staged Linked Source Post-Snapshot : This plugin operation is required and will need to be updated. How does it fail \u00b6 build will fail with the following error message if the new argument is not added to the affected staged plugin operations: $ dvp build Error: Number of arguments do not match in method staged_post_snapshot. Expected: [ 'staged_source' , 'repository' , 'source_config' , 'snapshot_parameters' ] , Found: [ 'repository' , 'source_config' , 'staged_source' ] . Error: Number of arguments do not match in method staged_pre_snapshot. Expected: [ 'staged_source' , 'repository' , 'source_config' , 'snapshot_parameters' ] , Found: [ 'repository' , 'source_config' , 'staged_source' ] . 0 Warning ( s ) . 2 Error ( s ) . BUILD FAILED. How to fix it \u00b6 Update the affected staged plugin operations to include the new argument snapshot_parameters . Previous releases from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # This was the function signature prior to 0.4.0 pass @plugin.linked.post_snapshot () def linked_post_snapshot_prior ( staged_source , repository , source_config ): # This was the function signature prior to 0.4.0 return SnapshotDefinition () 0.4.0 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot_040 ( staged_source , repository , source_config , snapshot_parameters ): # Updated function signature in 0.4.0 pass @plugin.linked.post_snapshot () def linked_post_snapshot_040 ( staged_source , repository , source_config , snapshot_parameters ): # Updated function signature in 0.4.0 return SnapshotDefinition () StagedSource Properties Modified \u00b6 Properties of the StagedSource class were modified: connection was renamed to source_connection . staged_connection was added to allow connecting to the staging environment. This will enable plugins to connect to both the source and staging environments. More details about these properties are here . What is affected \u00b6 This change applies only to staged plugins. Required Changes \u00b6 The plugin's source code will have to be updated for any staged plugin operations that accesses the connection propery of a StagedSource object. Optional Changes \u00b6 The plugin can choose to use the new staged_connection property to connect to the staging environment of a dSource. How does it fail \u00b6 Any Delphix Engine operation that calls a plugin operation that has not been fixed would fail with the following stack trace as part of the output of the user exception: AttributeError : 'StagedSource' object has no attribute 'connection' How to fix it \u00b6 Update any staged plugin operations that access the renamed property. Previous releases from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # Property name was 'connection' was the name of the property for staged_source prior to 0.4.0 libs . run_bash ( staged_source . connection , 'date' ) 0.4.0 from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # Property name was updated to 'source_connection' in 0.4.0 libs . run_bash ( staged_source . source_connection , 'date' )","title":"Breaking Changes - Early Preview 2 (v.0.4.0)"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#breaking-changes-early-preview-2-v040","text":"","title":"Breaking Changes - Early Preview 2 (v.0.4.0)"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#new-argument-snapshot_parameters","text":"A new argument snapshot_parameters was added to the following staged plugin operations: Staged Linked Source Pre-Snapshot Staged Linked Source Post-Snapshot This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are here .","title":"New Argument snapshot_parameters"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#what-is-affected","text":"This argument applies only to staged plugins. The plugin's source code will have to be updated for the following staged plugin operations: Staged Linked Source Pre-Snapshot : This plugin operation is optional and will need to be updated if the plugin implements it. Staged Linked Source Post-Snapshot : This plugin operation is required and will need to be updated.","title":"What is affected"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#how-does-it-fail","text":"build will fail with the following error message if the new argument is not added to the affected staged plugin operations: $ dvp build Error: Number of arguments do not match in method staged_post_snapshot. Expected: [ 'staged_source' , 'repository' , 'source_config' , 'snapshot_parameters' ] , Found: [ 'repository' , 'source_config' , 'staged_source' ] . Error: Number of arguments do not match in method staged_pre_snapshot. Expected: [ 'staged_source' , 'repository' , 'source_config' , 'snapshot_parameters' ] , Found: [ 'repository' , 'source_config' , 'staged_source' ] . 0 Warning ( s ) . 2 Error ( s ) . BUILD FAILED.","title":"How does it fail"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#how-to-fix-it","text":"Update the affected staged plugin operations to include the new argument snapshot_parameters . Previous releases from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # This was the function signature prior to 0.4.0 pass @plugin.linked.post_snapshot () def linked_post_snapshot_prior ( staged_source , repository , source_config ): # This was the function signature prior to 0.4.0 return SnapshotDefinition () 0.4.0 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot_040 ( staged_source , repository , source_config , snapshot_parameters ): # Updated function signature in 0.4.0 pass @plugin.linked.post_snapshot () def linked_post_snapshot_040 ( staged_source , repository , source_config , snapshot_parameters ): # Updated function signature in 0.4.0 return SnapshotDefinition ()","title":"How to fix it"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#stagedsource-properties-modified","text":"Properties of the StagedSource class were modified: connection was renamed to source_connection . staged_connection was added to allow connecting to the staging environment. This will enable plugins to connect to both the source and staging environments. More details about these properties are here .","title":"StagedSource Properties Modified"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#what-is-affected_1","text":"This change applies only to staged plugins.","title":"What is affected"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#required-changes","text":"The plugin's source code will have to be updated for any staged plugin operations that accesses the connection propery of a StagedSource object.","title":"Required Changes"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#optional-changes","text":"The plugin can choose to use the new staged_connection property to connect to the staging environment of a dSource.","title":"Optional Changes"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#how-does-it-fail_1","text":"Any Delphix Engine operation that calls a plugin operation that has not been fixed would fail with the following stack trace as part of the output of the user exception: AttributeError : 'StagedSource' object has no attribute 'connection'","title":"How does it fail"},{"location":"Release_Notes/0.4.0_Breaking_Changes/#how-to-fix-it_1","text":"Update any staged plugin operations that access the renamed property. Previous releases from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # Property name was 'connection' was the name of the property for staged_source prior to 0.4.0 libs . run_bash ( staged_source . connection , 'date' ) 0.4.0 from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs plugin = Plugin () @plugin.linked.pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # Property name was updated to 'source_connection' in 0.4.0 libs . run_bash ( staged_source . source_connection , 'date' )","title":"How to fix it"}]}